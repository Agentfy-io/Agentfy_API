# -*- coding: utf-8 -*-
"""
@file: customer_agent.py
@desc: å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€åˆ†æå’Œæ½œåœ¨å®¢æˆ·è¯†åˆ«åŠŸèƒ½
@auth: Callmeiks
"""

import json
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
import asyncio
import time

import numpy as np
import pandas as pd
from dotenv import load_dotenv
import os

from app.utils.logger import setup_logger
from services.ai_models.chatgpt import ChatGPT
from services.ai_models.claude import Claude
from services.cleaner.comment_cleaner import CommentCleaner
from services.crawler.comment_crawler import CommentCollector
from services.crawler.video_crawler import VideoCollector
from services.cleaner.video_cleaner import VideoCleaner
from app.config import settings
from app.core.exceptions import ValidationError, ExternalAPIError, InternalServerError

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = setup_logger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class CustomerAgent:
    """å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€åˆ†æå’Œæ½œåœ¨å®¢æˆ·è¯†åˆ«åŠŸèƒ½"""

    def __init__(self, tikhub_api_key: Optional[str] = None, tikhub_base_url: Optional[str] = None):
        """
        åˆå§‹åŒ–CustomerAgentï¼ŒåŠ è½½APIå¯†é’¥å’Œæç¤ºæ¨¡æ¿

        Args:
            tikhub_api_key: TikHub APIå¯†é’¥
            tikhub_base_url: TikHub APIåŸºç¡€URL
        """
        # åˆå§‹åŒ–AIæ¨¡å‹å®¢æˆ·ç«¯
        self.chatgpt = ChatGPT()
        self.claude = Claude()

        # ä¿å­˜TikHub APIé…ç½®
        self.tikhub_api_key = tikhub_api_key
        self.tikhub_base_url = tikhub_base_url

        # å¦‚æœæ²¡æœ‰æä¾›TikHub APIå¯†é’¥ï¼Œè®°å½•è­¦å‘Š
        if not self.tikhub_api_key:
            logger.warning("æœªæä¾›TikHub APIå¯†é’¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

        # æ”¯æŒçš„åˆ†æç±»å‹åˆ—è¡¨
        self.analysis_types = ['purchase_intent']

        # åŠ è½½ç³»ç»Ÿå’Œç”¨æˆ·æç¤º
        self._load_system_prompts()
        self._load_user_prompts()

    def _load_system_prompts(self) -> None:
        """åŠ è½½ç³»ç»Ÿæç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.system_prompts = {
            'purchase_intent': """You are an AI trained to analyze social media comments about products.
            For each comment in the provided list, analyze:
            1. Sentiment (positive, negative, or neutral)
            2. Purchase Intent (whether the user shows any interest in buying, trying, or acquiring the product)
            3. User Interest Level (high, medium, low)

            For each comment, provide analysis in the following JSON format:
            {
                "comment_id": "comment ID from input data",
                "text": "comment text",
                "sentiment": "positive/negative/neutral",
                "purchase_intent": true/false,
                "interest_level": "high/medium/low",
            }

            Respond with a JSON array containing analysis for all comments.
            Focus on identifying any signs of interest in the product, including:
            - Direct purchase intentions ("want to buy", "need this")
            - Indirect interest ("where can I find this", "love this")
            - Questions about product details
            - emojis indicating positive sentiment or interest
            - Positive reactions to product features
            - Any positive sentiment to the influencer herself/himself is consider neutral
            - Any engagement indicating potential future purchase""",
            'customer_reply': """# # Multilingual Customer Service AI Assistant
            
            ## System Instruction
            
            You are an advanced multilingual customer service AI for an e-commerce platform. Your task is to:
            1. Analyze the store information provided by the merchant
            2. Identify the language of both the store information and customer message
            3. Generate a helpful, accurate response to the customer inquiry
            4. Return your response in a structured JSON format
            
            ## Analysis Guidelines
            
            1. **Language Detection**:
               - Automatically detect the language of the store information
               - Automatically detect the language of the customer message
               - Translate the customer message to the store language
               - Determine the most appropriate language for your response (typically matching the customer's language)
            
            2. **Store Information Analysis**:
               - Extract key details about products, pricing, shipping, returns, promotions, etc.
               - Understand store policies across different languages
               - Identify store name and branding elements
            
            3. **Customer Message Analysis**:
               - Identify the customer's primary question or concern
               - Detect any secondary questions
               - Understand the customer's tone and respond appropriately
            
            ## Response Generation
            
            1. **Content Creation**:
               - Provide accurate information based only on the store details provided
               - If information is not available, acknowledge this limitation politely
               - Structure your response with greeting, answer, additional information.
               - Be concise and to the point, avoiding unnecessary details, usually one or two sentences is enough.
               - Maintain a helpful, professional tone appropriate to the detected culture
               - Also generate a translated version of your response in the shop language
            
            2. **Response Language**:
               - Respond in the same language as the customer message
               - If you cannot confidently respond in the customer's language, default to the store's language
               - Use culturally appropriate greetings and expressions
            
            ## JSON Output Format
            
            Your response must be formatted as a valid JSON object with the following structure:
            ```json
            {
              "detected_store_language": "language_code",
              "detected_customer_language": "language_code",
              "response_language": "language_code",
              'translated_customer_message': 'translated message',
              "response_text": "Your complete customer service response",
              "response_text_translated": "Your response translated to the store language",
              "confidence_score": 0.95
            }
            """,
            'batch_customer_reply':"""## Multilingual Batch Customer Service AI Assistant
### System Instruction
You are an advanced multilingual customer service AI for an e-commerce platform. Your task is to process multiple customer messages simultaneously and generate appropriate responses for each.
### Input Format
You will receive a JSON object with the following structure:
```json
{
  "shop_info": "Complete store information in any language",
  "messages": [
    {
      "message_id": 0,
      "commenter_uniqueId": "customer_unique_id_1",
      "comment_id": "optional_comment_id_1",
      "message_text": "Customer message 1 in any language"
    },
    {
      "message_id": 1,
      "commenter_uniqueId": "customer_unique_id_2",
      "comment_id": "optional_comment_id_2",
      "message_text": "Customer message 2 in any language"
    },
    ...
  ]
}
```
### Processing Steps
For each customer message, perform the following:

#### 1. Language Detection
- Detect the language of the store information.
- Detect the language of the customer message.
- Choose the appropriate language for your response (typically the customer's language).

#### 2. Message Translation
- If the customer's language differs from the store language, translate the customer message TO THE STORE'S LANGUAGE and save it as translated_customer_message.
- This translation helps the store owner understand what the customer is saying in the store owner's language.

#### 3. Content Analysis
- Understand the store policies, products, and services based on the `shop_info`.
- Identify the customer's question or concern.
- Formulate a helpful, accurate response based only on the available information.

#### 4. Response Creation
Generate a complete, professional response in the customer's language. Include:
- **Greeting:** Friendly opening appropriate to the language/culture.
- **Answer:** Directly address the customer's question.

If necessary, translate your response into the store's language for reference.

### Output Format
Return a JSON array containing one object for each input message in the same order as received. Each object must have the following structure:

```json
[
  {
    "message_id": 0,
    "detected_store_language": "language_code",
    "detected_customer_language": "language_code",
    'customer_unique_id': 'customer_unique_id_1',
    "translated_customer_message": "translated message",
    "response_text": "Your complete customer service response in customer's language",
    "response_text_translated": "Your response translated to the store language"
  },
  {
    "message_id": 1,
    "detected_store_language": "language_code",
    "detected_customer_language": "language_code",
    'customer_unique_id': 'customer_unique_id_2',
    "translated_customer_message": "translated message",
    "response_text": "Your complete customer service response in customer's language",
    "response_text_translated": "Your response translated to the store language"
  }
]
```

### Important Rules
âœ… Return **ONLY** a valid JSON array with objects matching the format above.
âœ… Use **ISO 639-1** codes for language identification (e.g., "en", "zh", "fr", "es").
âœ… Base responses **ONLY** on the provided store information.
âœ… If the provided information is insufficient to answer a question, clearly state this.
âœ… Maintain a professional and helpful tone.

### Response Content Guidelines
Each response should include:
- **Greeting** - Friendly and culturally appropriate.
- **Answer** - Direct and accurate, usually one or two sentences is enough.

This format ensures efficient multilingual customer support while maintaining high-quality, contextually relevant responses. ğŸš€
"""
        }

    def _load_user_prompts(self) -> None:
        """åŠ è½½ç”¨æˆ·æç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.user_prompts = {
            'purchase_intent': {
                'description': 'purchase intent'
            }
        }

    async def fetch_video_comments(self, aweme_id: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„æ¸…ç†åçš„è¯„è®ºæ•°æ®

        Args:
            aweme_id (str): è§†é¢‘ID

        Returns:
            Dict[str, Any]: æ¸…ç†åçš„è¯„è®ºæ•°æ®ï¼ŒåŒ…å«è§†é¢‘IDå’Œè¯„è®ºåˆ—è¡¨

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id or not isinstance(aweme_id, str):
                raise ValidationError(detail="aweme_idå¿…é¡»æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²", field="aweme_id")

            # è®°å½•å¼€å§‹è·å–è¯„è®º
            logger.info(f"å¼€å§‹è·å–è§†é¢‘ {aweme_id} çš„è¯„è®º")

            # è·å–è¯„è®º
            comment_collector = CommentCollector(self.tikhub_api_key, self.tikhub_base_url)
            comments = await comment_collector.collect_video_comments(aweme_id)

            if not comments or not comments.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æœªæ‰¾åˆ°è¯„è®º")
                return {
                    'aweme_id': aweme_id,
                    'comments': [],
                    'comment_count': 0,
                    'timestamp': datetime.now().isoformat()
                }

            # æ¸…æ´—è¯„è®º
            comment_cleaner = CommentCleaner()
            cleaned_comments = await comment_cleaner.clean_video_comments(comments)
            cleaned_comments = cleaned_comments.get('comments', [])

            processing_time = time.time() - start_time

            result = {
                'aweme_id': aweme_id,
                'comments': cleaned_comments,
                'comment_count': len(cleaned_comments),
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round(processing_time * 1000, 2)
            }

            logger.info(f"æˆåŠŸè·å–è§†é¢‘ {aweme_id} çš„è¯„è®º: {len(cleaned_comments)} æ¡ï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            return result

        except (ValidationError, ExternalAPIError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‡ºé”™: {str(e)}")
            raise

        except Exception as e:
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def analyze_comments_batch(
            self,
            df: pd.DataFrame,
            analysis_type: str,
            batch_size: int = 30,
            concurrency: int = 5
    ) -> pd.DataFrame:
        """
        åˆ†æ‰¹åˆ†æè¯„è®ºï¼Œä»¥é˜²æ­¢ä¸€æ¬¡æ€§å‘é€è¿‡å¤šæ•°æ®

        Args:
            df (pd.DataFrame): åŒ…å«è¯„è®ºæ•°æ®çš„DataFrameï¼Œéœ€åŒ…å«å­—æ®µï¼štext
            analysis_type (str): é€‰æ‹©åˆ†æç±»å‹ (purchase_intent)
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            pd.DataFrame: ç»“æœåˆå¹¶å›åŸå§‹DataFrame

        Raises:
            ValidationError: å½“analysis_typeæ— æ•ˆæˆ–dfä¸ºç©ºæ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if analysis_type not in self.analysis_types:
                raise ValidationError(
                    detail=f"æ— æ•ˆçš„åˆ†æç±»å‹: {analysis_type}. è¯·ä» {self.analysis_types} ä¸­é€‰æ‹©",
                    field="analysis_type"
                )

            if df.empty:
                raise ValidationError(detail="DataFrameä¸èƒ½ä¸ºç©º", field="df")

            if 'text' not in df.columns:
                raise ValidationError(detail="DataFrameå¿…é¡»åŒ…å«'text'åˆ—", field="df")

            # éªŒè¯å’Œè°ƒæ•´æ‰¹å¤„ç†å‚æ•°
            batch_size = min(batch_size, settings.MAX_BATCH_SIZE)
            concurrency = min(concurrency, 10)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º10

            # åˆ†æ‰¹å¤„ç†DataFrame
            num_splits = max(1, len(df) // batch_size + (1 if len(df) % batch_size > 0 else 0))
            comment_batches = np.array_split(df, num_splits)
            logger.info(
                f"ğŸš€ å¼€å§‹ {analysis_type} åˆ†æï¼Œå…± {len(comment_batches)} æ‰¹ï¼Œæ¯æ‰¹çº¦ {len(comment_batches[0]) if len(comment_batches) > 0 else 0} æ¡è¯„è®º"
            )

            # å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼ˆæ¯æ¬¡æœ€å¤š `concurrency` ç»„ï¼‰
            results = []
            for i in range(0, len(comment_batches), concurrency):
                batch_group = comment_batches[i:i + concurrency]

                # è®°å½•å½“å‰å¹¶å‘ç»„çš„èŒƒå›´
                batch_indices = [
                    f"{batch.index[0]}-{batch.index[-1]}" if not batch.empty else "-"
                    for batch in batch_group
                ]
                logger.info(
                    f"âš¡ å¤„ç†æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)}ï¼Œè¯„è®ºç´¢å¼•èŒƒå›´: {batch_indices}"
                )

                # å¹¶å‘æ‰§è¡Œ `concurrency` ä¸ªä»»åŠ¡
                tasks = [
                    self._analyze_aspect(analysis_type, batch[['text', 'comment_id']].to_dict('records'))
                    for batch in batch_group if not batch.empty
                ]

                if not tasks:
                    continue

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # å¤„ç†ç»“æœï¼Œè¿‡æ»¤æ‰å¼‚å¸¸
                valid_results = []
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"æ‰¹æ¬¡ {i + j + 1} åˆ†æå¤±è´¥: {str(result)}")
                    else:
                        valid_results.append(result)

                if len(valid_results) != len(batch_group):
                    logger.warning(
                        f"æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)} ä¸­æœ‰ {len(batch_group) - len(valid_results)} ä¸ªæ‰¹æ¬¡åˆ†æå¤±è´¥"
                    )

                results.extend(valid_results)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
            if not results:
                raise InternalServerError("æ‰€æœ‰æ‰¹æ¬¡åˆ†æå‡å¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆç»“æœ")

            # åˆå¹¶æ‰€æœ‰åˆ†æç»“æœ
            try:
                # å°†æ‰€æœ‰ç»“æœæ‰å¹³åŒ–ä¸ºå•ä¸ªåˆ—è¡¨
                all_results = []
                for batch_result in results:
                    if isinstance(batch_result, list):
                        all_results.extend(batch_result)

                # åˆ›å»ºç»“æœDataFrame
                analysis_df = pd.DataFrame(all_results)

                # ç¡®ä¿comment_idåˆ—å­˜åœ¨
                if 'comment_id' not in analysis_df.columns:
                    logger.warning("åˆ†æç»“æœç¼ºå°‘comment_idåˆ—ï¼Œæ— æ³•æ­£ç¡®åˆå¹¶")
                    # æ·»åŠ ç´¢å¼•ä½œä¸ºä¸´æ—¶åˆ—
                    analysis_df['temp_index'] = range(len(analysis_df))
                    df['temp_index'] = range(len(df))
                    # åŸºäºç´¢å¼•åˆå¹¶
                    merged_df = pd.merge(df, analysis_df, on='temp_index', how='left')
                    merged_df = merged_df.drop('temp_index', axis=1)
                else:
                    # åŸºäºcomment_idåˆå¹¶
                    merged_df = pd.merge(df, analysis_df, on='comment_id', how='left')

                # å¤„ç†é‡å¤çš„textåˆ—
                if 'text_y' in merged_df.columns:
                    merged_df = merged_df.drop('text_y', axis=1)
                    merged_df = merged_df.rename(columns={'text_x': 'text'})

                logger.info(f"âœ… {analysis_type} åˆ†æå®Œæˆï¼æ€»è®¡ {len(merged_df)} æ¡æ•°æ®")
                return merged_df

            except Exception as e:
                logger.error(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
                raise InternalServerError(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except InternalServerError:
            # ç›´æ¥å‘ä¸Šä¼ é€’å†…éƒ¨æœåŠ¡å™¨é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def _analyze_aspect(
            self,
            aspect_type: str,
            comment_data: List[Dict[str, Any]],
    ) -> Optional[List[Dict[str, Any]]]:
        """
        é€šç”¨åˆ†ææ–¹æ³•ï¼Œæ ¹æ®ä¸åŒçš„åˆ†æç±»å‹è°ƒç”¨ChatGPTæˆ–Claude AIæ¨¡å‹ã€‚

        Args:
            aspect_type (str): éœ€è¦åˆ†æçš„ç±»å‹ (purchase_intent)
            comment_data (List[Dict[str, Any]]): éœ€è¦åˆ†æçš„è¯„è®ºåˆ—è¡¨

        Returns:
            Optional[List[Dict[str, Any]]]: AIè¿”å›çš„åˆ†æç»“æœï¼Œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

        Raises:
            ValidationError: å½“aspect_typeæ— æ•ˆæ—¶
            ExternalAPIError: å½“è°ƒç”¨AIæœåŠ¡æ—¶å‡ºé”™
        """
        try:
            if aspect_type not in self.analysis_types:
                raise ValidationError(detail=f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {aspect_type}", field="aspect_type")

            if not comment_data:
                logger.warning("è¯„è®ºæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
                return []

            # è·å–åˆ†æçš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤º
            aspect_config = self.user_prompts[aspect_type]
            sys_prompt = self.system_prompts[aspect_type]
            user_prompt = (
                f"Analyze the {aspect_config['description']} for the following comments:\n"
                f"{json.dumps(comment_data, ensure_ascii=False)}"
            )

            # ä¸ºé¿å…tokené™åˆ¶ï¼Œé™åˆ¶è¯„è®ºæ–‡æœ¬é•¿åº¦
            for comment in comment_data:
                if 'text' in comment and len(comment['text']) > 1000:
                    comment['text'] = comment['text'][:997] + "..."

            # è°ƒç”¨AIè¿›è¡Œåˆ†æï¼Œä¼˜å…ˆä½¿ç”¨ChatGPTï¼Œå¤±è´¥æ—¶å°è¯•Claude
            try:
                response = await self.chatgpt.chat(
                    system_prompt=sys_prompt,
                    user_prompt=user_prompt
                )

                # è§£æChatGPTè¿”å›çš„ç»“æœ
                analysis_results = response["choices"][0]["message"]["content"].strip()

            except ExternalAPIError as e:
                logger.warning(f"ChatGPTåˆ†æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Claude: {str(e)}")
                try:
                    # å°è¯•ä½¿ç”¨Claudeä½œä¸ºå¤‡ä»½
                    response = await self.claude.chat(
                        system_prompt=sys_prompt,
                        user_prompt=user_prompt
                    )
                    analysis_results = response["choices"][0]["message"]["content"].strip()
                except Exception as claude_error:
                    logger.error(f"Claudeåˆ†æä¹Ÿå¤±è´¥: {str(claude_error)}")
                    raise ExternalAPIError(
                        detail="æ‰€æœ‰AIæœåŠ¡å‡æ— æ³•å®Œæˆåˆ†æ",
                        service="AI"
                    )

            # å¤„ç†è¿”å›çš„JSONæ ¼å¼ï¼ˆå¯èƒ½åŒ…å«åœ¨Markdownä»£ç å—ä¸­ï¼‰
            analysis_results = re.sub(
                r"```json\n|\n```|```|\n",
                "",
                analysis_results.strip()
            )

            try:
                analysis_result = json.loads(analysis_results)
                return analysis_result
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æé”™è¯¯: {str(e)}, åŸå§‹å†…å®¹: {analysis_results[:200]}...")
                raise ExternalAPIError(
                    detail="AIè¿”å›çš„ç»“æœæ— æ³•è§£æä¸ºJSON",
                    service="AI",
                    original_error=e
                )

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except ExternalAPIError:
            # ç›´æ¥å‘ä¸Šä¼ é€’APIé”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def get_purchase_intent_stats(
            self,
            aweme_id: str,
            batch_size: int = 30,
            concurrency: int = 5
    ) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): aiå¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“è°ƒç”¨å¤–éƒ¨æœåŠ¡å‡ºé”™æ—¶
            InternalServerError: å½“å†…éƒ¨å¤„ç†å‡ºé”™æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            if batch_size <= 0 or batch_size > settings.MAX_BATCH_SIZE:
                raise ValidationError(
                    detail=f"batch_sizeå¿…é¡»åœ¨1å’Œ{settings.MAX_BATCH_SIZE}ä¹‹é—´",
                    field="batch_size"
                )

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„ {len(comments_df)} æ¡è¯„è®º")
            analyzed_df = await self.analyze_comments_batch(
                comments_df,
                'purchase_intent',
                batch_size,
                concurrency
            )

            if analyzed_df.empty:
                raise InternalServerError(f"åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå¤±è´¥")

            # ç”Ÿæˆåˆ†ææ‘˜è¦
            analysis_summary = {
                'sentiment_distribution': self._analyze_sentiment_distribution(analyzed_df),
                'purchase_intent': self._analyze_purchase_intent(analyzed_df),
                'interest_levels': self._analyze_interest_levels(analyzed_df),
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'purchase_intent_stats',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }

            return analysis_summary

        except (ValidationError, ExternalAPIError, InternalServerError):
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"è·å–è´­ä¹°æ„å›¾ç»Ÿè®¡æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"è·å–è´­ä¹°æ„å›¾ç»Ÿè®¡æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def _analyze_sentiment_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ

        Args:
            df (pd.DataFrame): åŒ…å«sentimentåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ")
            if 'sentiment' not in df.columns:
                raise ValueError("DataFrameå¿…é¡»åŒ…å«'sentiment'åˆ—")

            # æ ‡å‡†åŒ–æƒ…æ„Ÿå€¼
            df['sentiment'] = df['sentiment'].str.lower()
            df.loc[df['sentiment'] == 'neg', 'sentiment'] = 'negative'
            df.loc[df['sentiment'] == 'pos', 'sentiment'] = 'positive'

            sentiment_counts = df['sentiment'].value_counts()
            return {
                'counts': sentiment_counts.to_dict(),
                'percentages': (sentiment_counts / len(df) * 100).round(2).to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†ææƒ…æ„Ÿåˆ†å¸ƒæ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'counts': {},
                'percentages': {}
            }

    def _analyze_purchase_intent(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æè´­ä¹°æ„å›¾

        Args:
            df (pd.DataFrame): åŒ…å«purchase_intentå’Œinterest_levelåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†æè´­ä¹°æ„å›¾")
            required_columns = ['purchase_intent', 'interest_level']
            for col in required_columns:
                if col not in df.columns:
                    raise ValueError(f"DataFrameå¿…é¡»åŒ…å«'{col}'åˆ—")

            # ç¡®ä¿purchase_intentæ˜¯å¸ƒå°”å€¼
            if df['purchase_intent'].dtype != bool:
                df['purchase_intent'] = df['purchase_intent'].apply(
                    lambda x: x if isinstance(x, bool) else (
                            str(x).lower() in ['true', 'yes', '1', 't']
                    )
                )

            # æ ‡å‡†åŒ–interest_level
            df['interest_level'] = df['interest_level'].str.lower()
            df.loc[df['interest_level'] == 'mid', 'interest_level'] = 'medium'

            intent_df = df[df['purchase_intent'] == True]

            return {
                'total_comments': len(df),
                'intent_count': len(intent_df),
                'intent_rate': round(len(intent_df) / len(df) * 100, 2) if len(df) > 0 else 0,
                'intent_by_interest_level': intent_df['interest_level'].value_counts().to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†æè´­ä¹°æ„å›¾æ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'total_comments': 0,
                'intent_count': 0,
                'intent_rate': 0,
                'intent_by_interest_level': {}
            }

    def _analyze_interest_levels(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æå…´è¶£æ°´å¹³

        Args:
            df (pd.DataFrame): åŒ…å«interest_levelåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: å…´è¶£æ°´å¹³ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†æå…´è¶£æ°´å¹³")
            if 'interest_level' not in df.columns:
                raise ValueError("DataFrameå¿…é¡»åŒ…å«'interest_level'åˆ—")

            # æ ‡å‡†åŒ–interest_level
            df['interest_level'] = df['interest_level'].str.lower()
            df.loc[df['interest_level'] == 'mid', 'interest_level'] = 'medium'

            interest_counts = df['interest_level'].value_counts()
            return {
                'counts': interest_counts.to_dict(),
                'percentages': (interest_counts / len(df) * 100).round(2).to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†æå…´è¶£æ°´å¹³æ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'counts': {},
                'percentages': {}
            }

    async def get_potential_customers(
            self,
            aweme_id: str,
            batch_size: int = 30,
            concurrency: int = 5,
            min_score: float = 50.0,
            max_score: float = 100.0,
            ins_filter: bool = False,
            twitter_filter: bool = False,
            region_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è®¡ç®—æ½œåœ¨å®¢æˆ·çš„å‚ä¸åº¦åˆ†æ•°ï¼Œå¹¶è¯†åˆ«æ½œåœ¨å®¢æˆ·

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): aiå¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5
            min_score (float, optional): æœ€å°å‚ä¸åº¦åˆ†æ•°ï¼Œé»˜è®¤50.0
            max_score (float, optional):f æœ€å¤§å‚ä¸åº¦åˆ†æ•°ï¼Œé»˜è®¤100.0
            ins_filter (bool, optional): æ˜¯å¦è¿‡æ»¤Instagramä¸ºNullç”¨æˆ·ï¼Œé»˜è®¤False
            twitter_filter (bool, optional): æ˜¯å¦è¿‡æ»¤Twitterä¸ºNullç”¨æˆ·ï¼Œé»˜è®¤False
            region_filter (Optional[str], optional): è¿‡æ»¤ç‰¹å®šåœ°åŒºçš„ç”¨æˆ·ï¼Œé»˜è®¤None

        Returns:
            Dict[str, Any]: æ½œåœ¨å®¢æˆ·ä¿¡æ¯

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if not aweme_id:
                raise ValueError("aweme_idä¸èƒ½ä¸ºç©º")

            if min_score < 0 or max_score > 100 or min_score >= max_score:
                raise ValueError("åˆ†æ•°èŒƒå›´æ— æ•ˆï¼Œåº”è¯¥æ»¡è¶³: 0 <= min_score < max_score <= 100")


            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            cleaned_comments = await self.fetch_video_comments(aweme_id)

            if not cleaned_comments.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            cleaned_comments = cleaned_comments.get('comments', [])
            comments_df = pd.DataFrame(cleaned_comments)

            # è¿‡æ»¤æ¡ä»¶
            if ins_filter:
                comments_df = comments_df[comments_df['ins_id']!= '']
            if twitter_filter:
                comments_df = comments_df[comments_df['twitter_id']!= '']
            if region_filter:
                comments_df = comments_df[comments_df['commenter_region'] == region_filter]
            if comments_df.empty:
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰ç¬¦åˆè¿‡æ»¤æ¡ä»¶çš„è¯„è®º")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found after filtering',
                    'analysis_timestamp': datetime.now().isoformat(),
                }
            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„ {len(comments_df)} æ¡è¯„è®ºä»¥è¯†åˆ«æ½œåœ¨å®¢æˆ·")

            analyzed_df = await self.analyze_comments_batch(
                comments_df,
                'purchase_intent',
                batch_size,
                concurrency
            )

            if analyzed_df.empty:
                raise RuntimeError(f"åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå¤±è´¥")

            # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„å‚ä¸åº¦åˆ†æ•°
            potential_customers = []
            for _, row in analyzed_df.iterrows():
                try:
                    potential_value = self._calculate_engagement_score(
                        row['sentiment'],
                        row['purchase_intent'],
                        row['interest_level']
                    )

                    # æ£€æŸ¥å¿…å¡«å­—æ®µ
                    user_id = row.get('commenter_uniqueId', '')
                    sec_uid = row.get('commenter_secuid', '')
                    text = row.get('text')

                    #if not all([user_id, sec_uid, text]):
                    #    logger.warning(f"è¯„è®ºæ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µï¼Œè·³è¿‡: {row}")
                    #    continue

                    potential_customers.append({
                        'user_uniqueId': user_id,
                        'potential_value': potential_value,
                        'user_secuid': sec_uid,
                        'ins_id': row.get('ins_id', ''),
                        'twitter_id': row.get('twitter_id', ''),
                        'region': row.get('commenter_region', ''),
                        'text': text
                    })
                except Exception as e:
                    logger.error(f"å¤„ç†ç”¨æˆ·è¯„è®ºæ—¶å‡ºé”™: {str(e)}, è·³è¿‡è¯¥è¯„è®º")
                    continue

            # åˆ›å»ºæ½œåœ¨å®¢æˆ·DataFrame
            if not potential_customers:
                logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æ½œåœ¨å®¢æˆ·")
                return {
                    'aweme_id': aweme_id,
                    'total_potential_customers': 0,
                    'min_engagement_score': min_score,
                    'max_engagement_score': max_score,
                    'average_potential_value': 0,
                    'potential_customers': []
                }

            potential_customers_df = pd.DataFrame(potential_customers)

            # æ ¹æ®æ½œåœ¨ä»·å€¼è¿‡æ»¤å’Œæ’åº
            filtered_df = potential_customers_df[
                potential_customers_df['potential_value'].between(min_score, max_score)
            ].sort_values(by='potential_value', ascending=False)

            avg_value = filtered_df['potential_value'].mean() if not filtered_df.empty else 0

            return {
                'aweme_id': aweme_id,
                'total_potential_customers': len(filtered_df),
                'min_engagement_score': min_score,
                'max_engagement_score': max_score,
                'average_potential_value': round(avg_value, 2),
                'potential_customers': filtered_df.to_dict(orient='records'),
                'analysis_timestamp': datetime.now().isoformat()
            }

        except ValueError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise ValueError
        except RuntimeError:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿è¡Œæ—¶é”™è¯¯
            raise RuntimeError
        except Exception as e:
            logger.error(f"è·å–æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise RuntimeError(f"è·å–æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")


    def _calculate_engagement_score(
            self,
            sentiment: str,
            purchase_intent: bool,
            interest_level: str
    ) -> float:
        """
        è®¡ç®—æ½œåœ¨å®¢æˆ·çš„å‚ä¸åº¦åˆ†æ•°

        Args:
            sentiment (str): æƒ…æ„Ÿåˆ†æç»“æœ ('positive', 'neutral', 'negative')
            purchase_intent (bool): æ˜¯å¦æœ‰è´­ä¹°æ„å›¾
            interest_level (str): å…´è¶£æ°´å¹³ ('high', 'medium', 'low')

        Returns:
            float: å‚ä¸åº¦åˆ†æ•° (0-100)
        """
        try:
            # å‚æ•°éªŒè¯å’Œæ ‡å‡†åŒ–
            if not isinstance(sentiment, str):
                sentiment = str(sentiment).lower()
            else:
                sentiment = sentiment.lower()

            if not isinstance(purchase_intent, bool):
                # å°è¯•è½¬æ¢ä¸ºå¸ƒå°”å€¼
                if isinstance(purchase_intent, str):
                    purchase_intent = purchase_intent.lower() in ['true', '1', 'yes', 't']
                else:
                    purchase_intent = bool(purchase_intent)

            if not isinstance(interest_level, str):
                interest_level = str(interest_level).lower()
            else:
                interest_level = interest_level.lower()

            # ä¿®æ­£æƒ…æ„Ÿæ ‡ç­¾
            if sentiment in ['neg', 'negative']:
                sentiment = 'negative'
            elif sentiment in ['pos', 'positive']:
                sentiment = 'positive'
            elif sentiment not in ['neutral']:
                sentiment = 'neutral'  # é»˜è®¤ä¸ºä¸­æ€§

            # å¤„ç†å…´è¶£æ°´å¹³ä¸­å€¼çš„ä¸åŒè¡¨ç¤º
            if interest_level in ['mid', 'medium']:
                interest_level = 'medium'
            elif interest_level not in ['high', 'low']:
                interest_level = 'low'  # é»˜è®¤ä¸ºä½å…´è¶£

            # 1. æƒ…æ„Ÿè½¬æ¢ (0-1 scale)
            sentiment_score = {
                'positive': 1.0,
                'neutral': 0.5,
                'negative': 0.0
            }.get(sentiment, 0.5)  # é»˜è®¤ä¸ºä¸­æ€§

            # 2. è´­ä¹°æ„å›¾åˆ†æ•°
            intent_score = 1.0 if purchase_intent else 0.0

            # 3. å…´è¶£æ°´å¹³åˆ†æ•°
            interest_score = {
                'high': 1.0,
                'medium': 0.5,
                'low': 0.2
            }.get(interest_level, 0.5)  # é»˜è®¤ä¸ºä¸­ç­‰

            # æ ¹æ®åŠ æƒå¹³å‡è®¡ç®—æ½œåœ¨ä»·å€¼
            potential_value = (
                                      0.3 * sentiment_score +
                                      0.4 * intent_score +
                                      0.3 * interest_score
                              ) * 100  # ç¼©æ”¾åˆ°0-100

            return round(potential_value, 2)

        except Exception as e:
            logger.error(f"è®¡ç®—å‚ä¸åº¦åˆ†æ•°æ—¶å‡ºé”™: {str(e)}")
            # è¿”å›é»˜è®¤å€¼
            return 0.0

    async def get_keyword_potential_customers(
            self,
            keyword: str,
            batch_size: int = 30,
            video_concurrency: int = 5,
            ai_concurrency: int = 5,
            min_score: float = 50.0,
            max_score: float = 100.0,
            ins_filter: bool = False,
            twitter_filter: bool = False,
            region_filter: Optional[str] = None,
            max_customers: Optional[int] = None,
    ) -> Dict[str, Any]:
        """
        æ ¹æ®å…³é”®è¯è·å–æ½œåœ¨å®¢æˆ·

        Args:
            keyword (str): å…³é”®è¯
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            video_concurrency (int, optional): è§†é¢‘å¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5
            ai_concurrency (int, optional): aiå¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5
            min_score (float, optional): æœ€å°å‚ä¸åº¦åˆ†æ•°ï¼Œé»˜è®¤50.0
            max_score (float, optional): æœ€å¤§å‚ä¸åº¦åˆ†æ•°ï¼Œé»˜è®¤100.0
            ins_filter (bool, optional): æ˜¯å¦è¿‡æ»¤Instagramä¸ºç©ºç”¨æˆ·ï¼Œé»˜è®¤False
            twitter_filter (bool, optional): æ˜¯å¦è¿‡æ»¤Twitterä¸ºç©ºç”¨æˆ·ï¼Œé»˜è®¤False
            region_filter (str, optional): åœ°åŒºè¿‡æ»¤ï¼Œé»˜è®¤None
            max_customers (int, optional): æœ€å¤§æ½œåœ¨å®¢æˆ·æ•°é‡ï¼Œé»˜è®¤None, Noneè¡¨ç¤ºä¸é™åˆ¶

        Returns:
            Dict[str, Any]: æ½œåœ¨å®¢æˆ·ä¿¡æ¯

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if not keyword:
                raise ValueError("keywordä¸èƒ½ä¸ºç©º")

            if min_score < 0 or max_score > 100 or min_score >= max_score:
                raise ValueError("åˆ†æ•°èŒƒå›´æ— æ•ˆï¼Œåº”è¯¥æ»¡è¶³: 0 <= min_score < max_score <= 100")

            # è·å–æ¸…ç†åçš„è§†é¢‘æ•°æ®
            video_collector = VideoCollector(self.tikhub_api_key, self.tikhub_base_url)
            video_cleaner = VideoCleaner()
            raw_videos = await video_collector.collect_videos_by_keyword(keyword)
            cleaned_videos = await video_cleaner.clean_videos_by_keyword(raw_videos)

            if not cleaned_videos.get('videos'):
                logger.warning(f"æœªæ‰¾åˆ°ä¸å…³é”®è¯ {keyword} ç›¸å…³çš„è§†é¢‘")
                return {
                    'keyword': keyword,
                    'error': 'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è§†é¢‘',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            videos_df = pd.DataFrame(cleaned_videos['videos'])
            aweme_ids = videos_df['aweme_id'].tolist()

            logger.info(f"å¼€å§‹åˆ†æä¸å…³é”®è¯ {keyword} ç›¸å…³çš„ {len(videos_df)} ä¸ªè§†é¢‘ä»¥è¯†åˆ«æ½œåœ¨å®¢æˆ·")

            # ä½¿ç”¨get_potential_customersæ–¹æ³•åˆ†ææ¯ä¸ªè§†é¢‘ï¼Œæ¯concurrencyä¸ªè§†é¢‘ä¸ºä¸€ç»„ï¼Œ
            potential_customers = []
            for i in range(0, len(aweme_ids), video_concurrency):
                batch_aweme_ids = aweme_ids[i:i + video_concurrency]
                tasks = [
                    self.get_potential_customers(
                        aweme_id,
                        batch_size,
                        ai_concurrency,
                        min_score,
                        max_score,
                        ins_filter,
                        twitter_filter,
                        region_filter,
                    ) for aweme_id in batch_aweme_ids
                ]

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                for result in batch_results:
                    if isinstance(result, Exception):
                        logger.error(f"å¤„ç†è§†é¢‘ {batch_aweme_ids} æ—¶å‡ºé”™: {str(result)}")
                        continue

                    if result.get('potential_customers') is None:
                        logger.warning(f"è§†é¢‘ {batch_aweme_ids} æ²¡æœ‰æ½œåœ¨å®¢æˆ·æ•°æ®")
                        continue

                    potential_customers.extend(result['potential_customers'])
                    if len(potential_customers) >= max_customers:
                        break
                if len(potential_customers) >= max_customers:
                    break
            # åˆ›å»ºæ½œåœ¨å®¢æˆ·DataFrame
            if not potential_customers:
                logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æ½œåœ¨å®¢æˆ·")
                return {
                    'keyword': keyword,
                    'total_potential_customers': 0,
                    'min_engagement_score': min_score,
                    'max_engagement_score': max_score,
                    'average_potential_value': 0,
                    'potential_customers': []
                }

            potential_customers_df = pd.DataFrame(potential_customers)
            # æ ¹æ®æ½œåœ¨ä»·å€¼è¿‡æ»¤å’Œæ’åº
            filtered_df = potential_customers_df[
                potential_customers_df['potential_value'].between(min_score, max_score)
            ].sort_values(by='potential_value', ascending=False)

            avg_value = filtered_df['potential_value'].mean() if not filtered_df.empty else 0

            return {
                'keyword': keyword,
                'total_potential_customers': len(filtered_df),
                'min_engagement_score': min_score,
                'max_engagement_score': max_score,
                'average_potential_value': round(avg_value, 2),
                'potential_customers': filtered_df.to_dict(orient='records'),
                'analysis_timestamp': datetime.now().isoformat()
            }
        except ValueError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise ValueError
        except RuntimeError:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿è¡Œæ—¶é”™è¯¯
            raise RuntimeError
        except Exception as e:
            logger.error(f"è·å–å…³é”®è¯æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise RuntimeError(f"è·å–å…³é”®è¯æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def generate_single_reply_message(
            self,
            shop_info: str,
            customer_id: str,
            customer_message: str,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯

        Args:
            shop_info (str): åº—é“ºä¿¡æ¯
            customer_id (str): å®¢æˆ·uniqueID
            customer_message (str): å®¢æˆ·æ¶ˆæ¯
        Returns:
            Dict[str, Any]: ç”Ÿæˆçš„å›å¤æ¶ˆæ¯

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if not customer_message:
                raise ValueError("customer_messageä¸èƒ½ä¸ºç©º")

            sys_prompt = self.system_prompts['customer_reply']
            user_prompt = f"Here is the shop information:\n{shop_info}\n\nHere is the customer message:\n{customer_message},\n\nPlease generate a reply message for the customer."

            # ç”Ÿæˆå›å¤æ¶ˆæ¯
            reply_message = await self.chatgpt.chat(
                system_prompt=sys_prompt,
                user_prompt=user_prompt,
                temperature=0.7,
            )

            # è§£æå›å¤æ¶ˆæ¯
            reply_message = reply_message["choices"][0]["message"]["content"].strip()
            # è§£æjson
            reply_message = re.sub(
                r"```json\n|\n```",
                "",
                reply_message.strip()
            )  # å»é™¤Markdownä»£ç å—

            reply_message = json.loads(reply_message)

            return {
                'customer_id': customer_id,
                'reply_message': reply_message,
            }
        except ValueError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise ValueError
        except RuntimeError:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿è¡Œæ—¶é”™è¯¯
            raise RuntimeError
        except Exception as e:
            logger.error(f"ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise RuntimeError(f"ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def generate_customer_reply_messages(
            self,
            shop_info: str,
            customer_messages: List[Dict[str, Any]],
            batch_size: int = 5
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡ç”Ÿæˆå®¢æˆ·å›å¤æ¶ˆæ¯

        Args:
            shop_info (str): åº—é“ºä¿¡æ¯
            customer_messages (List[Dict[str, Any]]): å®¢æˆ·æ¶ˆæ¯åˆ—è¡¨, æ¯ä¸ªæ¶ˆæ¯åŒ…æ‹¬commenter_uniqueId, comment_id, text
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„å®¢æˆ·æ¶ˆæ¯æ•°é‡. é»˜è®¤ä¸º5.

        Returns:
            List[Dict[str, Any]]: ç”Ÿæˆçš„å›å¤æ¶ˆæ¯åˆ—è¡¨, æ¯ä¸ªå›å¤åŒ…å«è¯­è¨€æ£€æµ‹å’Œå›å¤å†…å®¹

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if not shop_info:
                raise ValueError("åº—é“ºä¿¡æ¯ä¸èƒ½ä¸ºç©º")

            if not customer_messages:
                raise ValueError("å®¢æˆ·æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

            # æ£€æŸ¥æ¶ˆæ¯æ ¼å¼
            for msg in customer_messages:
                if "commenter_uniqueId" not in msg or "text" not in msg:
                    raise ValueError(f"æ¶ˆæ¯æ ¼å¼é”™è¯¯, å¿…é¡»åŒ…å«commenter_uniqueIdå’Œtextå­—æ®µ: {msg}")

            # å‡†å¤‡ç»“æœåˆ—è¡¨
            all_replies = []

            # æŒ‰æ‰¹æ¬¡å¤„ç†æ¶ˆæ¯
            for i in range(0, len(customer_messages), batch_size):
                batch = customer_messages[i:i + batch_size]

                # æ„å»ºæ‰¹å¤„ç†æç¤º
                batch_messages = []
                for idx, msg in enumerate(batch):
                    batch_messages.append({
                        "message_id": idx,
                        "commenter_uniqueId": msg.get("commenter_uniqueId"),
                        "comment_id": msg.get("comment_id", ""),
                        "message_text": msg.get("text")  # æ³¨æ„æ­¤å¤„keyæ”¹ä¸ºmessage_textä»¥é€‚é…prompt
                    })

                batch_prompt = {
                    "shop_info": shop_info,
                    "messages": batch_messages
                }

                # å°†å­—å…¸è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                batch_prompt_json = json.dumps(batch_prompt, ensure_ascii=False)

                # è°ƒç”¨AIç”Ÿæˆå›å¤
                batch_replies = await self.chatgpt.chat(
                    system_prompt=self.system_prompts['batch_customer_reply'],
                    user_prompt=batch_prompt_json,  # ç¡®ä¿è¿™é‡Œä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²
                    temperature=0.7
                )

                # è§£æAIå›å¤
                batch_replies = batch_replies["choices"][0]["message"]["content"].strip()
                # è§£æJSON
                batch_replies = re.sub(
                    r"```json\n|\n```",
                    "",
                    batch_replies.strip()
                )

                # è§£æå›å¤ç»“æœ
                try:
                    parsed_replies = json.loads(batch_replies)

                    # éªŒè¯å›å¤æ ¼å¼
                    if not isinstance(parsed_replies, list):
                        raise ValueError("AIè¿”å›çš„ç»“æœæ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºåˆ—è¡¨")

                    # å°†æ‰¹æ¬¡å›å¤æ·»åŠ åˆ°æ€»ç»“æœä¸­
                    for reply in parsed_replies:
                        # ç¡®ä¿uniqueIDåœ¨å›å¤ä¸­
                        message_id = reply.get("message_id")
                        if message_id is not None and 0 <= message_id < len(batch):
                            reply["commenter_uniqueId"] = batch[message_id].get("commenter_uniqueId")
                            reply["comment_id"] = batch[message_id].get("comment_id", "")

                        all_replies.append(reply)

                except json.JSONDecodeError as json_err:
                    logger.error(f"æ— æ³•è§£æAIè¿”å›çš„JSONç»“æœ: {batch_replies[:200]}... (é”™è¯¯: {str(json_err)})")
                    raise RuntimeError(f"AIè¿”å›çš„ç»“æœä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {str(json_err)}")

            return all_replies

        except ValueError as e:
            logger.warning(f"å‚æ•°éªŒè¯é”™è¯¯: {str(e)}")
            raise

        except RuntimeError as e:
            logger.error(f"è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
            raise

        except Exception as e:
            logger.error(f"æ‰¹é‡ç”Ÿæˆå®¢æˆ·å›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}", exc_info=True)
            raise RuntimeError(f"æ‰¹é‡ç”Ÿæˆå®¢æˆ·å›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

async def main():
    # åˆ›å»ºCustomerAgentå®ä¾‹
    agent = CustomerAgent()

    # ç¤ºä¾‹ï¼šè·å–keywordæ½œåœ¨å®¢æˆ·
    #keyword = "red liptick"
    #potential_customers = await agent.get_keyword_potential_customers(keyword,20, 5, 5, 0, 100.0, True, False, 'US', 100)

    #save to json
    #with open('potential_customers.json', 'w', encoding='utf-8') as f:
    #    json.dump(potential_customers, f, ensure_ascii=False, indent=4)

    # ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯æµ‹è¯•
    shop_info = """
    åº—é“ºåç§°ï¼šä¼˜é›…æ—¶å°šå±‹
åº—é“ºç®€ä»‹ï¼šä¼˜é›…æ—¶å°šå±‹æˆç«‹äº2015å¹´ï¼Œä¸“æ³¨äºæä¾›é«˜å“è´¨çš„æ—¶å°šæœè£…å’Œé…é¥°ã€‚æˆ‘ä»¬è‡´åŠ›äºä¸ºå®¢æˆ·æä¾›æœ€æ–°çš„æ—¶å°šè¶‹åŠ¿å’Œæ°¸æ’çš„ç»å…¸æ¬¾å¼ã€‚

äº§å“ä¿¡æ¯ï¼š
- å¥³å£«è¿è¡£è£™ï¼šä»·æ ¼èŒƒå›´åœ¨ï¿¥299-ï¿¥899ï¼Œæè´¨åŒ…æ‹¬æ£‰ã€ä¸ç»¸å’Œæ··çººé¢æ–™
- ç”·å£«è¡¬è¡«ï¼šä»·æ ¼èŒƒå›´åœ¨ï¿¥199-ï¿¥599ï¼Œæœ‰å¤šç§æ¬¾å¼å’Œé¢œè‰²å¯é€‰
- æ—¶å°šåŒ…åŒ…ï¼šä»·æ ¼èŒƒå›´åœ¨ï¿¥499-ï¿¥1899ï¼Œæœ‰çœŸçš®å’Œé«˜çº§PUæè´¨å¯é€‰
- ç²¾ç¾é¦–é¥°ï¼šä»·æ ¼èŒƒå›´åœ¨ï¿¥99-ï¿¥699ï¼ŒåŒ…æ‹¬é¡¹é“¾ã€è€³ç¯å’Œæ‰‹é“¾

ä¿ƒé”€æ´»åŠ¨ï¼š
- æ–°ç”¨æˆ·é¦–å•æ»¡ï¿¥500å‡ï¿¥50
- æ¯å‘¨ä¸‰ä¼šå‘˜æ—¥ï¼Œå…¨åœº9æŠ˜
- å­£æœ«æ¸…ä»“ï¼ŒæŒ‡å®šå•†å“ä½è‡³5æŠ˜

é…é€æ”¿ç­–ï¼š
- å›½å†…è®¢å•æ»¡ï¿¥199å…è¿è´¹ï¼Œå¦åˆ™è¿è´¹ï¿¥15
- å›½é™…é…é€å¯å‘å¾€äºšæ´²ã€æ¬§æ´²ã€åŒ—ç¾ç­‰åœ°åŒºï¼Œè¿è´¹æ ¹æ®é‡é‡å’Œç›®çš„åœ°è®¡ç®—
- æ­£å¸¸è®¢å•å¤„ç†æ—¶é—´ä¸º1-2ä¸ªå·¥ä½œæ—¥ï¼Œå›½å†…é…é€3-5å¤©ï¼Œå›½é™…é…é€7-15å¤©

é€€æ¢æ”¿ç­–ï¼š
- æ”¶åˆ°å•†å“å7å¤©å†…å¯ç”³è¯·é€€æ¢
- å•†å“éœ€ä¿æŒåŸåŒ…è£…å’ŒåŠç‰Œå®Œå¥½
- å®šåˆ¶å•†å“å’Œç‰¹ä»·å•†å“ä¸æ”¯æŒé€€æ¢

æ”¯ä»˜æ–¹å¼ï¼š
- æ”¯æŒæ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ã€é“¶è”å¡å’Œä¸»æµå›½é™…ä¿¡ç”¨å¡
- å›½é™…è®¢å•æ”¯æŒPayPalæ”¯ä»˜

è”ç³»æ–¹å¼ï¼š
- å®¢æœç”µè¯ï¼š400-888-7777ï¼ˆå·¥ä½œæ—¥9:00-18:00ï¼‰
- å®¢æœé‚®ç®±ï¼šservice@elegantfashion.com
- å¾®ä¿¡å…¬ä¼—å·ï¼šä¼˜é›…æ—¶å°šå±‹"""
    #customer_id = "12345"
    #customer_message = "OÃ¹ puis-je trouver les informations sur la livraison ?"
    #reply_lang = "zh"

    #reply_message = await agent.generate_single_reply_message(shop_info, customer_id, customer_message)
    #print(reply_message)

    # ç”Ÿæˆæ‰¹é‡å®¢æˆ·å›å¤æ¶ˆæ¯æµ‹è¯•
    customer_messages = [
        {
            "commenter_uniqueId": "user1",
            "comment_id": "c1",
            "text": "OÃ¹ puis-je trouver les informations sur la livraison ?"  # æ³•è¯­: æˆ‘åœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°é…é€ä¿¡æ¯ï¼Ÿ
        },
        {
            "commenter_uniqueId": "user2",
            "comment_id": "c2",
            "text": "What is the return policy?"  # è‹±è¯­: é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ
        },
        {
            "commenter_uniqueId": "user3",
            "comment_id": "c3",
            "text": "Â¿CuÃ¡nto tiempo tarda en llegar mi pedido a EspaÃ±a?"  # è¥¿ç­ç‰™è¯­: æˆ‘çš„è®¢å•å¤šä¹…èƒ½é€åˆ°è¥¿ç­ç‰™ï¼Ÿ
        },
        {
            "commenter_uniqueId": "user4",
            "comment_id": "c4",
            "text": "ã“ã®å•†å“ã¯æ—¥æœ¬ã«é…é€ã§ãã¾ã™ã‹ï¼Ÿé€æ–™ã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ"  # æ—¥è¯­: è¿™ä¸ªå•†å“å¯ä»¥é€åˆ°æ—¥æœ¬å—ï¼Ÿè¿è´¹æ˜¯å¤šå°‘ï¼Ÿ
        },
        {
            "commenter_uniqueId": "user5",
            "comment_id": "c5",
            "text": "Ich mÃ¶chte wissen, ob die GrÃ¶ÃŸe M noch verfÃ¼gbar ist?"  # å¾·è¯­: æˆ‘æƒ³çŸ¥é“Mç æ˜¯å¦è¿˜æœ‰åº“å­˜ï¼Ÿ
        },
        {
            "commenter_uniqueId": "user6",
            "comment_id": "c6",
            "text": "Ho fatto un ordine tre giorni fa ma non ho ricevuto nessuna conferma. Potete aiutarmi?"
            # æ„å¤§åˆ©è¯­: æˆ‘ä¸‰å¤©å‰ä¸‹äº†è®¢å•ä½†æ²¡æ”¶åˆ°ç¡®è®¤ã€‚æ‚¨èƒ½å¸®æˆ‘å—ï¼Ÿ
        },
        {
            "commenter_uniqueId": "user7",
            "comment_id": "c7",
            "text": "ĞŸÑ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚Ğµ Ğ»Ğ¸ Ğ²Ñ‹ Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ñƒ PayPal?"  # ä¿„è¯­: ä½ ä»¬æ¥å—PayPalä»˜æ¬¾å—ï¼Ÿ
        },
        {
            "commenter_uniqueId": "user8",
            "comment_id": "c8",
            "text": "æˆ‘æƒ³äº†è§£ä¸€ä¸‹è¿™ä¸ªäº§å“çš„æè´¨æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯çº¯æ£‰çš„å—ï¼Ÿ"  # ä¸­æ–‡: è¯¢é—®äº§å“æè´¨
        },
        {
            "commenter_uniqueId": "user9",
            "comment_id": "c9",
            "text": "Do you offer express shipping? I need this item by next week."  # è‹±è¯­: ä½ ä»¬æä¾›å¿«é€’å—ï¼Ÿæˆ‘éœ€è¦ä¸‹å‘¨æ”¶åˆ°è¿™ä¸ªç‰©å“ã€‚
        },
        {
            "commenter_uniqueId": "user10",
            "comment_id": "c10",
            "text": "Ù‡Ù„ ØªØ´Ø­Ù†ÙˆÙ† Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŸ ÙˆÙƒÙ… ØªÙƒÙ„ÙØ© Ø§Ù„Ø´Ø­Ù†ØŸ"  # é˜¿æ‹‰ä¼¯è¯­: ä½ ä»¬å‘è´§åˆ°æ²™ç‰¹é˜¿æ‹‰ä¼¯å—ï¼Ÿè¿è´¹æ˜¯å¤šå°‘ï¼Ÿ
        }
    ]

    batch_reply_messages = await agent.generate_customer_reply_messages(shop_info, customer_messages, 10)
    print(batch_reply_messages)



if __name__ == "__main__":
    asyncio.run(main())


