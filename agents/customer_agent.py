# -*- coding: utf-8 -*-
"""
@file: customer_agent.py
@desc: å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€åˆ†æå’Œæ½œåœ¨å®¢æˆ·è¯†åˆ«åŠŸèƒ½
@auth: Callmeiks
"""

import json
import re
import sys
import time
import asyncio
from datetime import datetime
from time import process_time
from typing import Dict, Any, List, Optional, Union, AsyncGenerator

import numpy as np
import pandas as pd
from dotenv import load_dotenv
import os

from app.config import settings
from app.utils.logger import setup_logger
from services.ai_models.chatgpt import ChatGPT
from services.ai_models.claude import Claude
from services.cleaner.comment_cleaner import CommentCleaner
from services.cleaner.video_cleaner import VideoCleaner
from services.crawler.comment_crawler import CommentCollector
from services.crawler.video_crawler import VideoCollector
from app.core.exceptions import ValidationError, ExternalAPIError, InternalServerError

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = setup_logger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class CustomerAgent:
    """å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€åˆ†æå’Œæ½œåœ¨å®¢æˆ·è¯†åˆ«åŠŸèƒ½"""

    def __init__(self, tikhub_api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–CustomerAgentï¼ŒåŠ è½½APIå¯†é’¥å’Œæç¤ºæ¨¡æ¿

        Args:
            tikhub_api_key: TikHub APIå¯†é’¥
        """
        self.total_customers = 0

        # åˆå§‹åŒ–AIæ¨¡å‹å®¢æˆ·ç«¯
        self.chatgpt = ChatGPT()
        self.claude = Claude()

        # åˆå§‹åŒ–æ”¶é›†å™¨å’Œæ¸…æ´å™¨
        self.comment_collector = CommentCollector(tikhub_api_key)
        self.comment_cleaner = CommentCleaner()

        # ä¿å­˜TikHub APIé…ç½®
        self.tikhub_api_key = tikhub_api_key
        self.tikhub_base_url = settings.TIKHUB_BASE_URL

        # å¦‚æœæ²¡æœ‰æä¾›TikHub APIå¯†é’¥ï¼Œè®°å½•è­¦å‘Š
        if not self.tikhub_api_key:
            logger.warning("æœªæä¾›TikHub APIå¯†é’¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

        # æ”¯æŒçš„åˆ†æç±»å‹åˆ—è¡¨
        self.analysis_types = ['purchase_intent']

        # åŠ è½½ç³»ç»Ÿå’Œç”¨æˆ·æç¤º
        self._load_system_prompts()
        self._load_user_prompts()

    def _load_system_prompts(self) -> None:
        """åŠ è½½ç³»ç»Ÿæç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.system_prompts = {
            'purchase_intent': """You are an AI trained to analyze social media comments about products.
            For each comment in the provided list, analyze:
            1. Sentiment (positive, negative, or neutral)
            2. Purchase Intent (whether the user shows any interest in buying, trying, or acquiring the product)
            3. User Interest Level (high, medium, low)

            For each comment, provide analysis in the following JSON format:
            {
                "comment_id": "comment ID from input data",
                "text": "comment text",
                "sentiment": "positive/negative/neutral",
                "purchase_intent": true/false,
                "interest_level": "high/medium/low",
            }

            Respond with a JSON array containing analysis for all comments.
            Focus on identifying any signs of interest in the product, including:
            - Direct purchase intentions ("want to buy", "need this")
            - Indirect interest ("where can I find this", "love this")
            - Questions about product details
            - emojis indicating positive sentiment or interest
            - Positive reactions to product features
            - Any positive sentiment to the influencer herself/himself is consider neutral
            - Any engagement indicating potential future purchase""",

            'purchase_intent_report': """You are a social commerce analyst specializing in conversion optimization. Your task is to create an actionable report analyzing comment data to reveal audience interests, product sentiment, and purchase considerations.        
                    Report Sections:
                    1. Report Metadata (video ID, total number of comments, analysis timestamp)
                    2. Audience Interest Overview (2-3 sentences summarizing what aspects of the product commenters focus on most)
                    3. Key Metrics (markdown table with statistics on product features mentioned, price sentiments, and overall attitude)
                    4. Product Aspect Analysis (analyze which product features/attributes receive the most attention and whether feedback is positive/negative)
                    5. Price Perception (analyze how commenters perceive pricing - too high, reasonable, good value, etc.)
                    6. Sales Approach Feedback (identify what selling approaches commenters respond to or suggest)
                    7. Conversion Barriers & Opportunities (specific insights about what might prevent or encourage purchases)
                    8. Recommendations (2-3 actionable steps to improve conversion based on comment analysis)
                    
                    Focus on actionable insights derived directly from the input data. Identify patterns in what commenters care about, their suggestions for selling, price perceptions, and overall product sentiment (positive or negative). Analyze variables that reflect audience opinions and highlight product effect analysis based strictly on the input data. Keep the report under 450 words.
                    """,

            'customer_reply': """# # Multilingual Customer Service AI Assistant

            ## System Instruction

            You are an advanced multilingual customer service AI for an e-commerce platform. Your task is to:
            1. Analyze the store information provided by the merchant
            2. Identify the language of both the store information and customer message
            3. Generate a helpful, accurate response to the customer inquiry
            4. Return your response in a structured JSON format

            ## Analysis Guidelines

            1. **Language Detection**:
               - Automatically detect the language of the store information
               - Automatically detect the language of the customer message
               - Translate the customer message to the store language
               - Determine the most appropriate language for your response (typically matching the customer's language)

            2. **Store Information Analysis**:
               - Extract key details about products, pricing, shipping, returns, promotions, etc.
               - Understand store policies across different languages
               - Identify store name and branding elements

            3. **Customer Message Analysis**:
               - Identify the customer's primary question or concern
               - Detect any secondary questions
               - Understand the customer's tone and respond appropriately

            ## Response Generation

            1. **Content Creation**:
               - Provide accurate information based only on the store details provided
               - If information is not available, acknowledge this limitation politely
               - Structure your response with greeting, answer, additional information.
               - Be concise and to the point, avoiding unnecessary details, usually one or two sentences is enough.
               - Maintain a helpful, professional tone appropriate to the detected culture
               - Also generate a translated version of your response in the shop language

            2. **Response Language**:
               - Respond in the same language as the customer message
               - If you cannot confidently respond in the customer's language, default to the store's language
               - Use culturally appropriate greetings and expressions

            ## JSON Output Format

            Your response must be formatted as a valid JSON object with the following structure:
            ```json
            {
              "detected_store_language": "language_code",
              "detected_customer_language": "language_code",
              "response_language": "language_code",
              'translated_customer_message': 'translated message',
              "response_text": "Your complete customer service response",
              "response_text_translated": "Your response translated to the store language",
              "confidence_score": 0.95
            }
            """,

            'batch_customer_reply': """## Multilingual Batch Customer Service AI Assistant
                ### System Instruction
                You are an advanced multilingual customer service AI for an e-commerce platform. Your task is to process multiple customer messages simultaneously and generate appropriate responses for each.
                ### Input Format
                You will receive a JSON object with the following structure:
                ```json
                {
                  "shop_info": "Complete store information in any language",
                  "messages": {
                    "jessica1h": "Customer message 1 in any language",
                    "adam_123": "Customer message 2 in any language",
                    ........
                  }
                }
                ```
                ### Processing Steps
                For each customer message, perform the following:
                
                #### 1. Language Detection
                - Detect the language of the store information.
                - Detect the language of the customer message.
                - Choose the appropriate language for your response (typically the customer's language).
                
                #### 2. Message Translation
                - If the customer's language differs from the store language, translate the customer message TO THE STORE'S LANGUAGE and save it as translated_customer_message.
                - This translation helps the store owner understand what the customer is saying in the store owner's language.
                
                #### 3. Content Analysis
                - Understand the store policies, products, and services based on the `shop_info`.
                - Identify the customer's question or concern.
                - Formulate a helpful, accurate response based only on the available information.
                
                #### 4. Response Creation
                Generate a complete, professional response in the customer's language. Include:
                - **Greeting:** Friendly opening appropriate to the language/culture.
                - **Answer:** Directly address the customer's question.
                
                If necessary, translate your response into the store's language for reference.
                
                ### Output Format
                Return a JSON array containing one object for each input message in the same order as received. Each object must have the following structure:
                
                ```json
                [
                  {
                    "message_id": 0,
                    "detected_store_language": "language_code",
                    "detected_customer_language": "language_code",
                    "customer_unique_id": "customer_unique_id_1",
                    "translated_customer_message": "translated message",
                    "response_text": "Your complete customer service response in customer's language",
                    "response_text_translated": "Your response translated to the store language"
                  },
                  {
                    "message_id": 1,
                    "detected_store_language": "language_code",
                    "detected_customer_language": "language_code",
                    'customer_unique_id': 'customer_unique_id_2',
                    "translated_customer_message": "translated message",
                    "response_text": "Your complete customer service response in customer's language",
                    "response_text_translated": "Your response translated to the store language"
                  }
                ]
                ```
                
                ### Important Rules
                âœ… Return **ONLY** a valid JSON array with objects matching the format above.
                âœ… Use **ISO 639-1** codes for language identification (e.g., "en", "zh", "fr", "es").
                âœ… Base responses **ONLY** on the provided store information.
                âœ… If the provided information is insufficient to answer a question, clearly state this.
                âœ… Maintain a professional and helpful tone.
                
                ### Response Content Guidelines
                Each response should include:
                - **Greeting** - Friendly and culturally appropriate.
                - **Answer** - Direct and accurate, usually one or two sentences is enough.
                
                This format ensures efficient multilingual customer support while maintaining high-quality, contextually relevant responses. ğŸš€
                """
        }

    def _load_user_prompts(self) -> None:
        """åŠ è½½ç”¨æˆ·æç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.user_prompts = {
            'purchase_intent': {
                'description': 'purchase intent'
            }
        }

    """---------------------------------------------é€šç”¨æ–¹æ³•/å·¥å…·ç±»æ–¹æ³•---------------------------------------------"""
    async def _analyze_aspect(
            self,
            aspect_type: str,
            comment_data: List[Dict[str, Any]],
    ) -> Optional[List[Dict[str, Any]]]:
        """
        é€šç”¨åˆ†ææ–¹æ³•ï¼Œæ ¹æ®ä¸åŒçš„åˆ†æç±»å‹è°ƒç”¨ChatGPTæˆ–Claude AIæ¨¡å‹ã€‚

        æ­¥éª¤:
        1. éªŒè¯åˆ†æç±»å‹æ˜¯å¦æ”¯æŒ
        2. æ„é€ åˆ†ææç¤º
        3. è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ
        4. è§£æå¹¶è¿”å›åˆ†æç»“æœ

        Args:
            aspect_type: éœ€è¦åˆ†æçš„ç±»å‹ (purchase_intent)
            comment_data: éœ€è¦åˆ†æçš„è¯„è®ºåˆ—è¡¨

        Returns:
            Optional[List[Dict[str, Any]]]: AIè¿”å›çš„åˆ†æç»“æœï¼Œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

        Raises:
            ValidationError: å½“aspect_typeæ— æ•ˆæ—¶
            ExternalAPIError: å½“è°ƒç”¨AIæœåŠ¡æ—¶å‡ºé”™
        """
        try:
            # éªŒè¯åˆ†æç±»å‹æ˜¯å¦æ”¯æŒ
            if aspect_type not in self.analysis_types:
                raise ValidationError(detail=f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {aspect_type}", field="aspect_type")

            # æ£€æŸ¥è¯„è®ºæ•°æ®æ˜¯å¦ä¸ºç©º
            if not comment_data:
                logger.warning("è¯„è®ºæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
                return []

            # è·å–åˆ†æçš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤º
            aspect_config = self.user_prompts[aspect_type]
            sys_prompt = self.system_prompts[aspect_type]
            user_prompt = (
                f"Analyze the {aspect_config['description']} for the following comments:\n"
                f"{json.dumps(comment_data, ensure_ascii=False)}"
            )

            # ä¸ºé¿å…tokené™åˆ¶ï¼Œé™åˆ¶è¯„è®ºæ–‡æœ¬é•¿åº¦
            for comment in comment_data:
                if 'text' in comment and len(comment['text']) > 1000:
                    comment['text'] = comment['text'][:997] + "..."

            # å°è¯•ä½¿ç”¨ChatGPTè¿›è¡Œåˆ†æ
            try:
                response = await self.chatgpt.chat(
                    system_prompt=sys_prompt,
                    user_prompt=user_prompt
                )

                # è§£æChatGPTè¿”å›çš„ç»“æœ
                analysis_results = response["choices"][0]["message"]["content"].strip()

            except ExternalAPIError as e:
                # ChatGPTå¤±è´¥æ—¶å°è¯•ä½¿ç”¨Claudeä½œä¸ºå¤‡ä»½
                logger.warning(f"ChatGPTåˆ†æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Claude: {str(e)}")
                try:
                    response = await self.claude.chat(
                        system_prompt=sys_prompt,
                        user_prompt=user_prompt
                    )
                    analysis_results = response["choices"][0]["message"]["content"].strip()
                except Exception as claude_error:
                    logger.error(f"Claudeåˆ†æä¹Ÿå¤±è´¥: {str(claude_error)}")
                    raise ExternalAPIError(
                        detail="æ‰€æœ‰AIæœåŠ¡å‡æ— æ³•å®Œæˆåˆ†æ",
                        service="AI"
                    )

            # å¤„ç†è¿”å›çš„JSONæ ¼å¼ï¼ˆå¯èƒ½åŒ…å«åœ¨Markdownä»£ç å—ä¸­ï¼‰
            analysis_results = re.sub(
                r"```json\n|\n```|```|\n",
                "",
                analysis_results.strip()
            )

            try:
                analysis_result = json.loads(analysis_results)
                return analysis_result
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æé”™è¯¯: {str(e)}, åŸå§‹å†…å®¹: {analysis_results[:200]}...")
                raise ExternalAPIError(
                    detail="AIè¿”å›çš„ç»“æœæ— æ³•è§£æä¸ºJSON",
                    service="AI",
                    original_error=e
                )

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except ExternalAPIError:
            # ç›´æ¥å‘ä¸Šä¼ é€’APIé”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def generate_analysis_report(self, aweme_id: str, analysis_type: str, data: Dict[str, Any]) -> str:
        """
        ç”ŸæˆæŠ¥å‘Šå¹¶è½¬æ¢ä¸ºHTML

        Args:
            aweme_id (str): è§†é¢‘ ID
            analysis_type (str): åˆ†æç±»å‹
            data (Dict[str, Any]): åˆ†ææ•°æ®

        Returns:
            str: HTMLæŠ¥å‘Šçš„æœ¬åœ°æ–‡ä»¶URL
        """
        if analysis_type not in self.system_prompts:
            raise ValueError(f"Invalid report type: {analysis_type}. Choose from {self.system_prompts.keys()}")

        # è·å–ç³»ç»Ÿæç¤º
        sys_prompt = self.system_prompts[analysis_type]

        # è·å–ç”¨æˆ·æç¤º
        user_prompt = f"Generate a report for the {analysis_type} analysis based on the following data:\n{json.dumps(data, ensure_ascii=False)}, for video ID: {aweme_id}"

        # ç”ŸæˆæŠ¥å‘Š
        response = await self.chatgpt.chat(
            system_prompt=sys_prompt,
            user_prompt=user_prompt
        )

        report = response["choices"][0]["message"]["content"].strip()

        # ä¿å­˜MarkdownæŠ¥å‘Š
        report_dir = "reports"
        os.makedirs(report_dir, exist_ok=True)

        report_md_path = os.path.join(report_dir, f"report_{aweme_id}.md")
        with open(report_md_path, "w", encoding="utf-8") as f:
            f.write(report)

        # è½¬æ¢ä¸ºHTML
        html_content = self.convert_markdown_to_html(report, f"{analysis_type.title()} Analysis for {aweme_id}")
        html_filename = f"report_{aweme_id}.html"
        html_path = os.path.join(report_dir, html_filename)

        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶URL
        absolute_path = os.path.abspath(html_path)

        # æ„å»ºfile://åè®®URL
        file_url = f"file://{absolute_path}"

        # ç¡®ä¿è·¯å¾„åˆ†éš”ç¬¦æ˜¯URLå…¼å®¹çš„
        if os.name == 'nt':  # Windowsç³»ç»Ÿ
            # Windowsè·¯å¾„éœ€è¦è½¬æ¢ä¸ºURLæ ¼å¼
            file_url = file_url.replace('\\', '/')

        print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: Markdown ({report_md_path}), HTML ({html_path})")
        print(f"æŠ¥å‘Šæœ¬åœ°URL: {file_url}")

        return file_url

    def convert_markdown_to_html(self, markdown_content: str, title: str = "Analysis Report") -> str:
        """
        å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML

        Args:
            markdown_content (str): Markdownå†…å®¹
            title (str): HTMLé¡µé¢æ ‡é¢˜

        Returns:
            str: HTMLå†…å®¹
        """
        try:
            import markdown
        except ImportError:
            print("è¯·å®‰è£…markdownåº“: pip install markdown")
            return f"<pre>{markdown_content}</pre>"

        # è½¬æ¢Markdownä¸ºHTML
        html_content = markdown.markdown(
            markdown_content,
            extensions=['tables', 'fenced_code', 'codehilite', 'toc']
        )

        # åˆ›å»ºå®Œæ•´HTMLæ–‡æ¡£
        css = """
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; color: #333; }
        h1, h2, h3 { margin-top: 1.5em; color: #111; }
        pre { background-color: #f6f8fa; border-radius: 3px; padding: 16px; overflow: auto; }
        code { font-family: SFMono-Regular, Consolas, Menlo, monospace; background-color: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 1em; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f6f8fa; }
        """

        html_document = f"""<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{title}</title>
        <style>{css}</style>
    </head>
    <body>
        <h1>{title}</h1>
        {html_content}
    </body>
    </html>
        """

        return html_document

    def _calculate_engagement_score(
            self,
            sentiment: str,
            purchase_intent: bool,
            interest_level: str
    ) -> float:
        """
        è®¡ç®—æ½œåœ¨å®¢æˆ·çš„å‚ä¸åº¦åˆ†æ•°

        æ­¥éª¤:
        1. æ ‡å‡†åŒ–è¾“å…¥å‚æ•°
        2. æ ¹æ®æƒ…æ„Ÿã€è´­ä¹°æ„å›¾å’Œå…´è¶£æ°´å¹³è®¡ç®—åŠ æƒå¾—åˆ†
        3. è¿”å›0-100çš„åˆ†æ•°

        Args:
            sentiment: æƒ…æ„Ÿåˆ†æç»“æœ ('positive', 'neutral', 'negative')
            purchase_intent: æ˜¯å¦æœ‰è´­ä¹°æ„å›¾
            interest_level: å…´è¶£æ°´å¹³ ('high', 'medium', 'low')

        Returns:
            float: å‚ä¸åº¦åˆ†æ•° (0-100)
        """
        try:
            # å‚æ•°éªŒè¯å’Œæ ‡å‡†åŒ–
            if not isinstance(sentiment, str):
                sentiment = str(sentiment).lower()
            else:
                sentiment = sentiment.lower()

            if not isinstance(purchase_intent, bool):
                # å°è¯•è½¬æ¢ä¸ºå¸ƒå°”å€¼
                if isinstance(purchase_intent, str):
                    purchase_intent = purchase_intent.lower() in ['true', '1', 'yes', 't']
                else:
                    purchase_intent = bool(purchase_intent)

            if not isinstance(interest_level, str):
                interest_level = str(interest_level).lower()
            else:
                interest_level = interest_level.lower()

            # ä¿®æ­£æƒ…æ„Ÿæ ‡ç­¾
            if sentiment in ['neg', 'negative']:
                sentiment = 'negative'
            elif sentiment in ['pos', 'positive']:
                sentiment = 'positive'
            elif sentiment not in ['neutral']:
                sentiment = 'neutral'  # é»˜è®¤ä¸ºä¸­æ€§

            # å¤„ç†å…´è¶£æ°´å¹³ä¸­å€¼çš„ä¸åŒè¡¨ç¤º
            if interest_level in ['mid', 'medium']:
                interest_level = 'medium'
            elif interest_level not in ['high', 'low']:
                interest_level = 'low'  # é»˜è®¤ä¸ºä½å…´è¶£

            # 1. æƒ…æ„Ÿè½¬æ¢ (0-1 scale)
            sentiment_score = {
                'positive': 1.0,
                'neutral': 0.5,
                'negative': 0.0
            }.get(sentiment, 0.5)  # é»˜è®¤ä¸ºä¸­æ€§

            # 2. è´­ä¹°æ„å›¾åˆ†æ•°
            intent_score = 1.0 if purchase_intent else 0.0

            # 3. å…´è¶£æ°´å¹³åˆ†æ•°
            interest_score = {
                'high': 1.0,
                'medium': 0.5,
                'low': 0.2
            }.get(interest_level, 0.5)  # é»˜è®¤ä¸ºä¸­ç­‰

            # æ ¹æ®åŠ æƒå¹³å‡è®¡ç®—æ½œåœ¨ä»·å€¼
            potential_value = (
                                      0.3 * sentiment_score +
                                      0.4 * intent_score +
                                      0.3 * interest_score
                              ) * 100  # ç¼©æ”¾åˆ°0-100

            return round(potential_value, 2)

        except Exception as e:
            logger.error(f"è®¡ç®—å‚ä¸åº¦åˆ†æ•°æ—¶å‡ºé”™: {str(e)}")
            # è¿”å›é»˜è®¤å€¼
            return 0.0

    """---------------------------------------------è·å–è§†é¢‘è¯„è®º-----------------------------------------------"""

    async def fetch_video_comments(
            self,
            aweme_id: str,
            ins_filter: bool = False,
            twitter_filter: bool = False,
            region_filter: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„æ¸…ç†åçš„è¯„è®ºæ•°æ®

        Args:
            aweme_id: è§†é¢‘ID
            ins_filter: æ˜¯å¦è¿‡æ»¤Instagramä¸ºNoneçš„è¯„è®º
            twitter_filter: æ˜¯å¦è¿‡æ»¤Twitterä¸ºNoneçš„è¯„è®º
            region_filter: è¯„è®ºåŒºåŸŸè¿‡æ»¤å™¨ï¼Œä¾‹å¦‚"US"ï¼Œ"GB"ç­‰

        Returns:
            Dict[str, Any]: æ¸…ç†åçš„è¯„è®ºæ•°æ®ï¼ŒåŒ…å«è§†é¢‘IDå’Œè¯„è®ºåˆ—è¡¨

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
        """
        start_time = time.time()
        processing_time = 0
        comments = []
        total_comments = 0

        try:
            # éªŒè¯è¾“å…¥å‚æ•°
            if not aweme_id or not isinstance(aweme_id, str):
                raise ValidationError(detail="aweme_idå¿…é¡»æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²", field="aweme_id")

            # è®°å½•å¼€å§‹è·å–è¯„è®º
            logger.info(f"ğŸ” å¼€å§‹è·å–è§†é¢‘ {aweme_id} çš„è¯„è®º")

            # è·å–è¯„è®º
            async for comment_batch in self.comment_collector.stream_video_comments(aweme_id):
                # å¯¹æ¯æ‰¹è¯„è®ºè¿›è¡Œæ¸…æ´—
                cleaned_comments = await self.comment_cleaner.clean_video_comments(comment_batch)

                # è½¬æ¢ä¸ºDataFrameä¾¿äºå¤„ç†
                comments_df = pd.DataFrame(cleaned_comments)

                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if ins_filter:
                    comments_df = comments_df[comments_df['ins_id'] != '']
                if twitter_filter:
                    comments_df = comments_df[comments_df['twitter_id'] != '']
                if region_filter:
                    comments_df = comments_df[comments_df['commenter_region'] == region_filter]

                # è®¡ç®—å¤„ç†æ—¶é—´
                processing_time = round((time.time() - start_time) * 1000, 2)
                total_comments += len(comments_df)

                comments.extend(comments_df.to_dict(orient='records'))

                yield {
                    'aweme_id': aweme_id,
                    'is_complete': False,
                    'current_batch_count': len(comments_df),
                    'current_batch_comments': comments_df.to_dict(orient='records'),
                    'comments': comments,
                    'timestamp': datetime.now().isoformat(),
                    'processing_time': processing_time
                }

            # è®°å½•è·å–è¯„è®ºç»“æŸ
            yield {
                'aweme_id': aweme_id,
                'is_complete': True,
                'total_comments': total_comments,
                'comments': comments,
                'timestamp': datetime.now().isoformat(),
                'processing_time': processing_time
            }
        except Exception as e:
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            yield {
                'aweme_id': aweme_id,
                'error': str(e),
                'total_comments': total_comments,
                'comments': comments,
                'timestamp': datetime.now().isoformat(),
                'processing_time': processing_time
            }

    """---------------------------------------------è·å–è´­ä¹°æ„æ„¿å®¢æˆ·ä¿¡æ¯-----------------------------------------"""

    async def stream_potential_customers(
            self,
            aweme_id: str,
            customer_count: int = 100,
            min_score: float = 50.0,
            max_score: float = 100.0,
            ins_filter: bool = False,
            twitter_filter: bool = False,
            region_filter: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼è·å–æ½œåœ¨å®¢æˆ·

        æ­¥éª¤:
        1. æµå¼è·å–è§†é¢‘è¯„è®º
        2. æ‰¹é‡åˆ†æè¯„è®ºï¼Œè¯†åˆ«æ½œåœ¨å®¢æˆ·
        3. å®æ—¶äº§å‡ºåˆ†æç»“æœ

        Args:
            aweme_id: è§†é¢‘ID
            batch_size: æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡
            customer_count: æœ€å¤§æ½œåœ¨å®¢æˆ·æ•°é‡
            concurrency: AIåˆ†æçš„å¹¶å‘æ•°
            min_score: æœ€å°å‚ä¸åº¦åˆ†æ•°
            max_score: æœ€å¤§å‚ä¸åº¦åˆ†æ•°
            ins_filter: æ˜¯å¦è¿‡æ»¤Instagramä¸ºNoneçš„è¯„è®º
            twitter_filter: æ˜¯å¦è¿‡æ»¤Twitterä¸ºNoneçš„è¯„è®º
            region_filter: è¯„è®ºåŒºåŸŸè¿‡æ»¤å™¨

        Yields:
            Dict[str, Any]: æ¯æ‰¹æ½œåœ¨å®¢æˆ·ä¿¡æ¯

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
        """
        start_time = time.time()
        potential_customers = []  # ä¸´æ—¶å­˜å‚¨åˆ†æç»“æœ

        try:
            # éªŒè¯è¾“å…¥å‚æ•°
            if not aweme_id or not isinstance(aweme_id, str):
                raise ValidationError(detail="aweme_idå¿…é¡»æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²", field="aweme_id")

            logger.info(f"å¼€å§‹æµå¼è·å–è§†é¢‘ {aweme_id} çš„æ½œåœ¨å®¢æˆ·")

            # æµå¼è·å–è¯„è®º
            async for comments_batch in self.comment_collector.stream_video_comments(aweme_id):
                # å¦‚æœå·²ç»è¾¾åˆ°ç›®æ ‡å®¢æˆ·æ•°é‡ï¼Œåˆ™åœæ­¢å¤„ç†
                if len(potential_customers) >= customer_count:
                    logger.info(f"å·²è¾¾åˆ°ç›®æ ‡å®¢æˆ·æ•°é‡ {customer_count}ï¼Œåœæ­¢å¤„ç†")
                    break
                # æ¸…æ´—è¯„è®ºæ‰¹æ¬¡
                cleaned_batch = await self.comment_cleaner.clean_video_comments(comments_batch)

                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if cleaned_batch:
                    batch_df = pd.DataFrame(cleaned_batch)
                    if ins_filter and 'ins_id' in batch_df.columns:
                        batch_df = batch_df[batch_df['ins_id'] != '']
                    if twitter_filter and 'twitter_id' in batch_df.columns:
                        batch_df = batch_df[batch_df['twitter_id'] != '']
                    if region_filter and 'commenter_region' in batch_df.columns:
                        batch_df = batch_df[batch_df['commenter_region'] == region_filter]
                    if batch_df.empty:
                        logger.warning(f"è¯„è®ºæ‰¹æ¬¡ä¸ºç©ºæˆ–è¢«è¿‡æ»¤ï¼Œè·³è¿‡å¤„ç†")
                        continue

                # å‡†å¤‡åˆ†ææ•°æ®
                analysis_data = [
                    {'text': comment.get('text', ''), 'comment_id': comment.get('comment_id', '')}
                    for comment in cleaned_batch
                ]
                logger.info(f"å‡†å¤‡åˆ†æè¯„è®ºæ‰¹æ¬¡: {len(analysis_data)} æ¡è¯„è®º")

                analysis_results = await self._analyze_aspect('purchase_intent', analysis_data)

                if analysis_results:
                    # å°†åˆ†æç»“æœä¸åŸå§‹è¯„è®ºåˆå¹¶
                    result_df = pd.DataFrame(analysis_results)
                    batch_df = pd.DataFrame(cleaned_batch)

                    if not result_df.empty and 'comment_id' in result_df.columns:
                        # åˆå¹¶åˆ†æç»“æœ
                        merged_df = pd.merge(
                            batch_df,
                            result_df,
                            on='comment_id',
                            how='inner',
                            suffixes=('', '_analysis')
                        )

                        # è¿‡æ»¤æ— æ•ˆè¯„è®º
                        merged_df = merged_df.drop_duplicates('commenter_uniqueId')
                        logger.info(f"åˆå¹¶åˆ†æç»“æœ: {len(merged_df)} æ¡è¯„è®º")

                        # è®¡ç®—å‚ä¸åº¦åˆ†æ•°
                        merged_df['engagement_score'] = merged_df.apply(
                            lambda row: self._calculate_engagement_score(
                                row.get('sentiment', 'neutral'),
                                row.get('purchase_intent', False),
                                row.get('interest_level', 'low')
                            ),
                            axis=1
                        )

                        # è¿‡æ»¤ç¬¦åˆåˆ†æ•°èŒƒå›´çš„å®¢æˆ·
                        filtered_df = merged_df[
                            (merged_df['engagement_score'] >= min_score) &
                            (merged_df['engagement_score'] <= max_score)
                            ]

                        filtered_list = filtered_df.to_dict('records')

                        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å®¢æˆ·é™åˆ¶å¹¶æˆªæ–­
                        remaining = customer_count - self.total_customers
                        if len(filtered_list) >= remaining:
                            filtered_list = filtered_list[:remaining]
                            potential_customers.extend(filtered_list)
                            self.total_customers = customer_count
                            self.comment_collector.status = False
                            self.comment_cleaner.status = False
                            logger.info(f"å·²è¾¾åˆ°æœ€å¤§å®¢æˆ·æ•°é‡ {customer_count}ï¼Œåœæ­¢å¤„ç†")
                            yield {
                                'aweme_id': aweme_id,
                                'is_complete': True,
                                'message': f"å·²è¾¾åˆ°æœ€å¤§å®¢æˆ·æ•°é‡ {customer_count}ï¼Œåœæ­¢å¤„ç†",
                                'current_batch_customers': filtered_list,
                                'potential_customers': potential_customers,
                                'customer_count': self.total_customers,
                                'timestamp': datetime.now().isoformat(),
                                'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                            }
                            break
                        else:
                            self.total_customers += len(filtered_df)
                            potential_customers.extend(filtered_list)
                            yield {
                                'aweme_id': aweme_id,
                                'is_complete': False,
                                'message': f"å·²è·å–æ½œåœ¨å®¢æˆ· {self.total_customers} ä¸ª, ç»§ç»­å¤„ç†...",
                                'current_batch_customers': filtered_list,
                                'potential_customers': potential_customers,
                                'customer_count': self.total_customers,
                                'timestamp': datetime.now().isoformat(),
                                'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                            }
            yield {
                'aweme_id': aweme_id,
                'is_complete': True,
                'message': f"å·²å®Œæˆå¤„ç†æ‰€æœ‰è¯„è®ºï¼Œæ€»å…±æ‰¾åˆ° {len(potential_customers)} ä¸ªæ½œåœ¨å®¢æˆ·",
                'potential_customers': potential_customers,
                'customer_count': self.total_customers,
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round((time.time() - start_time) * 1000, 2)
            }

        except (ValidationError, ExternalAPIError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"æµå¼è·å–æ½œåœ¨å®¢æˆ·æ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"æµå¼è·å–æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            yield {
                'aweme_id': aweme_id,
                'error': str(e),
                'message': f"å¤„ç†æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'potential_customers': potential_customers,  # è¿”å›å·²å¤„ç†çš„å®¢æˆ·
                'customer_count': len(potential_customers),
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round((time.time() - start_time) * 1000, 2)
            }

    async def stream_keyword_potential_customers(
            self,
            keyword: str,
            customer_count: int = 100,
            min_score: float = 50.0,
            max_score: float = 100.0,
            ins_filter: bool = False,
            twitter_filter: bool = False,
            region_filter: Optional[str] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼è·å–å…³é”®è¯ç›¸å…³çš„æ½œåœ¨å®¢æˆ·

        æ­¥éª¤:
        1. è·å–ä¸å…³é”®è¯ç›¸å…³çš„è§†é¢‘
        2. å¹¶å‘å¤„ç†å¤šä¸ªè§†é¢‘çš„è¯„è®ºï¼Œè¯†åˆ«æ½œåœ¨å®¢æˆ·
        3. å®æ—¶äº§å‡ºåˆ†æç»“æœ

        Args:
            keyword: å…³é”®è¯
            customer_count: æœ€å¤§æ½œåœ¨å®¢æˆ·æ•°é‡
            video_concurrency: è§†é¢‘å¤„ç†å¹¶å‘æ•°
            min_score: æœ€å°å‚ä¸åº¦åˆ†æ•°
            max_score: æœ€å¤§å‚ä¸åº¦åˆ†æ•°
            ins_filter: æ˜¯å¦è¿‡æ»¤Instagramä¸ºç©ºç”¨æˆ·
            twitter_filter: æ˜¯å¦è¿‡æ»¤Twitterä¸ºç©ºç”¨æˆ·
            region_filter: åœ°åŒºè¿‡æ»¤

        Yields:
            Dict[str, Any]: æ¯æ‰¹æ½œåœ¨å®¢æˆ·ä¿¡æ¯

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        start_time = time.time()
        total_customers = 0
        all_potential_customers = []
        processed_videos = 0

        try:
            # éªŒè¯è¾“å…¥å‚æ•°
            if not keyword or not isinstance(keyword, str):
                raise ValueError("æ— æ•ˆçš„å…³é”®è¯")

            logger.info(f"ğŸ” å¼€å§‹æµå¼è·å–å…³é”®è¯ '{keyword}' ç›¸å…³è§†é¢‘çš„æ½œåœ¨å®¢æˆ·")

            # è·å–æ¸…ç†åçš„è§†é¢‘æ•°æ®
            video_collector = VideoCollector(self.tikhub_api_key)
            video_cleaner = VideoCleaner()
            raw_videos = await video_collector.collect_videos_by_keyword(keyword)
            cleaned_videos = await video_cleaner.clean_videos_by_keyword(raw_videos)

            yield {
                'keyword': keyword,
                'is_complete': False,
                'message': f"å·²æ‰¾åˆ° {len(cleaned_videos.get('videos', []))} ä¸ªä¸å…³é”®è¯ '{keyword}' ç›¸å…³çš„è§†é¢‘",
                'customer_count': 0,
                'potential_customers': [],
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round((time.time() - start_time) * 1000, 2)
            }

            # æå–è§†é¢‘IDåˆ—è¡¨
            videos_df = pd.DataFrame(cleaned_videos.get('videos', []))

            if videos_df.empty:
                logger.warning(f"æœªæ‰¾åˆ°ä¸å…³é”®è¯ '{keyword}' ç›¸å…³çš„è§†é¢‘")
                yield {
                    'keyword': keyword,
                    'message': f"æœªæ‰¾åˆ°ä¸å…³é”®è¯ '{keyword}' ç›¸å…³çš„è§†é¢‘",
                    'potential_customers': [],
                    'customer_count': 0,
                    'timestamp': datetime.now().isoformat()
                }
                return

            aweme_ids = videos_df['aweme_id'].tolist()
            logger.info(f"æ‰¾åˆ°ä¸å…³é”®è¯ '{keyword}' ç›¸å…³çš„ {len(aweme_ids)} ä¸ªè§†é¢‘")

            for aweme_id in aweme_ids:
                if self.total_customers >= customer_count:
                    logger.info(f"å·²è¾¾åˆ°ç›®æ ‡å®¢æˆ·æ•°é‡ {customer_count}ï¼Œåœæ­¢å¤„ç†")
                    break
                async for result in self.stream_potential_customers(
                        aweme_id,
                        customer_count=customer_count,
                        min_score=min_score,
                        max_score=max_score,
                        ins_filter=ins_filter,
                        twitter_filter=twitter_filter,
                        region_filter=region_filter
                ):
                    processed_videos += 1
                    users_list = []
                    if result.get('current_batch_customers'):
                        users_list = result['current_batch_customers']

                    remaining = customer_count - total_customers
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡å®¢æˆ·æ•°é‡
                    if len(users_list) >= remaining:
                        users_list = users_list[:remaining]
                        all_potential_customers.extend(users_list)
                        total_customers = customer_count
                        logger.info(f"å·²è¾¾åˆ°ç›®æ ‡å®¢æˆ·æ•°é‡ {customer_count}ï¼Œåœæ­¢å¤„ç†")
                        yield {
                            'keyword': keyword,
                            'is_complete': True,
                            'message': f"å·²è¾¾åˆ°ç›®æ ‡å®¢æˆ·æ•°é‡ {customer_count}ï¼Œåœæ­¢å¤„ç†",
                            'aweme_id': result.get('aweme_id', ''),
                            'customer_count': total_customers,
                            'potential_customers': all_potential_customers,
                            'timestamp': datetime.now().isoformat(),
                            'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                        }
                        break
                    else:
                        total_customers += len(users_list)
                        all_potential_customers.extend(users_list)
                        yield {
                            'keyword': keyword,
                            'is_complete': False,
                            'message': f"å·²è·å–è§†é¢‘ID {aweme_id} æ½œåœ¨å®¢æˆ· {total_customers} ä¸ª, ç»§ç»­å¤„ç†...",
                            'customer_count': total_customers,
                            'potential_customers': users_list,
                            'timestamp': datetime.now().isoformat(),
                            'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                        }
            yield {
                'keyword': keyword,
                'is_complete': True,
                'message': f"å·²å®Œæˆå¤„ç†æ‰€æœ‰è§†é¢‘ï¼Œæ€»å…±æ‰¾åˆ° {len(all_potential_customers)} ä¸ªæ½œåœ¨å®¢æˆ·",
                'customer_count': total_customers,
                'potential_customers': all_potential_customers,
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round((time.time() - start_time) * 1000, 2)
            }
        except Exception as e:
            logger.error(f"æµå¼è·å–å…³é”®è¯ç›¸å…³æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            yield {
                'keyword': keyword,
                'error': str(e),
                'message': f"å¤„ç†å…³é”®è¯ç›¸å…³æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'potential_customers': all_potential_customers,
                'customer_count': total_customers,
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round((time.time() - start_time) * 1000, 2),
                'is_complete': False
            }

    """---------------------------------------------è·å–è´­ä¹°æ„æ„¿æŠ¥å‘Š-----------------------------------------"""

    async def fetch_purchase_intent_analysis(
            self,
            aweme_id: str,
            batch_size: int = 30,
            concurrency: int = 5
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): aiå¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“è°ƒç”¨å¤–éƒ¨æœåŠ¡å‡ºé”™æ—¶
            InternalServerError: å½“å†…éƒ¨å¤„ç†å‡ºé”™æ—¶
        """

        start_time = time.time()
        comments = []
        results = []
        analysis_summary = {}
        total_collected_comments = 0
        total_analyzed_comments = 0

        try:
            # è¾“å…¥éªŒè¯
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            if batch_size <= 0 or batch_size > settings.MAX_BATCH_SIZE:
                raise ValidationError(
                    detail=f"batch_sizeå¿…é¡»åœ¨1å’Œ{settings.MAX_BATCH_SIZE}ä¹‹é—´",
                    field="batch_size"
                )

            # æµå¼è·å–è¯„è®º
            async for comments_batch in self.fetch_video_comments(aweme_id):
                if 'error' not in comments_batch and not comments_batch['is_complete']:
                    comments.append(comments_batch['current_batch_comments'])
                    total_collected_comments += comments_batch['current_batch_count']
                else:
                    comments = comments_batch.get('comments', [])
                    total_collected_comments = comments_batch.get('total_comments', 0)

                yield {
                    'aweme_id': aweme_id,
                    'is_complete': False,
                    'total_collected_comments': total_collected_comments,
                    'total_analyzed_comments': total_analyzed_comments,
                    'analysis_summary': analysis_summary,
                    'message': f"æ­£åœ¨è·å–è¯„è®º: {total_collected_comments} æ¡",
                    'timestamp': comments_batch.get('timestamp', datetime.now().isoformat())
                }

            # æ•°æ®éªŒè¯
            if len(comments) == 0:
                raise ValidationError(detail="è·å–åˆ°çš„è¯„è®ºæ•°æ®ä¸ºç©º", field="comments")

            comments_df = pd.DataFrame(comments)

            if 'text' not in comments_df.columns:
                raise ValidationError(detail="è¯„è®ºæ•°æ®å¿…é¡»åŒ…å«'text'åˆ—", field="comments")

            # éªŒè¯å’Œè°ƒæ•´æ‰¹å¤„ç†å‚æ•°
            batch_size = min(batch_size, settings.MAX_BATCH_SIZE)
            concurrency = min(concurrency, 10)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º10

            # åˆ†æ‰¹å¤„ç†DataFrame
            num_splits = max(1, len(comments_df) // batch_size + (1 if len(comments_df) % batch_size > 0 else 0))
            comment_batches = np.array_split(comments_df, num_splits)

            # é¿å…ç©ºæ‰¹æ¬¡
            comment_batches = [batch for batch in comment_batches if not batch.empty]

            if not comment_batches:
                raise InternalServerError("åˆ†å‰²åçš„æ‰¹æ¬¡æ•°æ®ä¸ºç©º")

            avg_batch_size = len(comment_batches[0]) if comment_batches else 0

            # é€šçŸ¥å¼€å§‹åˆ†æ
            yield {
                'aweme_id': aweme_id,
                'is_complete': False,
                'total_collected_comments': total_collected_comments,
                'total_analyzed_comments': total_analyzed_comments,
                'analysis_summary': analysis_summary,
                'message': f"å¼€å§‹è´­ä¹°æ„å›¾åˆ†æï¼Œå…± {len(comment_batches)} æ‰¹ï¼Œæ¯æ‰¹çº¦ {avg_batch_size} æ¡è¯„è®º",
                'timestamp': datetime.now().isoformat()
            }

            logger.info(
                f"ğŸš€ å¼€å§‹è´­ä¹°æ„å›¾åˆ†æï¼Œå…± {len(comment_batches)} æ‰¹ï¼Œæ¯æ‰¹çº¦ {avg_batch_size} æ¡è¯„è®º"
            )

            # æ‰¹æ¬¡å¤„ç†
            for i in range(0, len(comment_batches), concurrency):
                batch_group = comment_batches[i:i + concurrency]

                # è®°å½•å½“å‰å¹¶å‘ç»„çš„èŒƒå›´
                batch_indices = [
                    f"{batch.index[0]}-{batch.index[-1]}" for batch in batch_group
                ]
                logger.info(
                    f"âš¡ å¤„ç†æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)}ï¼Œè¯„è®ºç´¢å¼•èŒƒå›´: {batch_indices}"
                )

                # å¹¶å‘æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡
                tasks = [
                    self._analyze_aspect('purchase_intent', batch[['text', 'comment_id']].to_dict('records'))
                    for batch in batch_group
                ]

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # å¤„ç†ç»“æœï¼Œè¿‡æ»¤æ‰å¼‚å¸¸
                valid_results = []
                error_count = 0

                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        error_msg = f"æ‰¹æ¬¡ {i + j + 1} åˆ†æå¤±è´¥: {str(result)}"
                        logger.error(error_msg)
                        error_count += 1
                    else:
                        valid_results.append(result)

                # åªåœ¨æœ‰é”™è¯¯æ—¶æ‰å‘é€é”™è¯¯è¿›åº¦æ›´æ–°
                if error_count > 0:
                    yield {
                        'aweme_id': aweme_id,
                        'is_complete': False,
                        'total_collected_comments': total_collected_comments,
                        'total_analyzed_comments': len(results),
                        'analysis_summary': analysis_summary,
                        'message': f"æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)} ä¸­æœ‰ {error_count} ä¸ªæ‰¹æ¬¡åˆ†æå¤±è´¥",
                        'timestamp': datetime.now().isoformat()
                    }

                # æ·»åŠ æœ‰æ•ˆç»“æœ
                results.extend(valid_results)

                # å‘é€è¿›åº¦æ›´æ–°
                yield {
                    'aweme_id': aweme_id,
                    'is_complete': False,
                    'total_collected_comments': total_collected_comments,
                    'total_analyzed_comments': len(results),
                    'analysis_summary': analysis_summary,
                    'message': f"å·²åˆ†æ {len(results)} æ¡è¯„è®ºï¼Œå®Œæˆåº¦ {i*concurrency/len(comment_batches)}%",
                    'timestamp': datetime.now().isoformat()
                }

            # åˆå¹¶æ‰€æœ‰åˆ†æç»“æœ
            try:
                # å°†æ‰€æœ‰ç»“æœæ‰å¹³åŒ–ä¸ºå•ä¸ªåˆ—è¡¨
                all_results = []
                for batch_result in results:
                    if isinstance(batch_result, list):
                        all_results.extend(batch_result)

                # åˆ›å»ºç»“æœDataFrame
                if not all_results:
                    raise InternalServerError("æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ")

                analysis_df = pd.DataFrame(all_results)

                # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
                if 'comment_id' not in analysis_df.columns:
                    logger.warning("åˆ†æç»“æœç¼ºå°‘comment_idåˆ—ï¼Œä½¿ç”¨ç´¢å¼•åˆå¹¶")
                    analysis_df['temp_index'] = range(len(analysis_df))
                    comments_df['temp_index'] = range(min(len(comments_df), len(analysis_df)))
                    merged_df = pd.merge(comments_df, analysis_df, on='temp_index', how='left')
                    merged_df = merged_df.drop('temp_index', axis=1)
                else:
                    # åŸºäºcomment_idåˆå¹¶
                    merged_df = pd.merge(comments_df, analysis_df, on='comment_id', how='left')

                # å¤„ç†é‡å¤çš„textåˆ—
                if 'text_y' in merged_df.columns:
                    merged_df = merged_df.drop('text_y', axis=1)
                    merged_df = merged_df.rename(columns={'text_x': 'text'})

                logger.info(f"âœ… æ‰€æœ‰è´­ä¹°æ„å‘åˆ†æå®Œæˆï¼æ€»è®¡ {len(merged_df)} æ¡æ•°æ®")
                yield {
                    'aweme_id': aweme_id,
                    'is_complete': False,
                    'total_collected_comments': total_collected_comments,
                    'total_analyzed_comments': len(merged_df),
                    'analysis_summary': analysis_summary,
                    'message': "æ‰€æœ‰è´­ä¹°æ„å‘åˆ†æå®Œæˆ, æ­£åœ¨åˆå¹¶ç»“æœï¼Œå‡†å¤‡ç”ŸæˆæŠ¥å‘Šï¼Œè¯·ç¨å€™...",
                    'timestamp': datetime.now().isoformat()
                }

            except Exception as e:
                error_msg = f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}"
                logger.error(error_msg)
                raise InternalServerError(error_msg)

            if merged_df.empty:
                raise InternalServerError(f"åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå¤±è´¥ï¼Œç»“æœä¸ºç©º")

            # æ ¹æ®commenter_uniqueIdå»é‡
            analyzed_df = merged_df.drop_duplicates(subset=['commenter_uniqueId'])

            # ç”Ÿæˆåˆ†ææ‘˜è¦
            analysis_summary = {
                'sentiment_distribution': self._analyze_sentiment_distribution(analyzed_df),
                'purchase_intent': self._analyze_purchase_intent(analyzed_df),
                'interest_levels': self._analyze_interest_levels(analyzed_df),
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'purchase_intent_stats',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }

            # ç”ŸæˆæŠ¥å‘Š
            report_url = await self.generate_analysis_report(aweme_id, 'purchase_intent_report', analysis_summary)
            analysis_summary['report_url'] = report_url

            # è¿”å›æœ€ç»ˆç»“æœ
            yield {
                'aweme_id': aweme_id,
                'is_complete': True,
                'total_collected_comments': total_collected_comments,
                'total_analyzed_comments': len(analyzed_df),
                'analysis_summary': analysis_summary,
                'message': "è´­ä¹°æ„å›¾åˆ†æå®Œæˆ",
                'timestamp': datetime.now().isoformat()
            }

        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé¢„æœŸé”™è¯¯: {str(e)}")
            yield {
                'aweme_id': aweme_id,
                'is_complete': True,
                'error': str(e),
                'total_collected_comments': total_collected_comments,
                'total_analyzed_comments': len(results) if 'results' in locals() else 0,
                'analysis_summary': analysis_summary,
                'message': f"è´­ä¹°æ„å›¾åˆ†æå¤±è´¥: {str(e)}",
                'timestamp': datetime.now().isoformat()
            }
            return  # ç¡®ä¿ç”Ÿæˆå™¨åœ¨è¿”å›é”™è¯¯ååœæ­¢
        except Exception as e:
            # å¤„ç†æœªé¢„æœŸçš„é”™è¯¯
            error_msg = f"è·å–è´­ä¹°æ„å›¾ç»Ÿè®¡æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}"
            logger.error(error_msg)
            yield {
                'aweme_id': aweme_id,
                'is_complete': True,
                'error': str(e),
                'total_collected_comments': total_collected_comments,
                'total_analyzed_comments': len(results) if 'results' in locals() else 0,
                'analysis_summary': analysis_summary,
                'message': f"è´­ä¹°æ„å›¾åˆ†æå¤±è´¥: {str(e)}",
                'timestamp': datetime.now().isoformat()
            }
            return  # ç¡®ä¿ç”Ÿæˆå™¨åœ¨è¿”å›é”™è¯¯ååœæ­¢

    def _analyze_sentiment_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ

        Args:
            df (pd.DataFrame): åŒ…å«sentimentåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ")
            if 'sentiment' not in df.columns:
                raise ValueError("DataFrameå¿…é¡»åŒ…å«'sentiment'åˆ—")

            # æ ‡å‡†åŒ–æƒ…æ„Ÿå€¼
            df['sentiment'] = df['sentiment'].str.lower()
            df.loc[df['sentiment'] == 'neg', 'sentiment'] = 'negative'
            df.loc[df['sentiment'] == 'pos', 'sentiment'] = 'positive'

            sentiment_counts = df['sentiment'].value_counts()
            return {
                'counts': sentiment_counts.to_dict(),
                'percentages': (sentiment_counts / len(df) * 100).round(2).to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†ææƒ…æ„Ÿåˆ†å¸ƒæ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'counts': {},
                'percentages': {}
            }

    def _analyze_purchase_intent(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æè´­ä¹°æ„å›¾

        Args:
            df (pd.DataFrame): åŒ…å«purchase_intentå’Œinterest_levelåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†æè´­ä¹°æ„å›¾")
            required_columns = ['purchase_intent', 'interest_level']
            for col in required_columns:
                if col not in df.columns:
                    raise ValueError(f"DataFrameå¿…é¡»åŒ…å«'{col}'åˆ—")

            # ç¡®ä¿purchase_intentæ˜¯å¸ƒå°”å€¼
            if df['purchase_intent'].dtype != bool:
                df['purchase_intent'] = df['purchase_intent'].apply(
                    lambda x: x if isinstance(x, bool) else (
                            str(x).lower() in ['true', 'yes', '1', 't']
                    )
                )

            # æ ‡å‡†åŒ–interest_level
            df['interest_level'] = df['interest_level'].str.lower()
            df.loc[df['interest_level'] == 'mid', 'interest_level'] = 'medium'

            intent_df = df[df['purchase_intent'] == True]

            return {
                'total_comments': len(df),
                'intent_count': len(intent_df),
                'intent_rate': round(len(intent_df) / len(df) * 100, 2) if len(df) > 0 else 0,
                'intent_by_interest_level': intent_df['interest_level'].value_counts().to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†æè´­ä¹°æ„å›¾æ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'total_comments': 0,
                'intent_count': 0,
                'intent_rate': 0,
                'intent_by_interest_level': {}
            }

    def _analyze_interest_levels(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æå…´è¶£æ°´å¹³

        Args:
            df (pd.DataFrame): åŒ…å«interest_levelåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: å…´è¶£æ°´å¹³ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†æå…´è¶£æ°´å¹³")
            if 'interest_level' not in df.columns:
                raise ValueError("DataFrameå¿…é¡»åŒ…å«'interest_level'åˆ—")

            # æ ‡å‡†åŒ–interest_level
            df['interest_level'] = df['interest_level'].str.lower()
            df.loc[df['interest_level'] == 'mid', 'interest_level'] = 'medium'

            interest_counts = df['interest_level'].value_counts()
            return {
                'counts': interest_counts.to_dict(),
                'percentages': (interest_counts / len(df) * 100).round(2).to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†æå…´è¶£æ°´å¹³æ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'counts': {},
                'percentages': {}
            }

    """---------------------------------------------ç”Ÿæˆå›å¤æ¶ˆæ¯-----------------------------------------"""

    async def generate_single_reply_message(
            self,
            shop_info: str,
            customer_id: str,
            customer_message: str,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯

        Args:
            shop_info (str): åº—é“ºä¿¡æ¯
            customer_id (str): å®¢æˆ·uniqueID
            customer_message (str): å®¢æˆ·æ¶ˆæ¯
        Returns:
            Dict[str, Any]: ç”Ÿæˆçš„å›å¤æ¶ˆæ¯

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        start_time = time.time()
        reply_message = ""
        try:
            # å‚æ•°éªŒè¯
            if not customer_message:
                raise ValueError("customer_messageä¸èƒ½ä¸ºç©º")

            sys_prompt = self.system_prompts['customer_reply']
            user_prompt = f"Here is the shop information:\n{shop_info}\n\nHere is the customer message:\n{customer_message},\n\nPlease generate a reply message for the customer."

            yield {
                'customer_id': customer_id,
                'is_complete': False,
                'reply_message': reply_message,
                'message': "å¼€å§‹ç”Ÿæˆå›å¤æ¶ˆæ¯",
                'timestamp': datetime.now().isoformat()
            }
            # ç”Ÿæˆå›å¤æ¶ˆæ¯
            reply_message = await self.chatgpt.chat(
                system_prompt=sys_prompt,
                user_prompt=user_prompt,
                temperature=0.7,
            )

            # è§£æå›å¤æ¶ˆæ¯
            reply_message = reply_message["choices"][0]["message"]["content"].strip()
            # è§£æjson
            reply_message = re.sub(
                r"```json\n|\n```",
                "",
                reply_message.strip()
            )  # å»é™¤Markdownä»£ç å—

            reply_message = json.loads(reply_message)

            yield {
                'customer_id': customer_id,
                'is_complete': True,
                'reply_message': reply_message,
                'message': "å›å¤æ¶ˆæ¯ç”Ÿæˆå®Œæˆ",
                'timestamp': datetime.now().isoformat(),
                'processing_time': round((time.time() - start_time) * 1000, 2)
            }
        except (ValueError, RuntimeError) as e:
            logger.error(f"ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                'customer_id': customer_id,
                'is_complete': True,
                'error': str(e),
                'reply_message': reply_message,
                'message': f"ç”Ÿæˆå›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'timestamp': datetime.now().isoformat(),
                'processing_time': round((time.time() - start_time) * 1000, 2)
            }
            raise
        except Exception as e:
            logger.error(f"ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            yield {
                'customer_id': customer_id,
                'is_complete': True,
                'error': str(e),
                'reply_message': reply_message,
                'message': f"ç”Ÿæˆå›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'timestamp': datetime.now().isoformat(),
                'processing_time': round((time.time() - start_time) * 1000, 2)
            }
            return

    async def generate_batch_reply_messages(
            self,
            shop_info: str,
            customer_messages: Dict[str, str],
            batch_size: int = 5
    ) -> AsyncGenerator[List[Dict[str, Any]], None]:
        """
        æ‰¹é‡ç”Ÿæˆå®¢æˆ·å›å¤æ¶ˆæ¯

        Args:
            shop_info (str): åº—é“ºä¿¡æ¯
            customer_messages (Dict[str, str]): å®¢æˆ·æ¶ˆæ¯å­—å…¸, é”®ä¸ºç”¨æˆ·ID, å€¼ä¸ºæ¶ˆæ¯å†…å®¹
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„å®¢æˆ·æ¶ˆæ¯æ•°é‡. é»˜è®¤ä¸º5.

        Returns:
            List[Dict[str, Any]]: ç”Ÿæˆçš„å›å¤æ¶ˆæ¯åˆ—è¡¨, æ¯ä¸ªå›å¤åŒ…å«è¯­è¨€æ£€æµ‹å’Œå›å¤å†…å®¹

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        all_replies = []
        total_replies_count = 0
        try:
            # å‚æ•°éªŒè¯
            if not shop_info:
                raise ValueError("åº—é“ºä¿¡æ¯ä¸èƒ½ä¸ºç©º")

            if not customer_messages:
                raise ValueError("å®¢æˆ·æ¶ˆæ¯å­—å…¸ä¸èƒ½ä¸ºç©º")

            # è½¬æ¢å­—å…¸ä¸ºåˆ—è¡¨æ ¼å¼
            messages_list = [{"commenter_uniqueId": uid, "text": text} for uid, text in customer_messages.items()]

            logger.info("å¼€å§‹æ‰¹é‡ç”Ÿæˆå®¢æˆ·å›å¤æ¶ˆæ¯")
            yield {
                'is_complete': False,
                'message': "å¼€å§‹æ‰¹é‡ç”Ÿæˆå®¢æˆ·å›å¤æ¶ˆæ¯",
                'replies': all_replies,
                'total_replies_count': total_replies_count,
                'timestamp': datetime.now().isoformat()
            }


            # æŒ‰æ‰¹æ¬¡å¤„ç†æ¶ˆæ¯
            for i in range(0, len(messages_list), batch_size):
                batch = messages_list[i:i + batch_size]

                # å°†å­—å…¸è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                user_prompt = f"here is the shop information:\n{shop_info}\n\nhere are the customer messages:\n{json.dumps(batch, ensure_ascii=False)}"

                # è°ƒç”¨AIç”Ÿæˆå›å¤
                batch_replies = await self.chatgpt.chat(
                    system_prompt=self.system_prompts['batch_customer_reply'],
                    user_prompt=user_prompt,
                    temperature=0.7
                )

                # è§£æAIå›å¤
                batch_replies = batch_replies["choices"][0]["message"]["content"].strip()
                # è§£æJSON
                batch_replies = re.sub(
                    r"```json\n|\n```",
                    "",
                    batch_replies.strip()
                )
                # è§£æå›å¤ç»“æœ
                try:
                    parsed_replies = json.loads(batch_replies)

                    # éªŒè¯å›å¤æ ¼å¼
                    if not isinstance(parsed_replies, list):
                        raise ValueError("AIè¿”å›çš„ç»“æœæ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºåˆ—è¡¨")

                    # å°†æ‰¹æ¬¡å›å¤æ·»åŠ åˆ°æ€»ç»“æœä¸­
                    for reply in parsed_replies:
                        # ç¡®ä¿uniqueIDåœ¨å›å¤ä¸­
                        message_id = reply.get("message_id")
                        if message_id is not None and 0 <= message_id < len(batch):
                            reply["commenter_uniqueId"] = batch[message_id].get("commenter_uniqueId")

                        all_replies.append(reply)
                        total_replies_count += 1

                    yield {
                        'is_complete': False,
                        'message': f"å·²ç”Ÿæˆ {len(all_replies)} æ¡å›å¤æ¶ˆæ¯ï¼Œ å®Œæˆåº¦ {i*batch_size / len(messages_list) * 100:.2f}%",
                        'total_replies_count': total_replies_count,
                        'replies': all_replies,
                        'timestamp': datetime.now().isoformat()
                    }

                except json.JSONDecodeError as json_err:
                    logger.error(f"æ— æ³•è§£æAIè¿”å›çš„JSONç»“æœ: {batch_replies[:200]}... (é”™è¯¯: {str(json_err)})")
                    raise RuntimeError(f"AIè¿”å›çš„ç»“æœä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {str(json_err)}")

            logger.info("æ‰¹é‡ç”Ÿæˆå®¢æˆ·å›å¤æ¶ˆæ¯å®Œæˆ")

            yield {
                'is_complete': True,
                'message': "æ‰¹é‡ç”Ÿæˆå®¢æˆ·å›å¤æ¶ˆæ¯å®Œæˆ",
                'total_replies_count': total_replies_count,
                'replies': all_replies,
                'timestamp': datetime.now().isoformat()
            }

        except (ValueError, RuntimeError) as e:
            logger.error(f"ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                'is_complete': True,
                'error': str(e),
                'message': f"æ‰¹é‡ç”Ÿæˆå›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'total_replies_count': total_replies_count,
                'replies': all_replies,
                'timestamp': datetime.now().isoformat()
            }
            raise
        except Exception as e:
            logger.error(f"ç”Ÿæˆå•æ¡å®¢æˆ·å›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                'is_complete': True,
                'error': str(e),
                'message': f"æ‰¹é‡ç”Ÿæˆå›å¤æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'total_replies_count': total_replies_count,
                'replies': all_replies,
                'timestamp': datetime.now().isoformat()
            }
            return



async def main():
    """æµ‹è¯•æµå¼å…³é”®è¯æ½œåœ¨å®¢æˆ·åŠŸèƒ½"""
    print("å¼€å§‹æµ‹è¯• stream_keyword_potential_customers æ–¹æ³•...")

    # åˆå§‹åŒ– CustomerAgent
    api_key = os.getenv("TIKHUB_API_KEY")
    if not api_key:
        print("é”™è¯¯: æœªè®¾ç½® TIKHUB_API_KEY ç¯å¢ƒå˜é‡")
        return

    agent = CustomerAgent(api_key)

    # æµ‹è¯•å…³é”®è¯åˆ—è¡¨
    keywords = [
        "skincare products",  # æŠ¤è‚¤äº§å“
        #"fitness equipment"  # å¥èº«è®¾å¤‡
    ]

    # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæµ‹è¯•
    for keyword in keywords:
        print(f"\n===== æµ‹è¯•å…³é”®è¯: '{keyword}' =====")

        try:
            start_time = time.time()
            batch_count = 0
            total_customers = 0

            # æµå¼è·å–å…³é”®è¯æ½œåœ¨å®¢æˆ·
            async for result in agent.stream_keyword_potential_customers(
                    keyword=keyword,
                    customer_count=20,  # ç›®æ ‡å®¢æˆ·æ•°é‡
                    min_score=50.0,  # æœ€å°å‚ä¸åº¦åˆ†æ•°
                    max_score=100.0,  # æœ€å¤§å‚ä¸åº¦åˆ†æ•°
                    ins_filter=False,  # ä¸è¿‡æ»¤Instagram
                    twitter_filter=False,  # ä¸è¿‡æ»¤Twitter
                    region_filter=None  # ä¸è¿‡æ»¤åœ°åŒº
            ):
                # æ˜¾ç¤ºæ‰¹æ¬¡ä¿¡æ¯
                if 'is_complete' in result:
                    # è¿™æ˜¯æœ€ç»ˆå®Œæˆçš„ç»“æœ
                    elapsed_time = time.time() - start_time
                    print(f"\nå®Œæˆå¤„ç† - æ€»è®¡ {result['total_customers']} ä¸ªå®¢æˆ·")
                    print(f"å¤„ç†äº† {result.get('videos_processed')} ä¸ªè§†é¢‘ (å…± {result.get('total_videos')} ä¸ª)")
                    print(f"æ€»å¤„ç†æ—¶é—´: {elapsed_time:.2f}ç§’")
                    print(f"APIæŠ¥å‘Šå¤„ç†æ—¶é—´: {result.get('processing_time_ms', 0) / 1000:.2f}ç§’")
                elif 'error' in result:
                    # å‘ç”Ÿé”™è¯¯
                    print(f"\nå¤„ç†å‡ºé”™: {result['error']}")
                elif 'potential_customers' in result:
                    # æ­£å¸¸æ‰¹æ¬¡ç»“æœ
                    batch_count += 1
                    new_customers = len(result.get('potential_customers', []))
                    total_customers = result.get('total_count', total_customers + new_customers)
                    processed_videos = result.get('total_videos_processed', 0)
                    total_videos = result.get('total_videos', 0)

                    print(f"\næ‰¹æ¬¡ {batch_count}: è·å¾— {new_customers} ä¸ªæ–°å®¢æˆ·, ç´¯è®¡: {total_customers}")
                    print(f"æ­£åœ¨å¤„ç†è§†é¢‘ {processed_videos}/{total_videos}")

                    # æ˜¾ç¤ºè¿™æ‰¹å®¢æˆ·çš„ç®€è¦ä¿¡æ¯
                    if new_customers > 0:
                        print("å®¢æˆ·ä¿¡æ¯é¢„è§ˆ:")
                        for i, customer in enumerate(result['potential_customers'][:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
                            print(f"  å®¢æˆ· {i + 1}:")
                            print(f"    ID: {customer.get('commenter_uniqueId', 'N/A')}")
                            print(f"    è¯„è®º: {customer.get('text', 'N/A')[:40]}..." if len(
                                customer.get('text', '')) > 40 else f"    è¯„è®º: {customer.get('text', 'N/A')}")
                            print(f"    å‚ä¸åº¦: {customer.get('engagement_score', 'N/A')}")
                            print(f"    æ¥æºè§†é¢‘: {customer.get('aweme_id', 'N/A')}")

                    # æ¯5ä¸ªæ‰¹æ¬¡æ˜¾ç¤ºä¸€ä¸ªè¿›åº¦æ‘˜è¦
                    if batch_count % 5 == 0:
                        elapsed = time.time() - start_time
                        print(f"\n--- è¿›åº¦æ‘˜è¦ ---")
                        print(f"å·²å¤„ç† {batch_count} æ‰¹æ¬¡ï¼Œè·å¾— {total_customers} ä¸ªå®¢æˆ·")
                        print(f"è€—æ—¶: {elapsed:.2f}ç§’ï¼Œå¹³å‡æ¯æ‰¹æ¬¡ {elapsed / batch_count:.2f}ç§’")
                        print(
                            f"å¹³å‡æ¯å®¢æˆ· {elapsed / total_customers:.2f}ç§’" if total_customers > 0 else "å°šæœªè·å¾—å®¢æˆ·")

        except Exception as e:
            print(f"æµ‹è¯•å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()

    print("\næµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    # è®¾ç½®å¼‚æ­¥äº‹ä»¶å¾ªç¯ç­–ç•¥
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())