# -*- coding: utf-8 -*-
"""
@file: customer_agent.py
@desc: å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€åˆ†æå’Œæ½œåœ¨å®¢æˆ·è¯†åˆ«åŠŸèƒ½
@auth: Callmeiks
"""

import json
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
import asyncio
import time

import numpy as np
import pandas as pd
from dotenv import load_dotenv
import os

from app.utils.logger import setup_logger
from services.ai_models.chatgpt import ChatGPT
from services.ai_models.claude import Claude
from services.cleaner.comment_cleaner import CommentCleaner
from services.crawler.comment_crawler import CommentCollector
from services.crawler.video_crawler import VideoCollector
from services.cleaner.video_cleaner import VideoCleaner
from app.config import settings
from app.core.exceptions import ValidationError, ExternalAPIError, InternalServerError

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = setup_logger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class CustomerAgent:
    """å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€åˆ†æå’Œæ½œåœ¨å®¢æˆ·è¯†åˆ«åŠŸèƒ½"""

    def __init__(self, tikhub_api_key: Optional[str] = None, tikhub_base_url: Optional[str] = None):
        """
        åˆå§‹åŒ–CommentAgentï¼ŒåŠ è½½APIå¯†é’¥å’Œæç¤ºæ¨¡æ¿

        Args:
            tikhub_api_key: TikHub APIå¯†é’¥
            tikhub_base_url: TikHub APIåŸºç¡€URL
        """
        # åˆå§‹åŒ–AIæ¨¡å‹å®¢æˆ·ç«¯
        self.chatgpt = ChatGPT()
        self.claude = Claude()

        # ä¿å­˜TikHub APIé…ç½®
        self.tikhub_api_key = tikhub_api_key
        self.tikhub_base_url = tikhub_base_url

        # å¦‚æœæ²¡æœ‰æä¾›TikHub APIå¯†é’¥ï¼Œè®°å½•è­¦å‘Š
        if not self.tikhub_api_key:
            logger.warning("æœªæä¾›TikHub APIå¯†é’¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

        # æ”¯æŒçš„åˆ†æç±»å‹åˆ—è¡¨
        self.analysis_types = ['purchase_intent']

        # åŠ è½½ç³»ç»Ÿå’Œç”¨æˆ·æç¤º
        self._load_system_prompts()
        self._load_user_prompts()

    def _load_system_prompts(self) -> None:
        """åŠ è½½ç³»ç»Ÿæç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.system_prompts = {
            'purchase_intent': """You are an AI trained to analyze social media comments about products.
            For each comment in the provided list, analyze:
            1. Sentiment (positive, negative, or neutral)
            2. Purchase Intent (whether the user shows any interest in buying, trying, or acquiring the product)
            3. User Interest Level (high, medium, low)

            For each comment, provide analysis in the following JSON format:
            {
                "comment_id": "comment ID from input data",
                "text": "comment text",
                "sentiment": "positive/negative/neutral",
                "purchase_intent": true/false,
                "interest_level": "high/medium/low",
            }

            Respond with a JSON array containing analysis for all comments.
            Focus on identifying any signs of interest in the product, including:
            - Direct purchase intentions ("want to buy", "need this")
            - Indirect interest ("where can I find this", "love this")
            - Questions about product details
            - emojis indicating positive sentiment or interest
            - Positive reactions to product features
            - Any positive sentiment to the influencer herself/himself is consider neutral
            - Any engagement indicating potential future purchase"""
        }

    def _load_user_prompts(self) -> None:
        """åŠ è½½ç”¨æˆ·æç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.user_prompts = {
            'purchase_intent': {
                'description': 'purchase intent'
            }
        }

    async def fetch_video_comments(self, aweme_id: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„æ¸…ç†åçš„è¯„è®ºæ•°æ®

        Args:
            aweme_id (str): è§†é¢‘ID

        Returns:
            Dict[str, Any]: æ¸…ç†åçš„è¯„è®ºæ•°æ®ï¼ŒåŒ…å«è§†é¢‘IDå’Œè¯„è®ºåˆ—è¡¨

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id or not isinstance(aweme_id, str):
                raise ValidationError(detail="aweme_idå¿…é¡»æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²", field="aweme_id")

            # è®°å½•å¼€å§‹è·å–è¯„è®º
            logger.info(f"å¼€å§‹è·å–è§†é¢‘ {aweme_id} çš„è¯„è®º")

            # è·å–è¯„è®º
            comment_collector = CommentCollector(self.tikhub_api_key, self.tikhub_base_url)
            comments = await comment_collector.collect_video_comments(aweme_id)

            if not comments or not comments.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æœªæ‰¾åˆ°è¯„è®º")
                return {
                    'aweme_id': aweme_id,
                    'comments': [],
                    'comment_count': 0,
                    'timestamp': datetime.now().isoformat()
                }

            # æ¸…æ´—è¯„è®º
            comment_cleaner = CommentCleaner()
            cleaned_comments = await comment_cleaner.clean_video_comments(comments)
            cleaned_comments = cleaned_comments.get('comments', [])

            processing_time = time.time() - start_time

            result = {
                'aweme_id': aweme_id,
                'comments': cleaned_comments,
                'comment_count': len(cleaned_comments),
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round(processing_time * 1000, 2)
            }

            logger.info(f"æˆåŠŸè·å–è§†é¢‘ {aweme_id} çš„è¯„è®º: {len(cleaned_comments)} æ¡ï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            return result

        except (ValidationError, ExternalAPIError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‡ºé”™: {str(e)}")
            raise

        except Exception as e:
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def analyze_comments_batch(
            self,
            df: pd.DataFrame,
            analysis_type: str,
            batch_size: int = 30,
            concurrency: int = 5
    ) -> pd.DataFrame:
        """
        åˆ†æ‰¹åˆ†æè¯„è®ºï¼Œä»¥é˜²æ­¢ä¸€æ¬¡æ€§å‘é€è¿‡å¤šæ•°æ®

        Args:
            df (pd.DataFrame): åŒ…å«è¯„è®ºæ•°æ®çš„DataFrameï¼Œéœ€åŒ…å«å­—æ®µï¼štext
            analysis_type (str): é€‰æ‹©åˆ†æç±»å‹ (purchase_intent)
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            pd.DataFrame: ç»“æœåˆå¹¶å›åŸå§‹DataFrame

        Raises:
            ValidationError: å½“analysis_typeæ— æ•ˆæˆ–dfä¸ºç©ºæ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if analysis_type not in self.analysis_types:
                raise ValidationError(
                    detail=f"æ— æ•ˆçš„åˆ†æç±»å‹: {analysis_type}. è¯·ä» {self.analysis_types} ä¸­é€‰æ‹©",
                    field="analysis_type"
                )

            if df.empty:
                raise ValidationError(detail="DataFrameä¸èƒ½ä¸ºç©º", field="df")

            if 'text' not in df.columns:
                raise ValidationError(detail="DataFrameå¿…é¡»åŒ…å«'text'åˆ—", field="df")

            # éªŒè¯å’Œè°ƒæ•´æ‰¹å¤„ç†å‚æ•°
            batch_size = min(batch_size, settings.MAX_BATCH_SIZE)
            concurrency = min(concurrency, 10)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º10

            # åˆ†æ‰¹å¤„ç†DataFrame
            num_splits = max(1, len(df) // batch_size + (1 if len(df) % batch_size > 0 else 0))
            comment_batches = np.array_split(df, num_splits)
            logger.info(
                f"ğŸš€ å¼€å§‹ {analysis_type} åˆ†æï¼Œå…± {len(comment_batches)} æ‰¹ï¼Œæ¯æ‰¹çº¦ {len(comment_batches[0]) if len(comment_batches) > 0 else 0} æ¡è¯„è®º"
            )

            # å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼ˆæ¯æ¬¡æœ€å¤š `concurrency` ç»„ï¼‰
            results = []
            for i in range(0, len(comment_batches), concurrency):
                batch_group = comment_batches[i:i + concurrency]

                # è®°å½•å½“å‰å¹¶å‘ç»„çš„èŒƒå›´
                batch_indices = [
                    f"{batch.index[0]}-{batch.index[-1]}" if not batch.empty else "-"
                    for batch in batch_group
                ]
                logger.info(
                    f"âš¡ å¤„ç†æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)}ï¼Œè¯„è®ºç´¢å¼•èŒƒå›´: {batch_indices}"
                )

                # å¹¶å‘æ‰§è¡Œ `concurrency` ä¸ªä»»åŠ¡
                tasks = [
                    self._analyze_aspect(analysis_type, batch[['text', 'comment_id']].to_dict('records'))
                    for batch in batch_group if not batch.empty
                ]

                if not tasks:
                    continue

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # å¤„ç†ç»“æœï¼Œè¿‡æ»¤æ‰å¼‚å¸¸
                valid_results = []
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"æ‰¹æ¬¡ {i + j + 1} åˆ†æå¤±è´¥: {str(result)}")
                    else:
                        valid_results.append(result)

                if len(valid_results) != len(batch_group):
                    logger.warning(
                        f"æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)} ä¸­æœ‰ {len(batch_group) - len(valid_results)} ä¸ªæ‰¹æ¬¡åˆ†æå¤±è´¥"
                    )

                results.extend(valid_results)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
            if not results:
                raise InternalServerError("æ‰€æœ‰æ‰¹æ¬¡åˆ†æå‡å¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆç»“æœ")

            # åˆå¹¶æ‰€æœ‰åˆ†æç»“æœ
            try:
                # å°†æ‰€æœ‰ç»“æœæ‰å¹³åŒ–ä¸ºå•ä¸ªåˆ—è¡¨
                all_results = []
                for batch_result in results:
                    if isinstance(batch_result, list):
                        all_results.extend(batch_result)

                # åˆ›å»ºç»“æœDataFrame
                analysis_df = pd.DataFrame(all_results)

                # ç¡®ä¿comment_idåˆ—å­˜åœ¨
                if 'comment_id' not in analysis_df.columns:
                    logger.warning("åˆ†æç»“æœç¼ºå°‘comment_idåˆ—ï¼Œæ— æ³•æ­£ç¡®åˆå¹¶")
                    # æ·»åŠ ç´¢å¼•ä½œä¸ºä¸´æ—¶åˆ—
                    analysis_df['temp_index'] = range(len(analysis_df))
                    df['temp_index'] = range(len(df))
                    # åŸºäºç´¢å¼•åˆå¹¶
                    merged_df = pd.merge(df, analysis_df, on='temp_index', how='left')
                    merged_df = merged_df.drop('temp_index', axis=1)
                else:
                    # åŸºäºcomment_idåˆå¹¶
                    merged_df = pd.merge(df, analysis_df, on='comment_id', how='left')

                # å¤„ç†é‡å¤çš„textåˆ—
                if 'text_y' in merged_df.columns:
                    merged_df = merged_df.drop('text_y', axis=1)
                    merged_df = merged_df.rename(columns={'text_x': 'text'})

                logger.info(f"âœ… {analysis_type} åˆ†æå®Œæˆï¼æ€»è®¡ {len(merged_df)} æ¡æ•°æ®")
                return merged_df

            except Exception as e:
                logger.error(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
                raise InternalServerError(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except InternalServerError:
            # ç›´æ¥å‘ä¸Šä¼ é€’å†…éƒ¨æœåŠ¡å™¨é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def _analyze_aspect(
            self,
            aspect_type: str,
            comment_data: List[Dict[str, Any]],
    ) -> Optional[List[Dict[str, Any]]]:
        """
        é€šç”¨åˆ†ææ–¹æ³•ï¼Œæ ¹æ®ä¸åŒçš„åˆ†æç±»å‹è°ƒç”¨ChatGPTæˆ–Claude AIæ¨¡å‹ã€‚

        Args:
            aspect_type (str): éœ€è¦åˆ†æçš„ç±»å‹ (purchase_intent)
            comment_data (List[Dict[str, Any]]): éœ€è¦åˆ†æçš„è¯„è®ºåˆ—è¡¨

        Returns:
            Optional[List[Dict[str, Any]]]: AIè¿”å›çš„åˆ†æç»“æœï¼Œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

        Raises:
            ValidationError: å½“aspect_typeæ— æ•ˆæ—¶
            ExternalAPIError: å½“è°ƒç”¨AIæœåŠ¡æ—¶å‡ºé”™
        """
        try:
            if aspect_type not in self.analysis_types:
                raise ValidationError(detail=f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {aspect_type}", field="aspect_type")

            if not comment_data:
                logger.warning("è¯„è®ºæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
                return []

            # è·å–åˆ†æçš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤º
            aspect_config = self.user_prompts[aspect_type]
            sys_prompt = self.system_prompts[aspect_type]
            user_prompt = (
                f"Analyze the {aspect_config['description']} for the following comments:\n"
                f"{json.dumps(comment_data, ensure_ascii=False)}"
            )

            # ä¸ºé¿å…tokené™åˆ¶ï¼Œé™åˆ¶è¯„è®ºæ–‡æœ¬é•¿åº¦
            for comment in comment_data:
                if 'text' in comment and len(comment['text']) > 1000:
                    comment['text'] = comment['text'][:997] + "..."

            # è°ƒç”¨AIè¿›è¡Œåˆ†æï¼Œä¼˜å…ˆä½¿ç”¨ChatGPTï¼Œå¤±è´¥æ—¶å°è¯•Claude
            try:
                response = await self.chatgpt.chat(
                    system_prompt=sys_prompt,
                    user_prompt=user_prompt
                )

                # è§£æChatGPTè¿”å›çš„ç»“æœ
                analysis_results = response["choices"][0]["message"]["content"].strip()

            except ExternalAPIError as e:
                logger.warning(f"ChatGPTåˆ†æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Claude: {str(e)}")
                try:
                    # å°è¯•ä½¿ç”¨Claudeä½œä¸ºå¤‡ä»½
                    response = await self.claude.chat(
                        system_prompt=sys_prompt,
                        user_prompt=user_prompt
                    )
                    analysis_results = response["choices"][0]["message"]["content"].strip()
                except Exception as claude_error:
                    logger.error(f"Claudeåˆ†æä¹Ÿå¤±è´¥: {str(claude_error)}")
                    raise ExternalAPIError(
                        detail="æ‰€æœ‰AIæœåŠ¡å‡æ— æ³•å®Œæˆåˆ†æ",
                        service="AI"
                    )

            # å¤„ç†è¿”å›çš„JSONæ ¼å¼ï¼ˆå¯èƒ½åŒ…å«åœ¨Markdownä»£ç å—ä¸­ï¼‰
            analysis_results = re.sub(
                r"```json\n|\n```|```|\n",
                "",
                analysis_results.strip()
            )

            try:
                analysis_result = json.loads(analysis_results)
                return analysis_result
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æé”™è¯¯: {str(e)}, åŸå§‹å†…å®¹: {analysis_results[:200]}...")
                raise ExternalAPIError(
                    detail="AIè¿”å›çš„ç»“æœæ— æ³•è§£æä¸ºJSON",
                    service="AI",
                    original_error=e
                )

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except ExternalAPIError:
            # ç›´æ¥å‘ä¸Šä¼ é€’APIé”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def get_purchase_intent_stats(
            self,
            aweme_id: str,
            batch_size: int = 30,
            concurrency: int = 5
    ) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): aiå¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“è°ƒç”¨å¤–éƒ¨æœåŠ¡å‡ºé”™æ—¶
            InternalServerError: å½“å†…éƒ¨å¤„ç†å‡ºé”™æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            if batch_size <= 0 or batch_size > settings.MAX_BATCH_SIZE:
                raise ValidationError(
                    detail=f"batch_sizeå¿…é¡»åœ¨1å’Œ{settings.MAX_BATCH_SIZE}ä¹‹é—´",
                    field="batch_size"
                )

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„ {len(comments_df)} æ¡è¯„è®º")
            analyzed_df = await self.analyze_comments_batch(
                comments_df,
                'purchase_intent',
                batch_size,
                concurrency
            )

            if analyzed_df.empty:
                raise InternalServerError(f"åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå¤±è´¥")

            # ç”Ÿæˆåˆ†ææ‘˜è¦
            analysis_summary = {
                'sentiment_distribution': self._analyze_sentiment_distribution(analyzed_df),
                'purchase_intent': self._analyze_purchase_intent(analyzed_df),
                'interest_levels': self._analyze_interest_levels(analyzed_df),
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'purchase_intent_stats',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }

            return analysis_summary

        except (ValidationError, ExternalAPIError, InternalServerError):
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"è·å–è´­ä¹°æ„å›¾ç»Ÿè®¡æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"è·å–è´­ä¹°æ„å›¾ç»Ÿè®¡æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def _analyze_sentiment_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ

        Args:
            df (pd.DataFrame): åŒ…å«sentimentåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ")
            if 'sentiment' not in df.columns:
                raise ValueError("DataFrameå¿…é¡»åŒ…å«'sentiment'åˆ—")

            # æ ‡å‡†åŒ–æƒ…æ„Ÿå€¼
            df['sentiment'] = df['sentiment'].str.lower()
            df.loc[df['sentiment'] == 'neg', 'sentiment'] = 'negative'
            df.loc[df['sentiment'] == 'pos', 'sentiment'] = 'positive'

            sentiment_counts = df['sentiment'].value_counts()
            return {
                'counts': sentiment_counts.to_dict(),
                'percentages': (sentiment_counts / len(df) * 100).round(2).to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†ææƒ…æ„Ÿåˆ†å¸ƒæ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'counts': {},
                'percentages': {}
            }

    def _analyze_purchase_intent(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æè´­ä¹°æ„å›¾

        Args:
            df (pd.DataFrame): åŒ…å«purchase_intentå’Œinterest_levelåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: è´­ä¹°æ„å›¾ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†æè´­ä¹°æ„å›¾")
            required_columns = ['purchase_intent', 'interest_level']
            for col in required_columns:
                if col not in df.columns:
                    raise ValueError(f"DataFrameå¿…é¡»åŒ…å«'{col}'åˆ—")

            # ç¡®ä¿purchase_intentæ˜¯å¸ƒå°”å€¼
            if df['purchase_intent'].dtype != bool:
                df['purchase_intent'] = df['purchase_intent'].apply(
                    lambda x: x if isinstance(x, bool) else (
                            str(x).lower() in ['true', 'yes', '1', 't']
                    )
                )

            # æ ‡å‡†åŒ–interest_level
            df['interest_level'] = df['interest_level'].str.lower()
            df.loc[df['interest_level'] == 'mid', 'interest_level'] = 'medium'

            intent_df = df[df['purchase_intent'] == True]

            return {
                'total_comments': len(df),
                'intent_count': len(intent_df),
                'intent_rate': round(len(intent_df) / len(df) * 100, 2) if len(df) > 0 else 0,
                'intent_by_interest_level': intent_df['interest_level'].value_counts().to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†æè´­ä¹°æ„å›¾æ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'total_comments': 0,
                'intent_count': 0,
                'intent_rate': 0,
                'intent_by_interest_level': {}
            }

    def _analyze_interest_levels(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æå…´è¶£æ°´å¹³

        Args:
            df (pd.DataFrame): åŒ…å«interest_levelåˆ—çš„DataFrame

        Returns:
            Dict[str, Any]: å…´è¶£æ°´å¹³ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("åˆ†æå…´è¶£æ°´å¹³")
            if 'interest_level' not in df.columns:
                raise ValueError("DataFrameå¿…é¡»åŒ…å«'interest_level'åˆ—")

            # æ ‡å‡†åŒ–interest_level
            df['interest_level'] = df['interest_level'].str.lower()
            df.loc[df['interest_level'] == 'mid', 'interest_level'] = 'medium'

            interest_counts = df['interest_level'].value_counts()
            return {
                'counts': interest_counts.to_dict(),
                'percentages': (interest_counts / len(df) * 100).round(2).to_dict()
            }
        except Exception as e:
            logger.error(f"åˆ†æå…´è¶£æ°´å¹³æ—¶å‡ºé”™: {str(e)}")
            return {
                'error': str(e),
                'counts': {},
                'percentages': {}
            }

    async def get_potential_customers(
            self,
            aweme_id: str,
            batch_size: int = 30,
            concurrency: int = 5,
            min_score: float = 50.0,
            max_score: float = 100.0,
            ins_filter: bool = False,
            twitter_filter: bool = False,
            region_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è®¡ç®—æ½œåœ¨å®¢æˆ·çš„å‚ä¸åº¦åˆ†æ•°ï¼Œå¹¶è¯†åˆ«æ½œåœ¨å®¢æˆ·

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): aiå¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5
            min_score (float, optional): æœ€å°å‚ä¸åº¦åˆ†æ•°ï¼Œé»˜è®¤50.0
            max_score (float, optional):f æœ€å¤§å‚ä¸åº¦åˆ†æ•°ï¼Œé»˜è®¤100.0
            ins_filter (bool, optional): æ˜¯å¦è¿‡æ»¤Instagramä¸ºNullç”¨æˆ·ï¼Œé»˜è®¤False
            twitter_filter (bool, optional): æ˜¯å¦è¿‡æ»¤Twitterä¸ºNullç”¨æˆ·ï¼Œé»˜è®¤False
            region_filter (Optional[str], optional): è¿‡æ»¤ç‰¹å®šåœ°åŒºçš„ç”¨æˆ·ï¼Œé»˜è®¤None

        Returns:
            Dict[str, Any]: æ½œåœ¨å®¢æˆ·ä¿¡æ¯

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if not aweme_id:
                raise ValueError("aweme_idä¸èƒ½ä¸ºç©º")

            if min_score < 0 or max_score > 100 or min_score >= max_score:
                raise ValueError("åˆ†æ•°èŒƒå›´æ— æ•ˆï¼Œåº”è¯¥æ»¡è¶³: 0 <= min_score < max_score <= 100")


            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            cleaned_comments = await self.fetch_video_comments(aweme_id)

            if not cleaned_comments.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            cleaned_comments = cleaned_comments.get('comments', [])
            comments_df = pd.DataFrame(cleaned_comments)

            # è¿‡æ»¤æ¡ä»¶
            if ins_filter:
                comments_df = comments_df[comments_df['ins_id']!= '']
            if twitter_filter:
                comments_df = comments_df[comments_df['twitter_id']!= '']
            if region_filter:
                comments_df = comments_df[comments_df['commenter_region'] == region_filter]
            if comments_df.empty:
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰ç¬¦åˆè¿‡æ»¤æ¡ä»¶çš„è¯„è®º")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found after filtering',
                    'analysis_timestamp': datetime.now().isoformat(),
                }
            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„ {len(comments_df)} æ¡è¯„è®ºä»¥è¯†åˆ«æ½œåœ¨å®¢æˆ·")

            analyzed_df = await self.analyze_comments_batch(
                comments_df,
                'purchase_intent',
                batch_size,
                concurrency
            )

            if analyzed_df.empty:
                raise RuntimeError(f"åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå¤±è´¥")

            # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„å‚ä¸åº¦åˆ†æ•°
            potential_customers = []
            for _, row in analyzed_df.iterrows():
                try:
                    potential_value = self._calculate_engagement_score(
                        row['sentiment'],
                        row['purchase_intent'],
                        row['interest_level']
                    )

                    # æ£€æŸ¥å¿…å¡«å­—æ®µ
                    user_id = row.get('commenter_uniqueId', '')
                    sec_uid = row.get('commenter_secuid', '')
                    text = row.get('text')

                    #if not all([user_id, sec_uid, text]):
                    #    logger.warning(f"è¯„è®ºæ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µï¼Œè·³è¿‡: {row}")
                    #    continue

                    potential_customers.append({
                        'user_uniqueID': user_id,
                        'potential_value': potential_value,
                        'user_secuid': sec_uid,
                        'ins_id': row.get('ins_id', ''),
                        'twitter_id': row.get('twitter_id', ''),
                        'region': row.get('commenter_region', ''),
                        'text': text
                    })
                except Exception as e:
                    logger.error(f"å¤„ç†ç”¨æˆ·è¯„è®ºæ—¶å‡ºé”™: {str(e)}, è·³è¿‡è¯¥è¯„è®º")
                    continue

            # åˆ›å»ºæ½œåœ¨å®¢æˆ·DataFrame
            if not potential_customers:
                logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æ½œåœ¨å®¢æˆ·")
                return {
                    'aweme_id': aweme_id,
                    'total_potential_customers': 0,
                    'min_engagement_score': min_score,
                    'max_engagement_score': max_score,
                    'average_potential_value': 0,
                    'potential_customers': []
                }

            potential_customers_df = pd.DataFrame(potential_customers)

            # æ ¹æ®æ½œåœ¨ä»·å€¼è¿‡æ»¤å’Œæ’åº
            filtered_df = potential_customers_df[
                potential_customers_df['potential_value'].between(min_score, max_score)
            ].sort_values(by='potential_value', ascending=False)

            avg_value = filtered_df['potential_value'].mean() if not filtered_df.empty else 0

            return {
                'aweme_id': aweme_id,
                'total_potential_customers': len(filtered_df),
                'min_engagement_score': min_score,
                'max_engagement_score': max_score,
                'average_potential_value': round(avg_value, 2),
                'potential_customers': filtered_df.to_dict(orient='records'),
                'analysis_timestamp': datetime.now().isoformat()
            }

        except ValueError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise ValueError
        except RuntimeError:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿è¡Œæ—¶é”™è¯¯
            raise RuntimeError
        except Exception as e:
            logger.error(f"è·å–æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise RuntimeError(f"è·å–æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")


    def _calculate_engagement_score(
            self,
            sentiment: str,
            purchase_intent: bool,
            interest_level: str
    ) -> float:
        """
        è®¡ç®—æ½œåœ¨å®¢æˆ·çš„å‚ä¸åº¦åˆ†æ•°

        Args:
            sentiment (str): æƒ…æ„Ÿåˆ†æç»“æœ ('positive', 'neutral', 'negative')
            purchase_intent (bool): æ˜¯å¦æœ‰è´­ä¹°æ„å›¾
            interest_level (str): å…´è¶£æ°´å¹³ ('high', 'medium', 'low')

        Returns:
            float: å‚ä¸åº¦åˆ†æ•° (0-100)
        """
        try:
            # å‚æ•°éªŒè¯å’Œæ ‡å‡†åŒ–
            if not isinstance(sentiment, str):
                sentiment = str(sentiment).lower()
            else:
                sentiment = sentiment.lower()

            if not isinstance(purchase_intent, bool):
                # å°è¯•è½¬æ¢ä¸ºå¸ƒå°”å€¼
                if isinstance(purchase_intent, str):
                    purchase_intent = purchase_intent.lower() in ['true', '1', 'yes', 't']
                else:
                    purchase_intent = bool(purchase_intent)

            if not isinstance(interest_level, str):
                interest_level = str(interest_level).lower()
            else:
                interest_level = interest_level.lower()

            # ä¿®æ­£æƒ…æ„Ÿæ ‡ç­¾
            if sentiment in ['neg', 'negative']:
                sentiment = 'negative'
            elif sentiment in ['pos', 'positive']:
                sentiment = 'positive'
            elif sentiment not in ['neutral']:
                sentiment = 'neutral'  # é»˜è®¤ä¸ºä¸­æ€§

            # å¤„ç†å…´è¶£æ°´å¹³ä¸­å€¼çš„ä¸åŒè¡¨ç¤º
            if interest_level in ['mid', 'medium']:
                interest_level = 'medium'
            elif interest_level not in ['high', 'low']:
                interest_level = 'low'  # é»˜è®¤ä¸ºä½å…´è¶£

            # 1. æƒ…æ„Ÿè½¬æ¢ (0-1 scale)
            sentiment_score = {
                'positive': 1.0,
                'neutral': 0.5,
                'negative': 0.0
            }.get(sentiment, 0.5)  # é»˜è®¤ä¸ºä¸­æ€§

            # 2. è´­ä¹°æ„å›¾åˆ†æ•°
            intent_score = 1.0 if purchase_intent else 0.0

            # 3. å…´è¶£æ°´å¹³åˆ†æ•°
            interest_score = {
                'high': 1.0,
                'medium': 0.5,
                'low': 0.2
            }.get(interest_level, 0.5)  # é»˜è®¤ä¸ºä¸­ç­‰

            # æ ¹æ®åŠ æƒå¹³å‡è®¡ç®—æ½œåœ¨ä»·å€¼
            potential_value = (
                                      0.3 * sentiment_score +
                                      0.4 * intent_score +
                                      0.3 * interest_score
                              ) * 100  # ç¼©æ”¾åˆ°0-100

            return round(potential_value, 2)

        except Exception as e:
            logger.error(f"è®¡ç®—å‚ä¸åº¦åˆ†æ•°æ—¶å‡ºé”™: {str(e)}")
            # è¿”å›é»˜è®¤å€¼
            return 0.0

    async def get_keyword_potential_customers(
            self,
            keyword: str,
            batch_size: int = 30,
            video_concurrency: int = 5,
            ai_concurrency: int = 5,
            min_score: float = 50.0,
            max_score: float = 100.0,
            ins_filter: bool = False,
            twitter_filter: bool = False,
            region_filter: Optional[str] = None,
            max_customers: Optional[int] = None,
    ) -> Dict[str, Any]:
        """
        æ ¹æ®å…³é”®è¯è·å–æ½œåœ¨å®¢æˆ·

        Args:
            keyword (str): å…³é”®è¯
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            video_concurrency (int, optional): è§†é¢‘å¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5
            ai_concurrency (int, optional): aiå¤„ç†å¹¶å‘æ•°ï¼Œé»˜è®¤5
            min_score (float, optional): æœ€å°å‚ä¸åº¦åˆ†æ•°ï¼Œé»˜è®¤50.0
            max_score (float, optional): æœ€å¤§å‚ä¸åº¦åˆ†æ•°ï¼Œé»˜è®¤100.0
            ins_filter (bool, optional): æ˜¯å¦è¿‡æ»¤Instagramä¸ºç©ºç”¨æˆ·ï¼Œé»˜è®¤False
            twitter_filter (bool, optional): æ˜¯å¦è¿‡æ»¤Twitterä¸ºç©ºç”¨æˆ·ï¼Œé»˜è®¤False
            region_filter (str, optional): åœ°åŒºè¿‡æ»¤ï¼Œé»˜è®¤None
            max_customers (int, optional): æœ€å¤§æ½œåœ¨å®¢æˆ·æ•°é‡ï¼Œé»˜è®¤None, Noneè¡¨ç¤ºä¸é™åˆ¶

        Returns:
            Dict[str, Any]: æ½œåœ¨å®¢æˆ·ä¿¡æ¯

        Raises:
            ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
            RuntimeError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if not keyword:
                raise ValueError("keywordä¸èƒ½ä¸ºç©º")

            if min_score < 0 or max_score > 100 or min_score >= max_score:
                raise ValueError("åˆ†æ•°èŒƒå›´æ— æ•ˆï¼Œåº”è¯¥æ»¡è¶³: 0 <= min_score < max_score <= 100")

            # è·å–æ¸…ç†åçš„è§†é¢‘æ•°æ®
            video_collector = VideoCollector(self.tikhub_api_key, self.tikhub_base_url)
            video_cleaner = VideoCleaner()
            raw_videos = await video_collector.collect_videos_by_keyword(keyword)
            cleaned_videos = await video_cleaner.clean_videos_by_keyword(raw_videos)

            if not cleaned_videos.get('videos'):
                logger.warning(f"æœªæ‰¾åˆ°ä¸å…³é”®è¯ {keyword} ç›¸å…³çš„è§†é¢‘")
                return {
                    'keyword': keyword,
                    'error': 'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è§†é¢‘',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            videos_df = pd.DataFrame(cleaned_videos['videos'])
            aweme_ids = videos_df['aweme_id'].tolist()

            logger.info(f"å¼€å§‹åˆ†æä¸å…³é”®è¯ {keyword} ç›¸å…³çš„ {len(videos_df)} ä¸ªè§†é¢‘ä»¥è¯†åˆ«æ½œåœ¨å®¢æˆ·")

            # ä½¿ç”¨get_potential_customersæ–¹æ³•åˆ†ææ¯ä¸ªè§†é¢‘ï¼Œæ¯concurrencyä¸ªè§†é¢‘ä¸ºä¸€ç»„ï¼Œ
            potential_customers = []
            for i in range(0, len(aweme_ids), video_concurrency):
                batch_aweme_ids = aweme_ids[i:i + video_concurrency]
                tasks = [
                    self.get_potential_customers(
                        aweme_id,
                        batch_size,
                        ai_concurrency,
                        min_score,
                        max_score,
                        ins_filter,
                        twitter_filter,
                        region_filter,
                    ) for aweme_id in batch_aweme_ids
                ]

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                for result in batch_results:
                    if isinstance(result, Exception):
                        logger.error(f"å¤„ç†è§†é¢‘ {batch_aweme_ids} æ—¶å‡ºé”™: {str(result)}")
                        continue

                    if result.get('potential_customers') is None:
                        logger.warning(f"è§†é¢‘ {batch_aweme_ids} æ²¡æœ‰æ½œåœ¨å®¢æˆ·æ•°æ®")
                        continue

                    potential_customers.extend(result['potential_customers'])
                    if len(potential_customers) >= max_customers:
                        break
                if len(potential_customers) >= max_customers:
                    break
            # åˆ›å»ºæ½œåœ¨å®¢æˆ·DataFrame
            if not potential_customers:
                logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æ½œåœ¨å®¢æˆ·")
                return {
                    'keyword': keyword,
                    'total_potential_customers': 0,
                    'min_engagement_score': min_score,
                    'max_engagement_score': max_score,
                    'average_potential_value': 0,
                    'potential_customers': []
                }

            potential_customers_df = pd.DataFrame(potential_customers)
            # æ ¹æ®æ½œåœ¨ä»·å€¼è¿‡æ»¤å’Œæ’åº
            filtered_df = potential_customers_df[
                potential_customers_df['potential_value'].between(min_score, max_score)
            ].sort_values(by='potential_value', ascending=False)

            avg_value = filtered_df['potential_value'].mean() if not filtered_df.empty else 0

            return {
                'keyword': keyword,
                'total_potential_customers': len(filtered_df),
                'min_engagement_score': min_score,
                'max_engagement_score': max_score,
                'average_potential_value': round(avg_value, 2),
                'potential_customers': filtered_df.to_dict(orient='records'),
                'analysis_timestamp': datetime.now().isoformat()
            }
        except ValueError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise ValueError
        except RuntimeError:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿è¡Œæ—¶é”™è¯¯
            raise RuntimeError
        except Exception as e:
            logger.error(f"è·å–å…³é”®è¯æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise RuntimeError(f"è·å–å…³é”®è¯æ½œåœ¨å®¢æˆ·æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")




async def main():
    # åˆ›å»ºCustomerAgentå®ä¾‹
    agent = CustomerAgent()

    # ç¤ºä¾‹ï¼šè·å–keywordæ½œåœ¨å®¢æˆ·
    keyword = "red liptick"
    potential_customers = await agent.get_keyword_potential_customers(keyword,20, 5, 5, 0, 100.0, True, False, 'US', 100)

    #save to json
    with open('potential_customers.json', 'w', encoding='utf-8') as f:
        json.dump(potential_customers, f, ensure_ascii=False, indent=4)


if __name__ == "__main__":
    asyncio.run(main())


