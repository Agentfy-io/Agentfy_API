# -*- coding: utf-8 -*-
"""
@file: sentiment_agent.py
@desc: å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€èˆ†æƒ…åˆ†æå’Œé»‘ç²‰è¯†åˆ«åŠŸèƒ½
@auth: Callmeiks
"""

import json
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
from collections import Counter
import asyncio
import time

import numpy as np
import pandas as pd
from dotenv import load_dotenv
import os

from app.utils.logger import setup_logger
from services.ai_models.chatgpt import ChatGPT
from services.ai_models.claude import Claude
from services.cleaner.comment_cleaner import CommentCleaner
from services.crawler.comment_crawler import CommentCollector
from services.crawler.video_crawler import VideoCollector
from services.cleaner.video_cleaner import VideoCleaner
from app.config import settings
from app.core.exceptions import ValidationError, ExternalAPIError, InternalServerError

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = setup_logger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class SentimentAgent:
    """å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€èˆ†æƒ…åˆ†æå’Œé»‘ç²‰è¯†åˆ«åŠŸèƒ½"""

    def __init__(self, tikhub_api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–SentimentAgentï¼ŒåŠ è½½APIå¯†é’¥å’Œæç¤ºæ¨¡æ¿

        Args:
            tikhub_api_key: TikHub APIå¯†é’¥
            tikhub_base_url: TikHub APIåŸºç¡€URL
        """
        # åˆå§‹åŒ–AIæ¨¡å‹å®¢æˆ·ç«¯
        self.chatgpt = ChatGPT()
        self.claude = Claude()

        # ä¿å­˜TikHub APIé…ç½®
        self.tikhub_api_key = tikhub_api_key
        self.tikhub_base_url = settings.TIKHUB_BASE_URL

        # å¦‚æœæ²¡æœ‰æä¾›TikHub APIå¯†é’¥ï¼Œè®°å½•è­¦å‘Š
        if not self.tikhub_api_key:
            logger.warning("æœªæä¾›TikHub APIå¯†é’¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

        self.analysis_types = ['sentiment', 'relationship', 'toxicity']

        # åŠ è½½ç³»ç»Ÿå’Œç”¨æˆ·æç¤º
        self._load_system_prompts()
        self._load_user_prompts()

    def _load_system_prompts(self) -> None:
        """åŠ è½½ç³»ç»Ÿæç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.system_prompts = {
            'sentiment': """You are an AI trained to analyze social media comments on TikTok videos. Your task is to categorize and summarize how commenters react to a given video.
                For each comment in the provided list, analyze:
                1. Sentiment (positive, negative, or neutral)
                2. Emotion (e.g., excitement, amusement, frustration, curiosity, etc.)
                3. Engagement Type (e.g., supportive, critical, humorous, questioning, spam)
                4. Key Themes (e.g., reaction to video content, reference to trends, personal anecdotes, meme usage)
                5. Virality Indicators (e.g., high engagement phrases, common phrases from viral trends, use of emojis, repeated comment patterns)
                
                Return the analysis in the following JSON format, and comment_id should still remain the same as input data:
                [
                    {
                        "comment_id": "comment ID from input data",
                        "text": "comment text",
                        "sentiment": "positive/negative/neutral",
                        "emotion": "emotion_label",
                        "engagement_type": "supportive/critical/humorous/questioning/spam",
                        "key_themes": ["theme1", "theme2"],
                        "virality_indicators": ["emoji", "phrase", "meme_reference"],
                    }
                ]
                
                Guidelines:
                - Identify emotions based on wording, punctuation, and emojis.
                - Consider trends and viral elements influencing reactions.
                - If a comment uses humor or sarcasm, categorize it accordingly.
                - If a comment references a meme or viral phrase, list it as a key theme.
                - Comments containing excessive emojis, repeated phrases, or engagement-bait (e.g., "like if you agree") should be marked as potential virality indicators.
                
                Respond with a JSON array containing analysis for all comments.
                """,

            'relationship': """You are an AI trained to analyze audience sentiment and engagement toward an influencer or creator. Your task is to assess how commenters perceive and interact with the influencer.
                For each comment, analyze:
                1. **Trust Level:** (loyal fan, skeptical, indifferent)
                2. **Tone Toward Influencer/Brand:** (supportive, critical, neutral)
                3. **Fandom Level:** (casual viewer, superfan, first-time viewer)
                4. **Previous Knowledge of Influencer:** (new follower, returning audience, long-time fan)
                5. **Engagement Type:** (praise, criticism, joke, curiosity, casual remark)
                
                **JSON Output Format:**
                [
                    {
                        "comment_id": "comment ID from input data",
                        "text": "comment text",
                        "trust_level": "loyal_fan/skeptical/indifferent",
                        "tone_toward_influencer": "supportive/critical/neutral",
                        "fandom_level": "casual_viewer/superfan/first_time_viewer",
                        "previous_knowledge": "new_follower/returning_audience/long_time_fan",
                        "engagement_type": "praise/criticism/joke/curiosity/casual_remark",
                    }
                ]
                
                **Guidelines:**
                - Identify loyalty through repeated engagement or references to past content.
                - Detect tone based on word choice, punctuation, and emojis.
                - Recognize when users joke, question, or provide constructive criticism.
                - Assess engagement depth (quick reaction vs. detailed discussion).
                
                Respond with a JSON array containing the analysis for all comments.
                """,

            'toxicity': """You are an AI trained to analyze social media comments for harmful, inappropriate,negative_product_review, negative_service_review, negative_shop_review or spam content. Your task is to detect and categorize toxicity levels in the provided comments.
                For each comment, analyze:
                1. **Toxicity Level:** (low, medium, high)
                2. **Type of Toxicity:** (e.g., negative_product_review, negative_service_review, negative_shop_review, hate speech, personal attack, trolling, misinformation, spam)
                3. **Report Worthiness:** (should report, needs review, not harmful)
                4. **Potential Community Guidelines Violation:** (yes/no)
                5. **Severity Score:** (scale from 1-10, with 10 being highly toxic)
        
                **JSON Output Format:**
                [
                    {
                        "comment_id": "comment ID from input data",
                        "text": "comment text",
                        "toxicity_level": "low/medium/high",
                        "toxicity_type": "negative_product_review/negative_service_review/negative_shop_review/hate_speech/personal_attack/trolling/misinformation/spam",
                        "report_worthiness": "should_report/needs_review/not_harmful",
                        "community_guidelines_violation": true/false,
                        "severity_score": "1-10",
                    }
                ]
        
                **Guidelines:**
                - Detect offensive language, threats, or harmful intent.
                - Recognize subtle toxicity, sarcasm, and coded language.
                - Mark spam comments with excessive emojis, engagement-bait phrases, or promotional links.
                - Ensure fairness by considering context and common internet slang.
                - emojis are not consider as spam.
        
                Respond with a JSON array containing the analysis for all comments.
                """,

            'relationship_report': """You are a social media analytics expert specializing in influencer-audience relationships. Your task is to create an engaging report analyzing comment data to reveal audience connection patterns and community dynamics.
                Report Sections:
                1. Report Metadata (video ID, total comments, analysis timestamp)
                2. Community Insight Summary (2-3 sentences highlighting key relationship patterns found in the data)
                3. Audience Connection Analysis (analyze specific ways commenters relate to the creator - personal stories shared, questions asked, consistent engagement patterns)
                4. Trust & Loyalty Indicators (markdown table showing metrics like returning commenters, defense of creator, personal disclosure levels)
                5. Engagement Quality Assessment (analyze depth of comments, conversation threads, and emotional investment)
                6. Audience Segmentation (identify distinct audience groups based on comment patterns and how they relate to the creator)
                7. Relationship Growth Opportunities (2-3 specific, data-backed suggestions to strengthen audience bonds)
                
                Focus on revealing relationship patterns from the input data rather than generic advice. Identify how the audience perceives their relationship with the creator, what connection points resonate most, and where deeper engagement opportunities exist. Keep the report under 500 words.""",

            'sentiment_report': """You are a content performance analyst specializing in audience emotional responses. Your task is to create an insightful report analyzing comment data to reveal how content emotionally resonates with the audience.
                Report Sections:
                1. Report Metadata (video ID, total comments, analysis timestamp)
                2. Emotional Response Overview (2-3 sentences summarizing the dominant emotional patterns found in comments)
                3. Sentiment Analysis (markdown table showing positive/negative/neutral distribution with specific emotion subcategories)
                4. Emotional Triggers (analyze which specific content elements/moments generated the strongest emotional responses)
                5. Audience Reaction Patterns (identify how emotions spread or change through comment threads)
                6. Sentiment by Audience Segment (how different viewer groups emotionally responded)
                7. Content Resonance Insights (which emotional appeals were most effective)
                8. Strategic Recommendations (2-3 actionable ways to enhance positive emotional engagement based on the data)
                
                Focus on extracting emotional patterns directly from the input data. Analyze the specific language, expressions, and reactions that reveal audience emotional states and how they connect to content elements. Keep the report under 450 words.""",

            'toxicity_report': """You are an expert content moderator and community safety analyst. Your task is to create a clear, concise report analyzing comment data to reveal community health and safety concerns.
                Report Sections:
                1. Report Metadata (video ID, total comments, analysis timestamp)
                2. Safety Assessment Summary (2-3 sentences highlighting the overall community health based on toxicity metrics)
                3. Toxicity Analysis (markdown table showing types and levels of problematic content identified)
                4. Context Pattern Analysis (analyze when/where toxicity appears - specific topics, timestamps, or triggers)
                5. Impact Assessment (analyze how toxic comments affect overall engagement and community interaction)
                6. Moderation Priority Areas (identify specific types of concerning content requiring attention)
                7. Community Management Recommendations (2-3 actionable, data-backed strategies to improve community health)
                
                Focus on presenting objective analysis from the input data rather than subjective judgments. Distinguish between genuine community concerns and minor issues. Provide balanced perspective that helps creators understand real community health without overemphasizing isolated incidents. Keep the report under 400 words."""
        }

    def _load_user_prompts(self) -> None:
        """åŠ è½½ç”¨æˆ·æç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.user_prompts = {
            'sentiment': {
                'description': "sentiment and engagement analysis",
            },
            'relationship': {
                'description': "relationship and engagement analysis",
            },
            'toxicity': {
                'description': "toxicity, spam analysis",
            },

        }

    async def generate_analysis_report(self, aweme_id: str, analysis_type: str, data: Dict[str, Any]) -> str:
        """
        ç”ŸæˆæŠ¥å‘Šå¹¶è½¬æ¢ä¸ºHTML

        Args:
            aweme_id (str): è§†é¢‘ ID
            analysis_type (str): åˆ†æç±»å‹
            data (Dict[str, Any]): åˆ†ææ•°æ®

        Returns:
            str: HTMLæŠ¥å‘Šçš„æœ¬åœ°æ–‡ä»¶URL
        """
        if analysis_type not in self.system_prompts:
            raise ValueError(f"Invalid report type: {analysis_type}. Choose from {self.system_prompts.keys()}")

        # è·å–ç³»ç»Ÿæç¤º
        sys_prompt = self.system_prompts[analysis_type]

        # è·å–ç”¨æˆ·æç¤º
        user_prompt = f"Generate a report for the {analysis_type} analysis based on the following data:\n{json.dumps(data, ensure_ascii=False)}, for video ID: {aweme_id}"

        # ç”ŸæˆæŠ¥å‘Š
        response = await self.chatgpt.chat(
            system_prompt=sys_prompt,
            user_prompt=user_prompt
        )

        report = response["choices"][0]["message"]["content"].strip()

        # ä¿å­˜MarkdownæŠ¥å‘Š
        report_dir = "reports"
        os.makedirs(report_dir, exist_ok=True)

        report_md_path = os.path.join(report_dir, f"report_{aweme_id}.md")
        with open(report_md_path, "w", encoding="utf-8") as f:
            f.write(report)

        # è½¬æ¢ä¸ºHTML
        html_content = self.convert_markdown_to_html(report, f"{analysis_type.title()} Analysis for {aweme_id}")
        html_filename = f"report_{aweme_id}.html"
        html_path = os.path.join(report_dir, html_filename)

        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶URL
        absolute_path = os.path.abspath(html_path)

        # æ„å»ºfile://åè®®URL
        file_url = f"file://{absolute_path}"

        # ç¡®ä¿è·¯å¾„åˆ†éš”ç¬¦æ˜¯URLå…¼å®¹çš„
        if os.name == 'nt':  # Windowsç³»ç»Ÿ
            # Windowsè·¯å¾„éœ€è¦è½¬æ¢ä¸ºURLæ ¼å¼
            file_url = file_url.replace('\\', '/')

        print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: Markdown ({report_md_path}), HTML ({html_path})")
        print(f"æŠ¥å‘Šæœ¬åœ°URL: {file_url}")

        return file_url

    def convert_markdown_to_html(self, markdown_content: str, title: str = "Analysis Report") -> str:
        """
        å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML

        Args:
            markdown_content (str): Markdownå†…å®¹
            title (str): HTMLé¡µé¢æ ‡é¢˜

        Returns:
            str: HTMLå†…å®¹
        """
        try:
            import markdown
        except ImportError:
            print("è¯·å®‰è£…markdownåº“: pip install markdown")
            return f"<pre>{markdown_content}</pre>"

        # è½¬æ¢Markdownä¸ºHTML
        html_content = markdown.markdown(
            markdown_content,
            extensions=['tables', 'fenced_code', 'codehilite', 'toc']
        )

        # åˆ›å»ºå®Œæ•´HTMLæ–‡æ¡£
        css = """
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; color: #333; }
        h1, h2, h3 { margin-top: 1.5em; color: #111; }
        pre { background-color: #f6f8fa; border-radius: 3px; padding: 16px; overflow: auto; }
        code { font-family: SFMono-Regular, Consolas, Menlo, monospace; background-color: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 1em; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f6f8fa; }
        """

        html_document = f"""<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{title}</title>
        <style>{css}</style>
    </head>
    <body>
        <h1>{title}</h1>
        {html_content}
    </body>
    </html>
        """

        return html_document

    async def fetch_video_comments(self, aweme_id: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„æ¸…ç†åçš„è¯„è®ºæ•°æ®

        Args:
            aweme_id (str): è§†é¢‘ID

        Returns:
            Dict[str, Any]: æ¸…ç†åçš„è¯„è®ºæ•°æ®ï¼ŒåŒ…å«è§†é¢‘IDå’Œè¯„è®ºåˆ—è¡¨

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id or not isinstance(aweme_id, str):
                raise ValidationError(detail="aweme_idå¿…é¡»æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²", field="aweme_id")

            # è®°å½•å¼€å§‹è·å–è¯„è®º
            logger.info(f"å¼€å§‹è·å–è§†é¢‘ {aweme_id} çš„è¯„è®º")

            # è·å–è¯„è®º
            comment_collector = CommentCollector(self.tikhub_api_key, self.tikhub_base_url)
            comments = await comment_collector.collect_video_comments(aweme_id)

            if not comments or not comments.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æœªæ‰¾åˆ°è¯„è®º")
                return {
                    'aweme_id': aweme_id,
                    'comments': [],
                    'comment_count': 0,
                    'timestamp': datetime.now().isoformat()
                }

            # æ¸…æ´—è¯„è®º
            comment_cleaner = CommentCleaner()
            cleaned_comments = await comment_cleaner.clean_video_comments(comments)
            cleaned_comments = cleaned_comments.get('comments', [])

            processing_time = time.time() - start_time

            result = {
                'aweme_id': aweme_id,
                'comments': cleaned_comments,
                'comment_count': len(cleaned_comments),
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round(processing_time * 1000, 2)
            }

            logger.info(f"æˆåŠŸè·å–è§†é¢‘ {aweme_id} çš„è¯„è®º: {len(cleaned_comments)} æ¡ï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            return result

        except (ValidationError, ExternalAPIError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‡ºé”™: {str(e)}")
            raise

        except Exception as e:
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def analyze_comments_batch(
            self,
            df: pd.DataFrame,
            analysis_type: str,
            batch_size: int = 30,
            concurrency: int = 5
    ) -> pd.DataFrame:
        """
        åˆ†æ‰¹åˆ†æè¯„è®ºï¼Œä»¥é˜²æ­¢ä¸€æ¬¡æ€§å‘é€è¿‡å¤šæ•°æ®

        Args:
            df (pd.DataFrame): åŒ…å«è¯„è®ºæ•°æ®çš„DataFrameï¼Œéœ€åŒ…å«å­—æ®µï¼štext
            analysis_type (str): é€‰æ‹©åˆ†æç±»å‹ (purchase_intent)
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            pd.DataFrame: ç»“æœåˆå¹¶å›åŸå§‹DataFrame

        Raises:
            ValidationError: å½“analysis_typeæ— æ•ˆæˆ–dfä¸ºç©ºæ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if analysis_type not in self.analysis_types:
                raise ValidationError(
                    detail=f"æ— æ•ˆçš„åˆ†æç±»å‹: {analysis_type}. è¯·ä» {self.analysis_types} ä¸­é€‰æ‹©",
                    field="analysis_type"
                )

            if df.empty:
                raise ValidationError(detail="DataFrameä¸èƒ½ä¸ºç©º", field="df")

            if 'text' not in df.columns:
                raise ValidationError(detail="DataFrameå¿…é¡»åŒ…å«'text'åˆ—", field="df")

            # éªŒè¯å’Œè°ƒæ•´æ‰¹å¤„ç†å‚æ•°
            batch_size = min(batch_size, settings.MAX_BATCH_SIZE)
            concurrency = min(concurrency, 10)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º10

            # åˆ†æ‰¹å¤„ç†DataFrame
            num_splits = max(1, len(df) // batch_size + (1 if len(df) % batch_size > 0 else 0))
            comment_batches = np.array_split(df, num_splits)
            logger.info(
                f"ğŸš€ å¼€å§‹ {analysis_type} åˆ†æï¼Œå…± {len(comment_batches)} æ‰¹ï¼Œæ¯æ‰¹çº¦ {len(comment_batches[0]) if len(comment_batches) > 0 else 0} æ¡è¯„è®º"
            )

            # å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼ˆæ¯æ¬¡æœ€å¤š `concurrency` ç»„ï¼‰
            results = []
            for i in range(0, len(comment_batches), concurrency):
                batch_group = comment_batches[i:i + concurrency]

                # è®°å½•å½“å‰å¹¶å‘ç»„çš„èŒƒå›´
                batch_indices = [
                    f"{batch.index[0]}-{batch.index[-1]}" if not batch.empty else "-"
                    for batch in batch_group
                ]
                logger.info(
                    f"âš¡ å¤„ç†æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)}ï¼Œè¯„è®ºç´¢å¼•èŒƒå›´: {batch_indices}"
                )

                # å¹¶å‘æ‰§è¡Œ `concurrency` ä¸ªä»»åŠ¡
                tasks = [
                    self._analyze_aspect(analysis_type, batch[['text', 'comment_id']].to_dict('records'))
                    for batch in batch_group if not batch.empty
                ]

                if not tasks:
                    continue

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # å¤„ç†ç»“æœï¼Œè¿‡æ»¤æ‰å¼‚å¸¸
                valid_results = []
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"æ‰¹æ¬¡ {i + j + 1} åˆ†æå¤±è´¥: {str(result)}")
                    else:
                        valid_results.append(result)

                if len(valid_results) != len(batch_group):
                    logger.warning(
                        f"æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)} ä¸­æœ‰ {len(batch_group) - len(valid_results)} ä¸ªæ‰¹æ¬¡åˆ†æå¤±è´¥"
                    )

                results.extend(valid_results)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
            if not results:
                raise InternalServerError("æ‰€æœ‰æ‰¹æ¬¡åˆ†æå‡å¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆç»“æœ")

            # åˆå¹¶æ‰€æœ‰åˆ†æç»“æœ
            try:
                # å°†æ‰€æœ‰ç»“æœæ‰å¹³åŒ–ä¸ºå•ä¸ªåˆ—è¡¨
                all_results = []
                for batch_result in results:
                    if isinstance(batch_result, list):
                        all_results.extend(batch_result)

                # åˆ›å»ºç»“æœDataFrame
                analysis_df = pd.DataFrame(all_results)

                # ç¡®ä¿comment_idåˆ—å­˜åœ¨
                if 'comment_id' not in analysis_df.columns:
                    logger.warning("åˆ†æç»“æœç¼ºå°‘comment_idåˆ—ï¼Œæ— æ³•æ­£ç¡®åˆå¹¶")
                    # æ·»åŠ ç´¢å¼•ä½œä¸ºä¸´æ—¶åˆ—
                    analysis_df['temp_index'] = range(len(analysis_df))
                    df['temp_index'] = range(len(df))
                    # åŸºäºç´¢å¼•åˆå¹¶
                    merged_df = pd.merge(df, analysis_df, on='temp_index', how='left')
                    merged_df = merged_df.drop('temp_index', axis=1)
                else:
                    # åŸºäºcomment_idåˆå¹¶
                    merged_df = pd.merge(df, analysis_df, on='comment_id', how='left')

                # å¤„ç†é‡å¤çš„textåˆ—
                if 'text_y' in merged_df.columns:
                    merged_df = merged_df.drop('text_y', axis=1)
                    merged_df = merged_df.rename(columns={'text_x': 'text'})

                logger.info(f"âœ… {analysis_type} åˆ†æå®Œæˆï¼æ€»è®¡ {len(merged_df)} æ¡æ•°æ®")
                return merged_df

            except Exception as e:
                logger.error(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
                raise InternalServerError(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except InternalServerError:
            # ç›´æ¥å‘ä¸Šä¼ é€’å†…éƒ¨æœåŠ¡å™¨é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def _analyze_aspect(
            self,
            aspect_type: str,
            comment_data: List[Dict[str, Any]],
    ) -> Optional[List[Dict[str, Any]]]:
        """
        é€šç”¨åˆ†ææ–¹æ³•ï¼Œæ ¹æ®ä¸åŒçš„åˆ†æç±»å‹è°ƒç”¨ChatGPTæˆ–Claude AIæ¨¡å‹ã€‚

        Args:
            aspect_type (str): éœ€è¦åˆ†æçš„ç±»å‹ (purchase_intent)
            comment_data (List[Dict[str, Any]]): éœ€è¦åˆ†æçš„è¯„è®ºåˆ—è¡¨

        Returns:
            Optional[List[Dict[str, Any]]]: AIè¿”å›çš„åˆ†æç»“æœï¼Œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

        Raises:
            ValidationError: å½“aspect_typeæ— æ•ˆæ—¶
            ExternalAPIError: å½“è°ƒç”¨AIæœåŠ¡æ—¶å‡ºé”™
        """
        try:
            if aspect_type not in self.analysis_types:
                raise ValidationError(detail=f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {aspect_type}", field="aspect_type")

            if not comment_data:
                logger.warning("è¯„è®ºæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
                return []

            # è·å–åˆ†æçš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤º
            aspect_config = self.user_prompts[aspect_type]
            sys_prompt = self.system_prompts[aspect_type]
            user_prompt = (
                f"Analyze the {aspect_config['description']} for the following comments:\n"
                f"{json.dumps(comment_data, ensure_ascii=False)}"
            )

            # ä¸ºé¿å…tokené™åˆ¶ï¼Œé™åˆ¶è¯„è®ºæ–‡æœ¬é•¿åº¦
            for comment in comment_data:
                if 'text' in comment and len(comment['text']) > 1000:
                    comment['text'] = comment['text'][:997] + "..."

            # è°ƒç”¨AIè¿›è¡Œåˆ†æï¼Œä¼˜å…ˆä½¿ç”¨ChatGPTï¼Œå¤±è´¥æ—¶å°è¯•Claude
            try:
                response = await self.chatgpt.chat(
                    system_prompt=sys_prompt,
                    user_prompt=user_prompt
                )

                # è§£æChatGPTè¿”å›çš„ç»“æœ
                analysis_results = response["choices"][0]["message"]["content"].strip()

            except ExternalAPIError as e:
                logger.warning(f"ChatGPTåˆ†æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Claude: {str(e)}")
                try:
                    # å°è¯•ä½¿ç”¨Claudeä½œä¸ºå¤‡ä»½
                    response = await self.claude.chat(
                        system_prompt=sys_prompt,
                        user_prompt=user_prompt
                    )
                    analysis_results = response["choices"][0]["message"]["content"].strip()
                except Exception as claude_error:
                    logger.error(f"Claudeåˆ†æä¹Ÿå¤±è´¥: {str(claude_error)}")
                    raise ExternalAPIError(
                        detail="æ‰€æœ‰AIæœåŠ¡å‡æ— æ³•å®Œæˆåˆ†æ",
                        service="AI"
                    )

            # å¤„ç†è¿”å›çš„JSONæ ¼å¼ï¼ˆå¯èƒ½åŒ…å«åœ¨Markdownä»£ç å—ä¸­ï¼‰
            analysis_results = re.sub(
                r"```json\n|\n```|```|\n",
                "",
                analysis_results.strip()
            )

            try:
                analysis_result = json.loads(analysis_results)
                return analysis_result
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æé”™è¯¯: {str(e)}, åŸå§‹å†…å®¹: {analysis_results[:200]}...")
                raise ExternalAPIError(
                    detail="AIè¿”å›çš„ç»“æœæ— æ³•è§£æä¸ºJSON",
                    service="AI",
                    original_error=e
                )

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except ExternalAPIError:
            # ç›´æ¥å‘ä¸Šä¼ é€’APIé”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def analyze_sentiment(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šè§†é¢‘çš„è¯„è®ºæƒ…æ„Ÿ

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: æƒ…æ„Ÿåˆ†æç»“æœ

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """

        start_time = time.time()

        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            if batch_size <= 0 or batch_size > settings.MAX_BATCH_SIZE:
                raise ValidationError(
                    detail=f"batch_sizeå¿…é¡»åœ¨1å’Œ{settings.MAX_BATCH_SIZE}ä¹‹é—´",
                    field="batch_size"
                )

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºæƒ…æ„Ÿ")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'sentiment', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„æƒ…æ„Ÿåˆ†æç»“æœ")
            analysis_summary = {
                'sentiment_distribution': self.analyze_sentiment_distribution(analyzed_df),
                'emotion_patterns': self.analyze_emotion_patterns(analyzed_df),
                'engagement_patterns': self.analyze_engagement_patterns(analyzed_df),
                'themes': self.analyze_themes(analyzed_df),
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'sentiment',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }

            report_url = await self.generate_analysis_report(aweme_id, 'sentiment_report', analysis_summary)
            analysis_summary['report_url'] = report_url

            return analysis_summary
        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºæƒ…æ„Ÿæ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºæƒ…æ„Ÿæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"åˆ†æè§†é¢‘è¯„è®ºæƒ…æ„Ÿæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def analyze_sentiment_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ"""
        sentiment_counts = df['sentiment'].value_counts()
        total_comments = len(df)

        return {
            'distribution': {
                sentiment: {
                    'count': int(count),
                    'percentage': round(count / total_comments * 100, 2)
                }
                for sentiment, count in sentiment_counts.items()
            },
            'dominant_sentiment': sentiment_counts.index[0],
            'sentiment_ratio': {
                'positive_ratio': round(len(df[df['sentiment'] == 'positive']) / total_comments * 100, 2),
                'negative_ratio': round(len(df[df['sentiment'] == 'negative']) / total_comments * 100, 2),
                'neutral_ratio': round(len(df[df['sentiment'] == 'neutral']) / total_comments * 100, 2)
            }
        }

    def analyze_emotion_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææƒ…ç»ªæ¨¡å¼"""
        emotion_counts = df['emotion'].value_counts()
        sentiment_emotion_matrix = pd.crosstab(df['sentiment'], df['emotion'])

        return {
            'dominant_emotions': {
                'overall': emotion_counts.index[0],
                'by_sentiment': {
                    sentiment: sentiment_emotion_matrix.loc[sentiment].idxmax()
                    for sentiment in sentiment_emotion_matrix.index
                }
            },
            'emotion_distribution': {
                emotion: int(count)
                for emotion, count in emotion_counts.items()
            },
            'emotion_sentiment_correlation': {
                sentiment: {
                    emotion: int(count)
                    for emotion, count in row.items()
                }
                for sentiment, row in sentiment_emotion_matrix.iterrows()
            }
        }

    def analyze_engagement_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æäº’åŠ¨æ¨¡å¼"""
        engagement_counts = df['engagement_type'].value_counts()
        engagement_sentiment = pd.crosstab(df['engagement_type'], df['sentiment'])

        return {
            'engagement_distribution': {
                engagement: int(count)
                for engagement, count in engagement_counts.items()
            },
            'engagement_by_sentiment': {
                engagement: {
                    sentiment: int(count)
                    for sentiment, count in row.items()
                }
                for engagement, row in engagement_sentiment.iterrows()
            },
            'key_findings': {
                'most_common_engagement': engagement_counts.index[0],
                'least_common_engagement': engagement_counts.index[-1],
                'positive_engagement_rate': round(
                    engagement_sentiment.loc['supportive', 'positive'] /
                    engagement_sentiment.loc['supportive'].sum() * 100, 2
                )
            }
        }

    def analyze_themes(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå…³é”®ä¸»é¢˜"""
        # å±•å¼€ä¸»é¢˜åˆ—è¡¨
        all_themes = [theme for themes in df['key_themes'] for theme in themes]
        theme_counts = Counter(all_themes)

        # æŒ‰æƒ…æ„Ÿåˆ†ç±»çš„ä¸»é¢˜
        theme_by_sentiment = {
            'positive': [
                theme for i, themes in df[df['sentiment'] == 'positive']['key_themes'].items()
                for theme in themes
            ],
            'negative': [
                theme for i, themes in df[df['sentiment'] == 'negative']['key_themes'].items()
                for theme in themes
            ],
            'neutral': [
                theme for i, themes in df[df['sentiment'] == 'neutral']['key_themes'].items()
                for theme in themes
            ]
        }

        return {
            'overall_themes': {
                theme: count
                for theme, count in theme_counts.most_common()
            },
            'themes_by_sentiment': {
                sentiment: dict(Counter(themes).most_common(5))
                for sentiment, themes in theme_by_sentiment.items()
            },
            'key_insights': {
                'top_themes': list(dict(theme_counts.most_common(5)).keys()),
                'theme_diversity': len(theme_counts),
                'average_themes_per_comment': round(len(all_themes) / len(df), 2)
            }
        }

    async def analyze_relationship(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šè§†é¢‘çš„è¯„è®ºä¸­çš„å…³ç³»å’Œäº’åŠ¨

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: å…³ç³»åˆ†æç»“æœ

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        start_time = time.time()
        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            if batch_size <= 0 or batch_size > settings.MAX_BATCH_SIZE:
                raise ValidationError(
                    detail=f"batch_sizeå¿…é¡»åœ¨1å’Œ{settings.MAX_BATCH_SIZE}ä¹‹é—´",
                    field="batch_size"
                )

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå…³ç³»")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'relationship', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„å…³ç³»åˆ†æç»“æœ")

            print(analyzed_df)

            analysis_summary = {
                'trust_analysis': self.analyze_trust_metrics(analyzed_df),
                'tone_analysis': self.analyze_audience_tone(analyzed_df),
                'fandom_analysis': self.analyze_fandom_composition(analyzed_df),
                'segment_analysis': self.analyze_audience_segments(analyzed_df),
                'meta': {
                    'total_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'relationship',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }

            report_url = await self.generate_analysis_report(aweme_id,'relationship_report',analysis_summary)
            analysis_summary['report_url'] = report_url

            return analysis_summary
        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºå…³ç³»æ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºå…³ç³»æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"åˆ†æè§†é¢‘è¯„è®ºå…³ç³»æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def analyze_trust_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå—ä¼—ä¿¡ä»»åº¦æŒ‡æ ‡"""
        trust_counts = df['trust_level'].value_counts()
        total_comments = len(df)

        # è®¡ç®—ä¿¡ä»»åˆ†æ•°
        trust_score = (
                              (len(df[df['trust_level'] == 'loyal_fan']) * 1.0 +
                               len(df[df['trust_level'] == 'indifferent']) * 0.5 +
                               len(df[df['trust_level'] == 'skeptical']) * 0.0) / total_comments
                      ) * 100

        return {
            'trust_distribution': {
                level: {
                    'count': int(count),
                    'percentage': round(count / total_comments * 100, 2)
                }
                for level, count in trust_counts.items()
            },
            'trust_metrics': {
                'overall_trust_score': round(trust_score, 2),
                'loyal_fan_ratio': round(len(df[df['trust_level'] == 'loyal_fan']) / total_comments * 100, 2),
                'skepticism_ratio': round(len(df[df['trust_level'] == 'skeptical']) / total_comments * 100, 2)
            },
            'key_findings': {
                'dominant_trust_level': trust_counts.index[0],
                'trust_trend': 'positive' if trust_score > 60 else 'neutral' if trust_score > 40 else 'concerning'
            }
        }

    def analyze_audience_tone(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå—ä¼—æ€åº¦å€¾å‘"""
        tone_counts = df['tone_toward_influencer'].value_counts()
        tone_by_trust = pd.crosstab(df['tone_toward_influencer'], df['trust_level'])

        return {
            'tone_distribution': {
                tone: {
                    'count': int(count),
                    'percentage': round(count / len(df) * 100, 2)
                }
                for tone, count in tone_counts.items()
            },
            'tone_by_trust_level': {
                tone: {
                    trust_level: int(count)
                    for trust_level, count in row.items()
                }
                for tone, row in tone_by_trust.iterrows()
            },
            'sentiment_metrics': {
                'positivity_ratio': round(
                    len(df[df['tone_toward_influencer'] == 'supportive']) / len(df) * 100, 2),
                'criticism_ratio': round(
                    len(df[df['tone_toward_influencer'] == 'critical']) / len(df) * 100, 2)
            }
        }

    def analyze_fandom_composition(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æç²‰ä¸æ„æˆ"""
        fandom_counts = df['fandom_level'].value_counts()
        knowledge_counts = df['previous_knowledge'].value_counts()

        # ç²‰ä¸å¿ è¯šåº¦çŸ©é˜µ
        loyalty_matrix = pd.crosstab(
            df['fandom_level'],
            df['previous_knowledge']
        )

        return {
            'fandom_distribution': {
                level: {
                    'count': int(count),
                    'percentage': round(count / len(df) * 100, 2)
                }
                for level, count in fandom_counts.items()
            },
            'audience_history': {
                knowledge: {
                    'count': int(count),
                    'percentage': round(count / len(df) * 100, 2)
                }
                for knowledge, count in knowledge_counts.items()
            },
            'loyalty_patterns': {
                fandom: {
                    knowledge: int(count)
                    for knowledge, count in row.items()
                }
                for fandom, row in loyalty_matrix.iterrows()
            },
            'audience_metrics': {
                'superfan_ratio': round(len(df[df['fandom_level'] == 'superfan']) / len(df) * 100, 2),
                'new_audience_ratio': round(
                    len(df[df['previous_knowledge'] == 'new_follower']) / len(df) * 100, 2),
                'retention_indicator': round(
                    len(df[df['previous_knowledge'].isin(['returning_audience', 'long_time_fan'])]) / len(
                        df) * 100, 2)
            }
        }

    def analyze_audience_segments(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå—ä¼—ç»†åˆ†"""
        # åˆ›å»ºå¤åˆç‰¹å¾
        df['audience_segment'] = df.apply(self._determine_segment, axis=1)
        segment_counts = df['audience_segment'].value_counts()

        return {
            'segment_distribution': {
                segment: {
                    'count': int(count),
                    'percentage': round(count / len(df) * 100, 2)
                }
                for segment, count in segment_counts.items()
            },
            'segment_characteristics': self._analyze_segment_characteristics(df),
            'segment_engagement': self._analyze_segment_engagement(df),
        }

    def _determine_segment(self, row) -> str:
        """ç¡®å®šå—ä¼—æ‰€å±ç»†åˆ†"""
        if row['trust_level'] == 'loyal_fan' and row['fandom_level'] == 'superfan':
            return 'core_community'
        elif row['previous_knowledge'] == 'new_follower':
            return 'new_audience'
        elif row['tone_toward_influencer'] == 'critical':
            return 'critics'
        elif row['engagement_type'] in ['curiosity', 'casual_remark']:
            return 'casual_viewers'
        else:
            return 'regular_audience'

    def _analyze_segment_characteristics(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """åˆ†æå„ç»†åˆ†å—ä¼—çš„ç‰¹å¾"""
        segments = df['audience_segment'].unique()
        characteristics = {}

        for segment in segments:
            segment_df = df[df['audience_segment'] == segment]
            characteristics[segment] = {
                'trust_level': segment_df['trust_level'].mode()[0],
                'typical_engagement': segment_df['engagement_type'].mode()[0],
                'average_fandom_level': segment_df['fandom_level'].mode()[0]
            }

        return characteristics

    def _analyze_segment_engagement(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """åˆ†æå„ç»†åˆ†å—ä¼—çš„äº’åŠ¨æ¨¡å¼"""
        segments = df['audience_segment'].unique()
        engagement_patterns = {}

        for segment in segments:
            segment_df = df[df['audience_segment'] == segment]
            engagement_patterns[segment] = {
                'common_engagement_types': segment_df['engagement_type'].value_counts().to_dict(),
                'engagement_rate': round(len(segment_df) / len(df) * 100, 2)
            }

        return engagement_patterns

    async def fetch_quality_audience_info(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[
        str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘è¯„è®ºåŒºçš„å„ç±»ç²‰ä¸ç±»å‹æ•°æ®

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: è¯„è®ºåŒºloyal_fançš„ç²‰ä¸ä¿¡æ¯

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id or not isinstance(aweme_id, str):
                raise ValidationError(detail="aweme_idå¿…é¡»æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²", field="aweme_id")

            # è®°å½•å¼€å§‹è·å–è¯„è®ºåŒºç²‰ä¸ç±»å‹
            logger.info(f"å¼€å§‹è·å–è§†é¢‘ {aweme_id} çš„è¯„è®ºåŒºç²‰ä¸ç±»å‹")

            # è·å–è¯„è®º
            comment_collector = CommentCollector(self.tikhub_api_key, self.tikhub_base_url)
            comments = await comment_collector.collect_video_comments(aweme_id)

            if not comments or not comments.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æœªæ‰¾åˆ°è¯„è®º")
                return {
                    'aweme_id': aweme_id,
                    'comments': [],
                    'comment_count': 0,
                    'timestamp': datetime.now().isoformat()
                }

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå…³ç³»")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'relationship', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„å…³ç³»åˆ†æç»“æœ")

            analysis_summary = {
                'loyal_fans_info': self.extract_fan_group(analyzed_df, 'trust_level', 'loyal_fan', 'loyal_fans'),
                'superfans_info': self.extract_fan_group(analyzed_df, 'fandom_level', 'superfan', 'superfans'),
                'new_followers_info': self.extract_fan_group(analyzed_df, 'previous_knowledge', 'new_follower', 'new_followers'),
                'returning_audience_info': self.extract_fan_group(analyzed_df, 'previous_knowledge', 'returning_audience', 'returning_audience'),
                'long_time_fans_info': self.extract_fan_group(analyzed_df, 'previous_knowledge', 'long_time_fan', 'long_time_fans'),
                'meta': {
                    'total_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'relationship',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }

            }

            return analysis_summary

        except (ValidationError, ExternalAPIError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"è·å–è§†é¢‘è¯„è®ºåŒºç²‰ä¸ç±»å‹æ—¶å‡ºé”™: {str(e)}")
            raise

        except Exception as e:
            logger.error(f"è·å–è§†é¢‘è¯„è®ºåŒºç²‰ä¸ç±»å‹æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"è·å–è§†é¢‘è¯„è®ºåŒºç²‰ä¸ç±»å‹æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def extract_fan_group(self, df: pd.DataFrame, filter_column: str, filter_value: str, group_name: str) -> Dict[
        str, Any]:
        """
        æå–ç‰¹å®šç±»å‹çš„ç²‰ä¸ç¾¤ä½“ä¿¡æ¯

        Args:
            df (pd.DataFrame): åŒ…å«ç²‰ä¸æ•°æ®çš„DataFrame
            filter_column (str): ç”¨äºç­›é€‰çš„åˆ—å
            filter_value (str): ç­›é€‰æ¡ä»¶çš„å€¼
            group_name (str): ç²‰ä¸ç¾¤ä½“çš„åç§°

        Returns:
            Dict[str, Any]: åŒ…å«ç²‰ä¸æ€»æ•°å’Œç²‰ä¸è¯¦æƒ…çš„å­—å…¸
        """
        fan_group = df[df[filter_column] == filter_value]
        columns = ['commenter_uniqueId', 'text', 'commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']
        fan_group = fan_group[columns]

        return {
            f'total_{group_name}': len(fan_group),
            f'{group_name}': fan_group.to_dict('records')
        }

    async def analyze_toxicity(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šè§†é¢‘çš„è¯„è®ºæ¯’æ€§/æœ‰å®³æ€§/è¿è§„æ€§

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: æ¯’æ€§åˆ†æç»“æœ

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            if batch_size <= 0 or batch_size > settings.MAX_BATCH_SIZE:
                raise ValidationError(
                    detail=f"batch_sizeå¿…é¡»åœ¨1å’Œ{settings.MAX_BATCH_SIZE}ä¹‹é—´",
                    field="batch_size"
                )

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºæ¯’æ€§")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'toxicity', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„æ¯’æ€§åˆ†æç»“æœ")

            harmful_comments = self._track_harmful_comments(analyzed_df)

            analysis_summary = {
                'overview': self._analyze_toxicity_overview(harmful_comments, analyzed_df),
                'types': self._analyze_toxicity_types(analyzed_df),
                'severity': self._analyze_severity_levels(analyzed_df),
                'violations': self._analyze_guideline_violations(analyzed_df),
                'extreme_harmful_comments': harmful_comments,
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'toxicity',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }

            report_url = await self.generate_analysis_report(aweme_id,'toxicity_report',analysis_summary)
            analysis_summary['report_url'] = report_url

            return analysis_summary
        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºæ¯’æ€§æ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºæ¯’æ€§æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"åˆ†æè§†é¢‘è¯„è®ºæ¯’æ€§æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def _track_harmful_comments(self,df: pd.DataFrame):
        """Track comments that need attention"""
        # æŠŠseverity scoreè½¬æˆint
        df['severity_score'] = df['severity_score'].astype(int)
        harmful_mask = (
                (df['toxicity_level'] == 'high') |
                (df['report_worthiness'] == 'should_report') |
                ((df['severity_score'] >= 7) & (df['community_guidelines_violation'] == True))
        )

        harmful_comments = df[harmful_mask]

        harmful_comments = [
            {
                'comment_id': row['comment_id'],
                'text': row['text'],
                'toxicity_type': row['toxicity_type'],
                'severity_score': row['severity_score'],
                'needs_action': row['report_worthiness'] == 'should_report'
            }
            for _, row in harmful_comments.iterrows()
        ]
        return harmful_comments

    def _analyze_toxicity_overview(self,harmful_comments, df: pd.DataFrame) -> Dict[str, Any]:
        """Generate overview of toxicity levels"""
        total = len(df)

        return {
            'toxicity_levels': {
                level: {
                    'count': int(count),
                    'percentage': round((count / total) * 100, 2)
                }
                for level, count in df['toxicity_level'].value_counts().items()
            },
            'total_harmful_comments': len(harmful_comments),
            'requires_moderation': len(df[df['report_worthiness'] == 'should_report']),
            'guidelines_violations': int(df['community_guidelines_violation'].sum())
        }

    def _analyze_toxicity_types(self,df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze different types of toxic content"""
        type_counts = df['toxicity_type'].value_counts()

        return {
            'distribution': {
                ttype: int(count)
                for ttype, count in type_counts.items()
            },
            'most_common_type': type_counts.index[0],
            'hate_speech_count': int(type_counts.get('hate_speech', 0)),
            'harassment_count': int(type_counts.get('harassment', 0))
        }

    def _analyze_severity_levels(self,df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze severity scores of toxic content"""
        severity_scores = df['severity_score']

        return {
            'average_severity': round(float(severity_scores.mean()), 2),
            'high_severity_count': int(len(df[df['severity_score'] >= 7])),
            'severity_distribution': {
                'low': int(len(severity_scores[severity_scores <= 3])),
                'medium': int(len(severity_scores[(severity_scores > 3) & (severity_scores < 7)])),
                'high': int(len(severity_scores[severity_scores >= 7]))
            }
        }

    def _analyze_guideline_violations(self,df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze community guidelines violations"""
        violations = df[df['community_guidelines_violation'] == True]

        return {
            'total_violations': len(violations),
            'violation_rate': round(len(violations) / len(df) * 100, 2),
            'violations_by_type': violations['toxicity_type'].value_counts().to_dict()
        }

    async def fetch_negative_shop_reviews(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[
        str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„è´Ÿé¢å•†åº—è¯„è®º

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: åŒ…å«è´Ÿé¢å•†åº—è¯„è®ºçš„æ•°æ®

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºæ¯’æ€§")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'toxicity', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„æ¯’æ€§åˆ†æç»“æœ")

            result = self._get_negative_shop_reviews(analyzed_df)

            return {
                'negative_shop_reviews': result,
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'toxicity',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }
        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºæ¯’æ€§æ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºæ¯’æ€§æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"åˆ†æè§†é¢‘è¯„è®ºæ¯’æ€§æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def fetch_hate_speech(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„ä»‡æ¨å’Œæ”»å‡»æ€§è¯„è®º

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: åŒ…å«ä»‡æ¨å’Œæ”»å‡»æ€§è¯„è®ºçš„æ•°æ®

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºä»‡æ¨å’Œæ”»å‡»æ€§")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'toxicity', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„ä»‡æ¨å’Œæ”»å‡»æ€§è¯„è®ºåˆ†æç»“æœ")

            result = self._get_hate_comments(analyzed_df)

            return {
                'hate_comments': result,
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'toxicity',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }
        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºä»‡æ¨å’Œæ”»å‡»æ€§æ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºä»‡æ¨å’Œæ”»å‡»æ€§æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"åˆ†æè§†é¢‘è¯„è®ºä»‡æ¨å’Œæ”»å‡»æ€§æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def fetch_spam_content(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„åƒåœ¾å’Œæ¬ºè¯ˆæ€§è¯„è®º

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: åŒ…å«åƒåœ¾å’Œæ¬ºè¯ˆæ€§è¯„è®ºçš„æ•°æ®

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºåƒåœ¾å’Œæ¬ºè¯ˆæ€§")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'toxicity', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„åƒåœ¾å’Œæ¬ºè¯ˆæ€§è¯„è®ºåˆ†æç»“æœ")

            result = self._get_scam_comments(analyzed_df)

            return {
                'scam_comments': result,
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'toxicity',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }
        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºåƒåœ¾å’Œæ¬ºè¯ˆæ€§æ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºåƒåœ¾å’Œæ¬ºè¯ˆæ€§æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"åˆ†æè§†é¢‘è¯„è®ºåƒåœ¾å’Œæ¬ºè¯ˆæ€§æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def _get_negative_shop_reviews(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        è·å–è´Ÿé¢å•†åº—è¯„è®º

        Args:
            df (pd.DataFrame): åŒ…å«è¯„è®ºæ•°æ®çš„DataFrame

        Returns:
            Dict[str, Any]: åŒ…å«å„ç±»å‹è´Ÿé¢å•†åº—è¯„è®ºçš„æ•°æ®
        """
        negative_product_reviews = df[df['toxicity_type'] == 'negative_product_review'][['commenter_uniqueId', 'text','commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']]
        negative_shop_reviews = df[df['toxicity_type'] == 'negative_shop_review'][['commenter_uniqueId', 'text','commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']]
        negative_service_reviews = df[df['toxicity_type'] == 'negative_service_review'][['commenter_uniqueId', 'text','commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']]
        return {
            'negative_product_reviews':{
                'total_counts': len(negative_product_reviews),
                'user_info': negative_product_reviews.to_dict('records')
            },
            'negative_shop_reviews':{
                'total_counts': len(negative_shop_reviews),
                'user_info': negative_shop_reviews.to_dict('records')
            },
            'negative_service_reviews':{
                'total_counts': len(negative_service_reviews),
                'user_info': negative_service_reviews.to_dict('records')
            }
        }

    def _get_hate_comments(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        è·å–ä»‡æ¨å’Œæ”»å‡»æ€§è¯„è®º

        Args:
            df (pd.DataFrame): åŒ…å«è¯„è®ºæ•°æ®çš„DataFrame

        Returns:
            Dict[str, Any]: åŒ…å«å„ç±»å‹ä»‡æ¨è¯„è®ºçš„æ•°æ®
        """
        hate_speech_comments = df[df['toxicity_type'] == 'hate_speech'][['commenter_uniqueId', 'text','commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']]
        personal_attack_comments = df[df['toxicity_type'] == 'personal_attack'][['commenter_uniqueId', 'text','commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']]
        trolling_comments = df[df['toxicity_type'] == 'trolling'][['commenter_uniqueId', 'text','commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']]

        return {
            'hate_speech_comments':{
                'total_counts': len(hate_speech_comments),
                'user_info': hate_speech_comments.to_dict('records')
            },
            'personal_attack_comments':{
                'total_counts': len(personal_attack_comments),
                'user_info': personal_attack_comments.to_dict('records')
            },
            'trolling_comments':{
                'total_counts': len(trolling_comments),
                'user_info': trolling_comments.to_dict('records')
            }
        }

    def _get_scam_comments(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        è·å–åƒåœ¾å’Œæ¬ºè¯ˆæ€§è¯„è®º

        Args:
            df (pd.DataFrame): åŒ…å«è¯„è®ºæ•°æ®çš„DataFrame

        Returns:
            Dict[str, Any]: åŒ…å«å„ç±»å‹åƒåœ¾è¯„è®ºçš„æ•°æ®
        """

        misinfo_comments = df[df['toxicity_type'] == 'misinformation'][['commenter_uniqueId', 'text','commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']]
        spam_comments = df[df['toxicity_type'] == 'spam'][['commenter_uniqueId', 'text','commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']]
        return {
            'misInfo_comments':{
                'total_counts': len(misinfo_comments),
                'user_info': misinfo_comments.to_dict('records')
            },
            'spam_comments':{
                'total_counts': len(spam_comments),
                'user_info': spam_comments.to_dict('records')
            }
        }


async def main():
    # åˆ›å»ºä»£ç†
    agent = SentimentAgent()

    # åˆ†æè§†é¢‘è¯„è®ºæƒ…æ„Ÿ
    aweme_id = "123456789"
    sentiment_analysis = await agent.analyze_sentiment(aweme_id)
    print(sentiment_analysis)


if __name__ == "__main__":
    asyncio.run(main())
