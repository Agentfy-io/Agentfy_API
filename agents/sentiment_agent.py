# -*- coding: utf-8 -*-
"""
@file: sentiment_agent.py
@desc: å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€èˆ†æƒ…åˆ†æå’Œé»‘ç²‰è¯†åˆ«åŠŸèƒ½
@auth: Callmeiks
"""

import json
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
from collections import Counter
import asyncio
import time

import numpy as np
import pandas as pd
from dotenv import load_dotenv
import os

from app.utils.logger import setup_logger
from services.ai_models.chatgpt import ChatGPT
from services.ai_models.claude import Claude
from services.cleaner.comment_cleaner import CommentCleaner
from services.crawler.comment_crawler import CommentCollector
from services.crawler.video_crawler import VideoCollector
from services.cleaner.video_cleaner import VideoCleaner
from app.config import settings
from app.core.exceptions import ValidationError, ExternalAPIError, InternalServerError

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = setup_logger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class SentimentAgent:
    """å¤„ç†TikTokè¯„è®ºçš„ä»£ç†ç±»ï¼Œæä¾›è¯„è®ºè·å–ã€èˆ†æƒ…åˆ†æå’Œé»‘ç²‰è¯†åˆ«åŠŸèƒ½"""

    def __init__(self, tikhub_api_key: Optional[str] = None, tikhub_base_url: Optional[str] = None):
        """
        åˆå§‹åŒ–SentimentAgentï¼ŒåŠ è½½APIå¯†é’¥å’Œæç¤ºæ¨¡æ¿

        Args:
            tikhub_api_key: TikHub APIå¯†é’¥
            tikhub_base_url: TikHub APIåŸºç¡€URL
        """
        # åˆå§‹åŒ–AIæ¨¡å‹å®¢æˆ·ç«¯
        self.chatgpt = ChatGPT()
        self.claude = Claude()

        # ä¿å­˜TikHub APIé…ç½®
        self.tikhub_api_key = tikhub_api_key
        self.tikhub_base_url = tikhub_base_url

        # å¦‚æœæ²¡æœ‰æä¾›TikHub APIå¯†é’¥ï¼Œè®°å½•è­¦å‘Š
        if not self.tikhub_api_key:
            logger.warning("æœªæä¾›TikHub APIå¯†é’¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

        self.analysis_types = ['sentiment', 'relationship', 'toxicity']

        # åŠ è½½ç³»ç»Ÿå’Œç”¨æˆ·æç¤º
        self._load_system_prompts()
        self._load_user_prompts()

    def _load_system_prompts(self) -> None:
        """åŠ è½½ç³»ç»Ÿæç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.system_prompts = {
            'sentiment': """You are an AI trained to analyze social media comments on TikTok videos. Your task is to categorize and summarize how commenters react to a given video.
                For each comment in the provided list, analyze:
                1. Sentiment (positive, negative, or neutral)
                2. Emotion (e.g., excitement, amusement, frustration, curiosity, etc.)
                3. Engagement Type (e.g., supportive, critical, humorous, questioning, spam)
                4. Key Themes (e.g., reaction to video content, reference to trends, personal anecdotes, meme usage)
                5. Virality Indicators (e.g., high engagement phrases, common phrases from viral trends, use of emojis, repeated comment patterns)
                
                Return the analysis in the following JSON format, and comment_id should still remain the same as input data:
                [
                    {
                        "comment_id": "comment ID from input data",
                        "text": "comment text",
                        "sentiment": "positive/negative/neutral",
                        "emotion": "emotion_label",
                        "engagement_type": "supportive/critical/humorous/questioning/spam",
                        "key_themes": ["theme1", "theme2"],
                        "virality_indicators": ["emoji", "phrase", "meme_reference"],
                    }
                ]
                
                Guidelines:
                - Identify emotions based on wording, punctuation, and emojis.
                - Consider trends and viral elements influencing reactions.
                - If a comment uses humor or sarcasm, categorize it accordingly.
                - If a comment references a meme or viral phrase, list it as a key theme.
                - Comments containing excessive emojis, repeated phrases, or engagement-bait (e.g., "like if you agree") should be marked as potential virality indicators.
                
                Respond with a JSON array containing analysis for all comments.
                """,
            'relationship': """You are an AI trained to analyze audience sentiment and engagement toward an influencer or creator. Your task is to assess how commenters perceive and interact with the influencer.
                For each comment, analyze:
                1. **Trust Level:** (loyal fan, skeptical, indifferent)
                2. **Tone Toward Influencer/Brand:** (supportive, critical, neutral)
                3. **Fandom Level:** (casual viewer, superfan, first-time viewer)
                4. **Previous Knowledge of Influencer:** (new follower, returning audience, long-time fan)
                5. **Engagement Type:** (praise, criticism, joke, curiosity, casual remark)
                
                **JSON Output Format:**
                [
                    {
                        "comment_id": "comment ID from input data",
                        "text": "comment text",
                        "trust_level": "loyal_fan/skeptical/indifferent",
                        "tone_toward_influencer": "supportive/critical/neutral",
                        "fandom_level": "casual_viewer/superfan/first_time_viewer",
                        "previous_knowledge": "new_follower/returning_audience/long_time_fan",
                        "engagement_type": "praise/criticism/joke/curiosity/casual_remark",
                    }
                ]
                
                **Guidelines:**
                - Identify loyalty through repeated engagement or references to past content.
                - Detect tone based on word choice, punctuation, and emojis.
                - Recognize when users joke, question, or provide constructive criticism.
                - Assess engagement depth (quick reaction vs. detailed discussion).
                
                Respond with a JSON array containing the analysis for all comments.
                """
        }

    def _load_user_prompts(self) -> None:
        """åŠ è½½ç”¨æˆ·æç¤ºç”¨äºä¸åŒçš„è¯„è®ºåˆ†æç±»å‹"""
        self.user_prompts = {
            'sentiment': {
                'description': "sentiment and engagement analysis",
            },
            'relationship': {
                'description': "relationship and engagement analysis",
            }
        }

    async def fetch_video_comments(self, aweme_id: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘çš„æ¸…ç†åçš„è¯„è®ºæ•°æ®

        Args:
            aweme_id (str): è§†é¢‘ID

        Returns:
            Dict[str, Any]: æ¸…ç†åçš„è¯„è®ºæ•°æ®ï¼ŒåŒ…å«è§†é¢‘IDå’Œè¯„è®ºåˆ—è¡¨

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id or not isinstance(aweme_id, str):
                raise ValidationError(detail="aweme_idå¿…é¡»æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²", field="aweme_id")

            # è®°å½•å¼€å§‹è·å–è¯„è®º
            logger.info(f"å¼€å§‹è·å–è§†é¢‘ {aweme_id} çš„è¯„è®º")

            # è·å–è¯„è®º
            comment_collector = CommentCollector(self.tikhub_api_key, self.tikhub_base_url)
            comments = await comment_collector.collect_video_comments(aweme_id)

            if not comments or not comments.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æœªæ‰¾åˆ°è¯„è®º")
                return {
                    'aweme_id': aweme_id,
                    'comments': [],
                    'comment_count': 0,
                    'timestamp': datetime.now().isoformat()
                }

            # æ¸…æ´—è¯„è®º
            comment_cleaner = CommentCleaner()
            cleaned_comments = await comment_cleaner.clean_video_comments(comments)
            cleaned_comments = cleaned_comments.get('comments', [])

            processing_time = time.time() - start_time

            result = {
                'aweme_id': aweme_id,
                'comments': cleaned_comments,
                'comment_count': len(cleaned_comments),
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': round(processing_time * 1000, 2)
            }

            logger.info(f"æˆåŠŸè·å–è§†é¢‘ {aweme_id} çš„è¯„è®º: {len(cleaned_comments)} æ¡ï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            return result

        except (ValidationError, ExternalAPIError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‡ºé”™: {str(e)}")
            raise

        except Exception as e:
            logger.error(f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"è·å–è§†é¢‘è¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def analyze_comments_batch(
            self,
            df: pd.DataFrame,
            analysis_type: str,
            batch_size: int = 30,
            concurrency: int = 5
    ) -> pd.DataFrame:
        """
        åˆ†æ‰¹åˆ†æè¯„è®ºï¼Œä»¥é˜²æ­¢ä¸€æ¬¡æ€§å‘é€è¿‡å¤šæ•°æ®

        Args:
            df (pd.DataFrame): åŒ…å«è¯„è®ºæ•°æ®çš„DataFrameï¼Œéœ€åŒ…å«å­—æ®µï¼štext
            analysis_type (str): é€‰æ‹©åˆ†æç±»å‹ (purchase_intent)
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            pd.DataFrame: ç»“æœåˆå¹¶å›åŸå§‹DataFrame

        Raises:
            ValidationError: å½“analysis_typeæ— æ•ˆæˆ–dfä¸ºç©ºæ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        try:
            # å‚æ•°éªŒè¯
            if analysis_type not in self.analysis_types:
                raise ValidationError(
                    detail=f"æ— æ•ˆçš„åˆ†æç±»å‹: {analysis_type}. è¯·ä» {self.analysis_types} ä¸­é€‰æ‹©",
                    field="analysis_type"
                )

            if df.empty:
                raise ValidationError(detail="DataFrameä¸èƒ½ä¸ºç©º", field="df")

            if 'text' not in df.columns:
                raise ValidationError(detail="DataFrameå¿…é¡»åŒ…å«'text'åˆ—", field="df")

            # éªŒè¯å’Œè°ƒæ•´æ‰¹å¤„ç†å‚æ•°
            batch_size = min(batch_size, settings.MAX_BATCH_SIZE)
            concurrency = min(concurrency, 10)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º10

            # åˆ†æ‰¹å¤„ç†DataFrame
            num_splits = max(1, len(df) // batch_size + (1 if len(df) % batch_size > 0 else 0))
            comment_batches = np.array_split(df, num_splits)
            logger.info(
                f"ğŸš€ å¼€å§‹ {analysis_type} åˆ†æï¼Œå…± {len(comment_batches)} æ‰¹ï¼Œæ¯æ‰¹çº¦ {len(comment_batches[0]) if len(comment_batches) > 0 else 0} æ¡è¯„è®º"
            )

            # å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼ˆæ¯æ¬¡æœ€å¤š `concurrency` ç»„ï¼‰
            results = []
            for i in range(0, len(comment_batches), concurrency):
                batch_group = comment_batches[i:i + concurrency]

                # è®°å½•å½“å‰å¹¶å‘ç»„çš„èŒƒå›´
                batch_indices = [
                    f"{batch.index[0]}-{batch.index[-1]}" if not batch.empty else "-"
                    for batch in batch_group
                ]
                logger.info(
                    f"âš¡ å¤„ç†æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)}ï¼Œè¯„è®ºç´¢å¼•èŒƒå›´: {batch_indices}"
                )

                # å¹¶å‘æ‰§è¡Œ `concurrency` ä¸ªä»»åŠ¡
                tasks = [
                    self._analyze_aspect(analysis_type, batch[['text', 'comment_id']].to_dict('records'))
                    for batch in batch_group if not batch.empty
                ]

                if not tasks:
                    continue

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # å¤„ç†ç»“æœï¼Œè¿‡æ»¤æ‰å¼‚å¸¸
                valid_results = []
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"æ‰¹æ¬¡ {i + j + 1} åˆ†æå¤±è´¥: {str(result)}")
                    else:
                        valid_results.append(result)

                if len(valid_results) != len(batch_group):
                    logger.warning(
                        f"æ‰¹æ¬¡ {i + 1} è‡³ {i + len(batch_group)} ä¸­æœ‰ {len(batch_group) - len(valid_results)} ä¸ªæ‰¹æ¬¡åˆ†æå¤±è´¥"
                    )

                results.extend(valid_results)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
            if not results:
                raise InternalServerError("æ‰€æœ‰æ‰¹æ¬¡åˆ†æå‡å¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆç»“æœ")

            # åˆå¹¶æ‰€æœ‰åˆ†æç»“æœ
            try:
                # å°†æ‰€æœ‰ç»“æœæ‰å¹³åŒ–ä¸ºå•ä¸ªåˆ—è¡¨
                all_results = []
                for batch_result in results:
                    if isinstance(batch_result, list):
                        all_results.extend(batch_result)

                # åˆ›å»ºç»“æœDataFrame
                analysis_df = pd.DataFrame(all_results)

                # ç¡®ä¿comment_idåˆ—å­˜åœ¨
                if 'comment_id' not in analysis_df.columns:
                    logger.warning("åˆ†æç»“æœç¼ºå°‘comment_idåˆ—ï¼Œæ— æ³•æ­£ç¡®åˆå¹¶")
                    # æ·»åŠ ç´¢å¼•ä½œä¸ºä¸´æ—¶åˆ—
                    analysis_df['temp_index'] = range(len(analysis_df))
                    df['temp_index'] = range(len(df))
                    # åŸºäºç´¢å¼•åˆå¹¶
                    merged_df = pd.merge(df, analysis_df, on='temp_index', how='left')
                    merged_df = merged_df.drop('temp_index', axis=1)
                else:
                    # åŸºäºcomment_idåˆå¹¶
                    merged_df = pd.merge(df, analysis_df, on='comment_id', how='left')

                # å¤„ç†é‡å¤çš„textåˆ—
                if 'text_y' in merged_df.columns:
                    merged_df = merged_df.drop('text_y', axis=1)
                    merged_df = merged_df.rename(columns={'text_x': 'text'})

                logger.info(f"âœ… {analysis_type} åˆ†æå®Œæˆï¼æ€»è®¡ {len(merged_df)} æ¡æ•°æ®")
                return merged_df

            except Exception as e:
                logger.error(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
                raise InternalServerError(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except InternalServerError:
            # ç›´æ¥å‘ä¸Šä¼ é€’å†…éƒ¨æœåŠ¡å™¨é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def _analyze_aspect(
            self,
            aspect_type: str,
            comment_data: List[Dict[str, Any]],
    ) -> Optional[List[Dict[str, Any]]]:
        """
        é€šç”¨åˆ†ææ–¹æ³•ï¼Œæ ¹æ®ä¸åŒçš„åˆ†æç±»å‹è°ƒç”¨ChatGPTæˆ–Claude AIæ¨¡å‹ã€‚

        Args:
            aspect_type (str): éœ€è¦åˆ†æçš„ç±»å‹ (purchase_intent)
            comment_data (List[Dict[str, Any]]): éœ€è¦åˆ†æçš„è¯„è®ºåˆ—è¡¨

        Returns:
            Optional[List[Dict[str, Any]]]: AIè¿”å›çš„åˆ†æç»“æœï¼Œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

        Raises:
            ValidationError: å½“aspect_typeæ— æ•ˆæ—¶
            ExternalAPIError: å½“è°ƒç”¨AIæœåŠ¡æ—¶å‡ºé”™
        """
        try:
            if aspect_type not in self.analysis_types:
                raise ValidationError(detail=f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {aspect_type}", field="aspect_type")

            if not comment_data:
                logger.warning("è¯„è®ºæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
                return []

            # è·å–åˆ†æçš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤º
            aspect_config = self.user_prompts[aspect_type]
            sys_prompt = self.system_prompts[aspect_type]
            user_prompt = (
                f"Analyze the {aspect_config['description']} for the following comments:\n"
                f"{json.dumps(comment_data, ensure_ascii=False)}"
            )

            # ä¸ºé¿å…tokené™åˆ¶ï¼Œé™åˆ¶è¯„è®ºæ–‡æœ¬é•¿åº¦
            for comment in comment_data:
                if 'text' in comment and len(comment['text']) > 1000:
                    comment['text'] = comment['text'][:997] + "..."

            # è°ƒç”¨AIè¿›è¡Œåˆ†æï¼Œä¼˜å…ˆä½¿ç”¨ChatGPTï¼Œå¤±è´¥æ—¶å°è¯•Claude
            try:
                response = await self.chatgpt.chat(
                    system_prompt=sys_prompt,
                    user_prompt=user_prompt
                )

                # è§£æChatGPTè¿”å›çš„ç»“æœ
                analysis_results = response["choices"][0]["message"]["content"].strip()

            except ExternalAPIError as e:
                logger.warning(f"ChatGPTåˆ†æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Claude: {str(e)}")
                try:
                    # å°è¯•ä½¿ç”¨Claudeä½œä¸ºå¤‡ä»½
                    response = await self.claude.chat(
                        system_prompt=sys_prompt,
                        user_prompt=user_prompt
                    )
                    analysis_results = response["choices"][0]["message"]["content"].strip()
                except Exception as claude_error:
                    logger.error(f"Claudeåˆ†æä¹Ÿå¤±è´¥: {str(claude_error)}")
                    raise ExternalAPIError(
                        detail="æ‰€æœ‰AIæœåŠ¡å‡æ— æ³•å®Œæˆåˆ†æ",
                        service="AI"
                    )

            # å¤„ç†è¿”å›çš„JSONæ ¼å¼ï¼ˆå¯èƒ½åŒ…å«åœ¨Markdownä»£ç å—ä¸­ï¼‰
            analysis_results = re.sub(
                r"```json\n|\n```|```|\n",
                "",
                analysis_results.strip()
            )

            try:
                analysis_result = json.loads(analysis_results)
                return analysis_result
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æé”™è¯¯: {str(e)}, åŸå§‹å†…å®¹: {analysis_results[:200]}...")
                raise ExternalAPIError(
                    detail="AIè¿”å›çš„ç»“æœæ— æ³•è§£æä¸ºJSON",
                    service="AI",
                    original_error=e
                )

        except ValidationError:
            # ç›´æ¥å‘ä¸Šä¼ é€’éªŒè¯é”™è¯¯
            raise
        except ExternalAPIError:
            # ç›´æ¥å‘ä¸Šä¼ é€’APIé”™è¯¯
            raise
        except Exception as e:
            logger.error(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(f"åˆ†æè¯„è®ºæ–¹é¢æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    async def analyze_sentiment(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šè§†é¢‘çš„è¯„è®ºæƒ…æ„Ÿ

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: æƒ…æ„Ÿåˆ†æç»“æœ

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """

        start_time = time.time()

        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            if batch_size <= 0 or batch_size > settings.MAX_BATCH_SIZE:
                raise ValidationError(
                    detail=f"batch_sizeå¿…é¡»åœ¨1å’Œ{settings.MAX_BATCH_SIZE}ä¹‹é—´",
                    field="batch_size"
                )

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºæƒ…æ„Ÿ")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'sentiment', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„æƒ…æ„Ÿåˆ†æç»“æœ")
            analysis_summary = {
                'sentiment_distribution': self.analyze_sentiment_distribution(analyzed_df),
                'emotion_patterns': self.analyze_emotion_patterns(analyzed_df),
                'engagement_patterns': self.analyze_engagement_patterns(analyzed_df),
                'themes': self.analyze_themes(analyzed_df),
                'meta': {
                    'total_analyzed_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'sentiment',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }

            return analysis_summary
        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºæƒ…æ„Ÿæ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºæƒ…æ„Ÿæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"åˆ†æè§†é¢‘è¯„è®ºæƒ…æ„Ÿæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def analyze_sentiment_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ"""
        sentiment_counts = df['sentiment'].value_counts()
        total_comments = len(df)

        return {
            'distribution': {
                sentiment: {
                    'count': int(count),
                    'percentage': round(count / total_comments * 100, 2)
                }
                for sentiment, count in sentiment_counts.items()
            },
            'dominant_sentiment': sentiment_counts.index[0],
            'sentiment_ratio': {
                'positive_ratio': round(len(df[df['sentiment'] == 'positive']) / total_comments * 100, 2),
                'negative_ratio': round(len(df[df['sentiment'] == 'negative']) / total_comments * 100, 2),
                'neutral_ratio': round(len(df[df['sentiment'] == 'neutral']) / total_comments * 100, 2)
            }
        }

    def analyze_emotion_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææƒ…ç»ªæ¨¡å¼"""
        emotion_counts = df['emotion'].value_counts()
        sentiment_emotion_matrix = pd.crosstab(df['sentiment'], df['emotion'])

        return {
            'dominant_emotions': {
                'overall': emotion_counts.index[0],
                'by_sentiment': {
                    sentiment: sentiment_emotion_matrix.loc[sentiment].idxmax()
                    for sentiment in sentiment_emotion_matrix.index
                }
            },
            'emotion_distribution': {
                emotion: int(count)
                for emotion, count in emotion_counts.items()
            },
            'emotion_sentiment_correlation': {
                sentiment: {
                    emotion: int(count)
                    for emotion, count in row.items()
                }
                for sentiment, row in sentiment_emotion_matrix.iterrows()
            }
        }

    def analyze_engagement_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æäº’åŠ¨æ¨¡å¼"""
        engagement_counts = df['engagement_type'].value_counts()
        engagement_sentiment = pd.crosstab(df['engagement_type'], df['sentiment'])

        return {
            'engagement_distribution': {
                engagement: int(count)
                for engagement, count in engagement_counts.items()
            },
            'engagement_by_sentiment': {
                engagement: {
                    sentiment: int(count)
                    for sentiment, count in row.items()
                }
                for engagement, row in engagement_sentiment.iterrows()
            },
            'key_findings': {
                'most_common_engagement': engagement_counts.index[0],
                'least_common_engagement': engagement_counts.index[-1],
                'positive_engagement_rate': round(
                    engagement_sentiment.loc['supportive', 'positive'] /
                    engagement_sentiment.loc['supportive'].sum() * 100, 2
                )
            }
        }

    def analyze_themes(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå…³é”®ä¸»é¢˜"""
        # å±•å¼€ä¸»é¢˜åˆ—è¡¨
        all_themes = [theme for themes in df['key_themes'] for theme in themes]
        theme_counts = Counter(all_themes)

        # æŒ‰æƒ…æ„Ÿåˆ†ç±»çš„ä¸»é¢˜
        theme_by_sentiment = {
            'positive': [
                theme for i, themes in df[df['sentiment'] == 'positive']['key_themes'].items()
                for theme in themes
            ],
            'negative': [
                theme for i, themes in df[df['sentiment'] == 'negative']['key_themes'].items()
                for theme in themes
            ],
            'neutral': [
                theme for i, themes in df[df['sentiment'] == 'neutral']['key_themes'].items()
                for theme in themes
            ]
        }

        return {
            'overall_themes': {
                theme: count
                for theme, count in theme_counts.most_common()
            },
            'themes_by_sentiment': {
                sentiment: dict(Counter(themes).most_common(5))
                for sentiment, themes in theme_by_sentiment.items()
            },
            'key_insights': {
                'top_themes': list(dict(theme_counts.most_common(5)).keys()),
                'theme_diversity': len(theme_counts),
                'average_themes_per_comment': round(len(all_themes) / len(df), 2)
            }
        }

    async def analyze_relationship(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šè§†é¢‘çš„è¯„è®ºä¸­çš„å…³ç³»å’Œäº’åŠ¨

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: å…³ç³»åˆ†æç»“æœ

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
            InternalServerError: å½“åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
        """
        start_time = time.time()
        try:
            if not aweme_id:
                raise ValidationError(detail="aweme_idä¸èƒ½ä¸ºç©º", field="aweme_id")

            if batch_size <= 0 or batch_size > settings.MAX_BATCH_SIZE:
                raise ValidationError(
                    detail=f"batch_sizeå¿…é¡»åœ¨1å’Œ{settings.MAX_BATCH_SIZE}ä¹‹é—´",
                    field="batch_size"
                )

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå…³ç³»")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'relationship', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„å…³ç³»åˆ†æç»“æœ")

            analysis_summary = {
                'trust_analysis': self.analyze_trust_metrics(analyzed_df),
                'tone_analysis': self.analyze_audience_tone(analyzed_df),
                'fandom_analysis': self.analyze_fandom_composition(analyzed_df),
                'segment_analysis': self.analyze_audience_segments(analyzed_df),
                'meta': {
                    'total_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'relationship',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }
            }

            return analysis_summary
        except (ValidationError, ExternalAPIError, InternalServerError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºå…³ç³»æ—¶å‡ºé”™: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"åˆ†æè§†é¢‘è¯„è®ºå…³ç³»æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"åˆ†æè§†é¢‘è¯„è®ºå…³ç³»æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def analyze_trust_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå—ä¼—ä¿¡ä»»åº¦æŒ‡æ ‡"""
        trust_counts = df['trust_level'].value_counts()
        total_comments = len(df)

        # è®¡ç®—ä¿¡ä»»åˆ†æ•°
        trust_score = (
                              (len(df[df['trust_level'] == 'loyal_fan']) * 1.0 +
                               len(df[df['trust_level'] == 'indifferent']) * 0.5 +
                               len(df[df['trust_level'] == 'skeptical']) * 0.0) / total_comments
                      ) * 100

        return {
            'trust_distribution': {
                level: {
                    'count': int(count),
                    'percentage': round(count / total_comments * 100, 2)
                }
                for level, count in trust_counts.items()
            },
            'trust_metrics': {
                'overall_trust_score': round(trust_score, 2),
                'loyal_fan_ratio': round(len(df[df['trust_level'] == 'loyal_fan']) / total_comments * 100, 2),
                'skepticism_ratio': round(len(df[df['trust_level'] == 'skeptical']) / total_comments * 100, 2)
            },
            'key_findings': {
                'dominant_trust_level': trust_counts.index[0],
                'trust_trend': 'positive' if trust_score > 60 else 'neutral' if trust_score > 40 else 'concerning'
            }
        }

    def analyze_audience_tone(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå—ä¼—æ€åº¦å€¾å‘"""
        tone_counts = df['tone_toward_influencer'].value_counts()
        tone_by_trust = pd.crosstab(df['tone_toward_influencer'], df['trust_level'])

        return {
            'tone_distribution': {
                tone: {
                    'count': int(count),
                    'percentage': round(count / len(df) * 100, 2)
                }
                for tone, count in tone_counts.items()
            },
            'tone_by_trust_level': {
                tone: {
                    trust_level: int(count)
                    for trust_level, count in row.items()
                }
                for tone, row in tone_by_trust.iterrows()
            },
            'sentiment_metrics': {
                'positivity_ratio': round(
                    len(df[df['tone_toward_influencer'] == 'supportive']) / len(df) * 100, 2),
                'criticism_ratio': round(
                    len(df[df['tone_toward_influencer'] == 'critical']) / len(df) * 100, 2)
            }
        }

    def analyze_fandom_composition(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æç²‰ä¸æ„æˆ"""
        fandom_counts = df['fandom_level'].value_counts()
        knowledge_counts = df['previous_knowledge'].value_counts()

        # ç²‰ä¸å¿ è¯šåº¦çŸ©é˜µ
        loyalty_matrix = pd.crosstab(
            df['fandom_level'],
            df['previous_knowledge']
        )

        return {
            'fandom_distribution': {
                level: {
                    'count': int(count),
                    'percentage': round(count / len(df) * 100, 2)
                }
                for level, count in fandom_counts.items()
            },
            'audience_history': {
                knowledge: {
                    'count': int(count),
                    'percentage': round(count / len(df) * 100, 2)
                }
                for knowledge, count in knowledge_counts.items()
            },
            'loyalty_patterns': {
                fandom: {
                    knowledge: int(count)
                    for knowledge, count in row.items()
                }
                for fandom, row in loyalty_matrix.iterrows()
            },
            'audience_metrics': {
                'superfan_ratio': round(len(df[df['fandom_level'] == 'superfan']) / len(df) * 100, 2),
                'new_audience_ratio': round(
                    len(df[df['previous_knowledge'] == 'new_follower']) / len(df) * 100, 2),
                'retention_indicator': round(
                    len(df[df['previous_knowledge'].isin(['returning_audience', 'long_time_fan'])]) / len(
                        df) * 100, 2)
            }
        }

    def analyze_audience_segments(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå—ä¼—ç»†åˆ†"""
        # åˆ›å»ºå¤åˆç‰¹å¾
        df['audience_segment'] = df.apply(self._determine_segment, axis=1)
        segment_counts = df['audience_segment'].value_counts()

        return {
            'segment_distribution': {
                segment: {
                    'count': int(count),
                    'percentage': round(count / len(df) * 100, 2)
                }
                for segment, count in segment_counts.items()
            },
            'segment_characteristics': self._analyze_segment_characteristics(df),
            'segment_engagement': self._analyze_segment_engagement(df),
        }

    def _determine_segment(self, row) -> str:
        """ç¡®å®šå—ä¼—æ‰€å±ç»†åˆ†"""
        if row['trust_level'] == 'loyal_fan' and row['fandom_level'] == 'superfan':
            return 'core_community'
        elif row['previous_knowledge'] == 'new_follower':
            return 'new_audience'
        elif row['tone_toward_influencer'] == 'critical':
            return 'critics'
        elif row['engagement_type'] in ['curiosity', 'casual_remark']:
            return 'casual_viewers'
        else:
            return 'regular_audience'

    def _analyze_segment_characteristics(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """åˆ†æå„ç»†åˆ†å—ä¼—çš„ç‰¹å¾"""
        segments = df['audience_segment'].unique()
        characteristics = {}

        for segment in segments:
            segment_df = df[df['audience_segment'] == segment]
            characteristics[segment] = {
                'trust_level': segment_df['trust_level'].mode()[0],
                'typical_engagement': segment_df['engagement_type'].mode()[0],
                'average_fandom_level': segment_df['fandom_level'].mode()[0]
            }

        return characteristics

    def _analyze_segment_engagement(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """åˆ†æå„ç»†åˆ†å—ä¼—çš„äº’åŠ¨æ¨¡å¼"""
        segments = df['audience_segment'].unique()
        engagement_patterns = {}

        for segment in segments:
            segment_df = df[df['audience_segment'] == segment]
            engagement_patterns[segment] = {
                'common_engagement_types': segment_df['engagement_type'].value_counts().to_dict(),
                'engagement_rate': round(len(segment_df) / len(df) * 100, 2)
            }

        return engagement_patterns

    async def fetch_audience_info(self, aweme_id: str, batch_size: int = 30, concurrency: int = 5) -> Dict[
        str, Any]:
        """
        è·å–æŒ‡å®šè§†é¢‘è¯„è®ºåŒºçš„å„ç±»ç²‰ä¸ç±»å‹æ•°æ®

        Args:
            aweme_id (str): è§†é¢‘ID
            batch_size (int, optional): æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤30
            concurrency (int, optional): å¹¶å‘å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼Œé»˜è®¤5

        Returns:
            Dict[str, Any]: è¯„è®ºåŒºloyal_fançš„ç²‰ä¸ä¿¡æ¯

        Raises:
            ValidationError: å½“aweme_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶
            ExternalAPIError: å½“ç½‘ç»œè¿æ¥å¤±è´¥æ—¶
        """
        start_time = time.time()

        try:
            if not aweme_id or not isinstance(aweme_id, str):
                raise ValidationError(detail="aweme_idå¿…é¡»æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²", field="aweme_id")

            # è®°å½•å¼€å§‹è·å–è¯„è®ºåŒºç²‰ä¸ç±»å‹
            logger.info(f"å¼€å§‹è·å–è§†é¢‘ {aweme_id} çš„è¯„è®ºåŒºç²‰ä¸ç±»å‹")

            # è·å–è¯„è®º
            comment_collector = CommentCollector(self.tikhub_api_key, self.tikhub_base_url)
            comments = await comment_collector.collect_video_comments(aweme_id)

            if not comments or not comments.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æœªæ‰¾åˆ°è¯„è®º")
                return {
                    'aweme_id': aweme_id,
                    'comments': [],
                    'comment_count': 0,
                    'timestamp': datetime.now().isoformat()
                }

            # è·å–æ¸…ç†åçš„è¯„è®ºæ•°æ®
            comments_data = await self.fetch_video_comments(aweme_id)

            if not comments_data.get('comments'):
                logger.warning(f"è§†é¢‘ {aweme_id} æ²¡æœ‰è¯„è®ºæ•°æ®")
                return {
                    'aweme_id': aweme_id,
                    'error': 'No comments found',
                    'analysis_timestamp': datetime.now().isoformat(),
                }

            comments_df = pd.DataFrame(comments_data['comments'])

            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘ {aweme_id} çš„è¯„è®ºå…³ç³»")
            analyzed_df = await self.analyze_comments_batch(comments_df, 'relationship', batch_size, concurrency)

            if analyzed_df.empty:
                raise InternalServerError("æœªè·å¾—æœ‰æ•ˆçš„å…³ç³»åˆ†æç»“æœ")

            analysis_summary = {
                'loyal_fans_info': self.fetch_loyal_fans_info(analyzed_df),
                'superfans_info': self.fetch_superfans_info(analyzed_df),
                'new_followers_info': self.fetch_new_followers_info(analyzed_df),
                'returning_audience_info': self.fetch_returning_audience_info(analyzed_df),
                'long_time_fans_info': self.fetch_long_time_fans_info(analyzed_df),
                'meta': {
                    'total_comments': len(analyzed_df),
                    'aweme_id': aweme_id,
                    'analysis_type': 'relationship',
                    'analysis_timestamp': datetime.now().isoformat(),
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2)
                }

            }

            return analysis_summary

        except (ValidationError, ExternalAPIError) as e:
            # ç›´æ¥å‘ä¸Šä¼ é€’è¿™äº›å·²å¤„ç†çš„é”™è¯¯
            logger.error(f"è·å–è§†é¢‘è¯„è®ºåŒºç²‰ä¸ç±»å‹æ—¶å‡ºé”™: {str(e)}")
            raise

        except Exception as e:
            logger.error(f"è·å–è§†é¢‘è¯„è®ºåŒºç²‰ä¸ç±»å‹æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            raise InternalServerError(detail=f"è·å–è§†é¢‘è¯„è®ºåŒºç²‰ä¸ç±»å‹æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")

    def extract_fan_group(self, df: pd.DataFrame, filter_column: str, filter_value: str, group_name: str) -> Dict[
        str, Any]:
        """
        æå–ç‰¹å®šç±»å‹çš„ç²‰ä¸ç¾¤ä½“ä¿¡æ¯

        Args:
            df (pd.DataFrame): åŒ…å«ç²‰ä¸æ•°æ®çš„DataFrame
            filter_column (str): ç”¨äºç­›é€‰çš„åˆ—å
            filter_value (str): ç­›é€‰æ¡ä»¶çš„å€¼
            group_name (str): ç²‰ä¸ç¾¤ä½“çš„åç§°

        Returns:
            Dict[str, Any]: åŒ…å«ç²‰ä¸æ€»æ•°å’Œç²‰ä¸è¯¦æƒ…çš„å­—å…¸
        """
        fan_group = df[df[filter_column] == filter_value]
        columns = ['commenter_name', 'text', 'commenter_secuid', 'ins_id', 'twitter_id', 'commenter_region']
        fan_group = fan_group[columns]

        return {
            f'total_{group_name}': len(fan_group),
            f'{group_name}': fan_group.to_dict('records')
        }

    def fetch_loyal_fans_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–å¿ å®ç²‰ä¸ä¿¡æ¯"""
        return self.extract_fan_group(df, 'trust_level', 'loyal_fan', 'loyal_fans')

    def fetch_superfans_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–è¶…çº§ç²‰ä¸ä¿¡æ¯"""
        return self.extract_fan_group(df, 'fandom_level', 'superfan', 'superfans')

    def fetch_new_followers_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–æ–°å…³æ³¨è€…ä¿¡æ¯"""
        return self.extract_fan_group(df, 'previous_knowledge', 'new_follower', 'new_followers')

    def fetch_returning_audience_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–å›å¤´è§‚ä¼—ä¿¡æ¯"""
        return self.extract_fan_group(df, 'previous_knowledge', 'returning_audience', 'returning_audience')

    def fetch_long_time_fans_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–é•¿æœŸç²‰ä¸ä¿¡æ¯"""
        return self.extract_fan_group(df, 'previous_knowledge', 'long_time_fan', 'long_time_fans')


async def main():
    # åˆ›å»ºä»£ç†
    agent = SentimentAgent()

    # åˆ†æè§†é¢‘è¯„è®ºæƒ…æ„Ÿ
    aweme_id = "123456789"
    sentiment_analysis = await agent.analyze_sentiment(aweme_id)
    print(sentiment_analysis)

if __name__ == "__main__":
    asyncio.run(main())


