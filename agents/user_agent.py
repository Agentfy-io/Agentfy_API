import json
import os
import re
import time
from datetime import datetime
import matplotlib.pyplot as plt
import pandas as pd
import asyncio
from app.config import settings
from services.ai_models.chatgpt import ChatGPT
from services.ai_models.claude import Claude
from typing import Dict, Any, List, Optional, AsyncGenerator
from dotenv import load_dotenv
from app.utils.logger import logger
from services.crawler.user_crawler import UserCollector
from services.cleaner.user_cleaner import UserCleaner

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class UserAgent:
    """ç”¨æˆ·/è¾¾äººåˆ†æå™¨
    åŸºç¡€ç”¨æˆ·æ•°æ®å±•ç¤º ï¼ˆç²‰ä¸æ•°ã€ç‚¹èµæ•°ã€è¯„è®ºæ•°ã€è½¬å‘æ•°ã€ä½œå“æ•°ã€å…³æ³¨æ•°ã€è·èµæ•°ã€è·èµç‡ã€è¯„è®ºç‡ã€è½¬å‘ç‡ã€äº’åŠ¨ç‡,å•†ä¸š
    ç”¨æˆ·å‘å¸ƒä½œå“ç»Ÿè®¡ (ä½œå“ç±»å‹ã€ä½œå“é£æ ¼ã€å¹³å‡ç‚¹èµæ•°ã€å¹³å‡è¯„è®ºæ•°ã€å¹³å‡è½¬å‘æ•°ã€å¹³å‡äº’åŠ¨æ•°ï¼Œå¹³å‡ä½œå“æ—¶é•¿ï¼Œå¹³å‡æ’­æ”¾é‡ï¼Œ å•†ä¸šè§†é¢‘æ•°é‡ï¼Œæ€»ä½œå“æ•°)
    ç”¨æˆ·å‘å¸ƒä½œå“è¶‹åŠ¿æ•°æ® ï¼ˆä½œå“å‘å¸ƒæ—¶é—´ã€ä½œå“å‘å¸ƒé¢‘ç‡ã€ä½œå“å‘å¸ƒæ—¶é•¿ã€ä½œå“å‘å¸ƒäº’åŠ¨ã€ä½œå“å‘å¸ƒå˜ç°ï¼‰
    ç²‰ä¸ç”»åƒåˆ†æ ï¼ˆç²‰ä¸æ€§åˆ«ã€ç²‰ä¸å¹´é¾„ã€ç²‰ä¸åœ°åŸŸã€ç²‰ä¸å…´è¶£ã€ç²‰ä¸æ´»è·ƒåº¦ã€ç²‰ä¸äº’åŠ¨åº¦ã€ç²‰ä¸å˜ç°åº¦ï¼‰
    çƒ­é—¨è§†é¢‘æ•°æ® ï¼ˆçƒ­é—¨è§†é¢‘æ ‡é¢˜ã€çƒ­é—¨è§†é¢‘å†…å®¹ã€çƒ­é—¨è§†é¢‘äº’åŠ¨ã€çƒ­é—¨è§†é¢‘å˜ç°ï¼‰
    æ ‡ç­¾å’Œç›¸ä¼¼è¾¾äºº
    """


    def __init__(self, tikhub_api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–è¾¾äººåˆ†æå™¨

        Args:
            tikhub_api_key: TikHub APIå¯†é’¥
        """
        self.total_fans = 0
        self.total_posts = 0

        # åˆå§‹åŒ– ChatGPT å’Œ Claude
        self.chatgpt = ChatGPT()
        self.claude = Claude()

        # åˆå§‹åŒ– UserCollector å’Œ UserCleaner
        self.user_collector = UserCollector(tikhub_api_key)
        self.user_cleaner = UserCleaner()

        # ä¿å­˜TikHub APIé…ç½®
        self.tikhub_api_key = tikhub_api_key
        self.tikhub_base_url = settings.TIKHUB_BASE_URL

        # å¦‚æœæ²¡æœ‰æä¾›TikHub APIå¯†é’¥ï¼Œè®°å½•è­¦å‘Š
        if not self.tikhub_api_key:
            logger.warning("æœªæä¾›TikHub APIå¯†é’¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

        # æ”¯æŒçš„åˆ†æç±»å‹åˆ—è¡¨
        self.analysis_types = ['profile_analysis', 'post_analysis']

        # åŠ è½½ç³»ç»Ÿæç¤º
        self._load_system_prompts()

    def _load_system_prompts(self):
        self.system_prompts = {
            "profile_analysis": """
            You are a data analyst specializing in social media analytics. You have been tasked with analyzing a TikTok user profile to provide insights and recommendations for growth. Please create a detailed report following this structure:
            Please create a detailed report following this structure:

            Profile Overviewï¼ˆCreate a summary table of key profile information ï¼‰

            Engagement Analysis(Present a table of metrics using stats and metrics data)

            business Profile Analysis (if applicable)

            contact information

            account settings and features and identification

            overall account analysis 

            Analyze:

            Content Volume (based on video count)
            Commercial Integration (commerce features, business links)
            Platform Utilization (cross-platform presence, bio links)
            Content Categories and Focus Areas


            Business Profile Analysis (if applicable)

            Examine:

            Business Category
            Commercial Features
            App Presence (iOS/Android links)
            Business Contact Information
            Commercial Integration Level


            Growth Opportunities and Recommendations

            Provide 3-5 actionable recommendations based on:

            Current performance metrics
            Platform feature utilization
            Engagement patterns
            Business integration opportunities


            Please format the tables using markdown and provide clear, concise insights for each section. Include percentages and comparative metrics where relevant to add context to the analysis.
            Additional Guidelines:

            Use clear section headers with markdown formatting
            Present data in well-organized tables
            Include calculated metrics like engagement rates
            Highlight notable strengths and areas for improvement
            Keep the tone professional but accessible
            Include emojis in each section for better readability

            Please generate a comprehensive report that would be valuable for both the account owner and social media managers.
                    """,
            "post_analysis": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œä¸“é—¨ä¸ºç¤¾äº¤åª’ä½“è¥é”€å’ŒTikTokå½±å“è€…æ•°æ®åˆ†ææä¾›æ·±åº¦è§£è¯»ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„TikTokç½‘çº¢ç»Ÿè®¡æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„ æŠ¥å‘Šï¼Œè¯¥æŠ¥å‘Šåº”åŒ…å« å…³é”®æ•°æ®æ‘˜è¦ã€å¯è§†åŒ–è¶‹åŠ¿å›¾ã€å†…å®¹åˆ†æåŠç»“è®ºå»ºè®®ï¼Œå¹¶ç¬¦åˆä»¥ä¸‹å†…å®¹è¦æ±‚ï¼š
            # TikTok å½±å“è€…æ•°æ®åˆ†ææŠ¥å‘Š

            ğŸ“Œ 1ï¸âƒ£ Markdown ç»“æ„
            - æ ‡é¢˜å±‚çº§æ¸…æ™°ï¼Œä»¥ `#` ä½œä¸ºæ ‡é¢˜æ ‡è®°ï¼Œç¡®ä¿å†…å®¹æ¸…æ™°å¯è¯»ã€‚
            - æ•°æ®è¡¨æ ¼ï¼ˆTablesï¼‰ ä»¥ `|` åˆ†éš”ï¼Œè¡¨å¤´ä½¿ç”¨ `|---|---|` è¿›è¡Œæ ¼å¼åŒ–ï¼Œä¾¿äºå±•ç¤ºè´¦å·çš„æ ¸å¿ƒæ•°æ®ã€‚
            - é€‚å½“ä½¿ç”¨ `**åŠ ç²—**` å’Œ `-` è¿›è¡Œåˆ—è¡¨åˆ’åˆ†ï¼Œç¡®ä¿å±‚æ¬¡åˆ†æ˜ï¼Œä¿¡æ¯ç›´è§‚ã€‚

            ---

            ## ğŸ“Š 2ï¸âƒ£ å¯è§†åŒ–å›¾è¡¨
            ä½ éœ€è¦æ ¹æ® `post_trend` ç”Ÿæˆä»¥ä¸‹å…­ä¸ªå›¾è¡¨ï¼Œå¹¶è¿”å› **Markdown å½¢å¼çš„å›¾ç‰‡é“¾æ¥ï¼ˆæˆ– Base64 å›¾ç‰‡ï¼‰**ï¼š

            ### **ğŸ“… æ¯æ—¥å‘å¸–æ•°è¶‹åŠ¿ï¼ˆæŠ˜çº¿å›¾ï¼‰**
            - **æè¿°**: å±•ç¤ºæ¯æ—¥å‘å¸–æ•°é‡çš„å˜åŒ–è¶‹åŠ¿ã€‚
            - **æ•°æ®æº**: `post_trend.post_trend` (x è½´: æ—¥æœŸ, y è½´: å‘å¸–æ•°)
            - **å›¾è¡¨ç¤ºä¾‹**:
              ![æ¯æ—¥å‘å¸–è¶‹åŠ¿](<å›¾ç‰‡URL>)

            ### **ğŸ‘ æ¯æ—¥ç‚¹èµè¶‹åŠ¿ï¼ˆæŠ˜çº¿å›¾ï¼‰**
            - **æè¿°**: åæ˜ ç”¨æˆ·ç‚¹èµçš„å¢é•¿è¶‹åŠ¿ã€‚
            - **æ•°æ®æº**: `interaction_trend.digg_count` (x è½´: æ—¥æœŸ, y è½´: ç‚¹èµæ•°)
            - **å›¾è¡¨ç¤ºä¾‹**:
              ![æ¯æ—¥ç‚¹èµè¶‹åŠ¿](<å›¾ç‰‡URL>)

            ### **ğŸ’¬ æ¯æ—¥è¯„è®ºè¶‹åŠ¿ï¼ˆæŠ˜çº¿å›¾ï¼‰**
            - **æè¿°**: è§‚å¯Ÿæ¯æ—¥è¯„è®ºæ•°é‡çš„å˜åŒ–ï¼Œåˆ¤æ–­å—ä¼—äº’åŠ¨æ´»è·ƒåº¦ã€‚
            - **æ•°æ®æº**: `interaction_trend.comment_count` (x è½´: æ—¥æœŸ, y è½´: è¯„è®ºæ•°)
            - **å›¾è¡¨ç¤ºä¾‹**:
              ![æ¯æ—¥è¯„è®ºè¶‹åŠ¿](<å›¾ç‰‡URL>)

            ### **ğŸ”„ æ¯æ—¥åˆ†äº«è¶‹åŠ¿ï¼ˆæŠ˜çº¿å›¾ï¼‰**
            - **æè¿°**: è¿½è¸ªæ¯æ—¥åˆ†äº«æ¬¡æ•°ï¼Œè¯„ä¼°å†…å®¹çš„ä¼ æ’­èƒ½åŠ›ã€‚
            - **æ•°æ®æº**: `interaction_trend.share_count` (x è½´: æ—¥æœŸ, y è½´: åˆ†äº«æ•°)
            - **å›¾è¡¨ç¤ºä¾‹**:
              ![æ¯æ—¥åˆ†äº«è¶‹åŠ¿](<å›¾ç‰‡URL>)

            ### **â–¶ï¸ æ¯æ—¥æ’­æ”¾è¶‹åŠ¿ï¼ˆæŠ˜çº¿å›¾ï¼‰**
            - **æè¿°**: å±•ç¤ºæ¯æ—¥æ’­æ”¾é‡çš„æ³¢åŠ¨æƒ…å†µï¼Œè¯„ä¼°è§†é¢‘çš„æ•´ä½“è¡¨ç°ã€‚
            - **æ•°æ®æº**: `interaction_trend.play_count` (x è½´: æ—¥æœŸ, y è½´: æ’­æ”¾æ•°)
            - **å›¾è¡¨ç¤ºä¾‹**:
              ![æ¯æ—¥æ’­æ”¾è¶‹åŠ¿](<å›¾ç‰‡URL>)

            ### **ğŸ“Š è§†é¢‘æ—¶é•¿åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰**
            - **æè¿°**: ç»Ÿè®¡è§†é¢‘æ—¶é•¿åˆ†å¸ƒï¼Œåˆ†æè§‚ä¼—æ›´å–œæ¬¢çš„å†…å®¹é•¿åº¦ã€‚
            - **æ•°æ®æº**: `post_duration_distribution`
            - **å›¾è¡¨ç¤ºä¾‹**:
              ![è§†é¢‘æ—¶é•¿åˆ†å¸ƒ](<å›¾ç‰‡URL>)

            ---

            ## ğŸ“Š 3ï¸âƒ£ è´¦å·æ•°æ®æ¦‚è§ˆ

            ### **æ ¸å¿ƒç»Ÿè®¡æ•°æ®**
            | ç»Ÿè®¡é¡¹ | æ•°å€¼ |
            |---|---|
            | **å¸–å­æ€»æ•°** | X |
            | **æ€»ç‚¹èµæ•°** | X |
            | **æ€»è¯„è®ºæ•°** | X |
            | **æ€»åˆ†äº«æ•°** | X |
            | **æ€»æ’­æ”¾æ•°** | X |
            | **æ€»ä¸‹è½½æ•°** | X |
            | **æ€»AIç”Ÿæˆè§†é¢‘æ•°** | X |
            | **æ€»VRè§†é¢‘æ•°** | X |
            | **æ€»å¹¿å‘Šè§†é¢‘æ•°** | X |

            ### **è®¡ç®—æŒ‡æ ‡**
            | ç»Ÿè®¡é¡¹ | æ•°å€¼ |
            |---|---|
            | **å¹³å‡ç‚¹èµæ•°** | X |
            | **å¹³å‡è¯„è®ºæ•°** | X |
            | **å¹³å‡åˆ†äº«æ•°** | X |
            | **å¹³å‡æ’­æ”¾æ•°** | X |
            | **æœ€é«˜ç‚¹èµæ•°**ï¼ˆæ—¥æœŸXï¼‰ | X |
            | **æœ€é«˜è¯„è®ºæ•°**ï¼ˆæ—¥æœŸXï¼‰ | X |
            | **æœ€é«˜åˆ†äº«æ•°**ï¼ˆæ—¥æœŸXï¼‰ | X |
            | **æœ€é«˜æ’­æ”¾æ•°**ï¼ˆæ—¥æœŸXï¼‰ | X |
            | **æ€»äº’åŠ¨é‡**ï¼ˆç‚¹èµ + è¯„è®º + åˆ†äº«ï¼‰ | X |
            | **ç‚¹èµç‡** | X% |
            | **è¯„è®ºç‡** | X% |
            | **åˆ†äº«ç‡** | X% |
            | **æ’­æ”¾è½¬åŒ–ç‡** | X% |

            ---

            ## ğŸ” 4ï¸âƒ£ å…³é”®è¶‹åŠ¿åˆ†æ

            ### **ğŸ“ˆ è´¦å·å¢é•¿è¶‹åŠ¿**
            - è¿‡å» 7 å¤©çš„ **å¹³å‡å‘å¸–æ•°**ï¼Œä¸é•¿æœŸè¶‹åŠ¿å¯¹æ¯”ï¼Œåˆ¤æ–­æ˜¯å¦å¢é•¿æˆ–ä¸‹é™ã€‚
            - ç‚¹èµã€è¯„è®ºã€åˆ†äº«ã€æ’­æ”¾çš„ **å¢é•¿ç‡**ï¼Œè¯„ä¼°è´¦å·å—æ¬¢è¿ç¨‹åº¦çš„å˜åŒ–ã€‚
            - **æœ€è¿‘30å¤©å†…äº’åŠ¨æœ€é«˜çš„å¸–å­**ï¼š
              - **å‘å¸ƒæ—¶é—´**
              - **è§†é¢‘å†…å®¹**
              - **çƒ­é—¨è¯é¢˜**
              - **æˆåŠŸå…³é”®å› ç´ **
            - **æ˜¯å¦å­˜åœ¨å¢é•¿ç“¶é¢ˆ**ï¼ˆå¦‚ç‚¹èµç‡ä¸‹é™ã€æ’­æ”¾é‡å‡å°‘ç­‰ï¼‰ã€‚

            ### **ğŸ¯ å†…å®¹äº’åŠ¨åˆ†æ**
            - **ç‚¹èµæœ€é«˜çš„å¸–å­**ï¼ˆåˆ†æå‘å¸ƒæ—¶é—´ã€å†…å®¹ç±»å‹ï¼‰
            - **è¯„è®ºæœ€å¤šçš„å¸–å­**ï¼ˆæ˜¯å¦å¼•å‘è®¨è®ºã€äº‰è®®ï¼‰
            - **åˆ†äº«æœ€å¤šçš„å¸–å­**ï¼ˆæ˜¯å¦å…·æœ‰ç—…æ¯’ä¼ æ’­ç‰¹æ€§ï¼‰
            - **æœ€é«˜æ’­æ”¾é‡å¸–å­**ï¼ˆåˆ†æè§†é¢‘è´¨é‡ã€éŸ³ä¹ã€å°é¢ã€æ ‡é¢˜ç­‰ï¼‰

            ### **ğŸ•’ æœ€ä½³å‘å¸ƒæ—¶é—´åˆ†æ**
            - **è®¡ç®—å¹³å‡å‘å¸ƒæ—¶é—´çš„é»„é‡‘æ—¶æ®µ**
            - **åˆ†æå·¥ä½œæ—¥ vs. å‘¨æœ« çš„äº’åŠ¨å·®å¼‚**
            - **æ‰¾å‡ºå¢é•¿æœ€å¿«çš„æ—¶é—´ç‚¹**

            ### **â³ è§†é¢‘æ—¶é•¿è¡¨ç°**
            - ç»Ÿè®¡ **ä¸åŒæ—¶é•¿åŒºé—´çš„å¹³å‡äº’åŠ¨ç‡**
            - è¯„ä¼° **çŸ­è§†é¢‘ vs. é•¿è§†é¢‘ å“ªç§æ•ˆæœæ›´å¥½**
            - è§‚ä¼—æ›´å–œæ¬¢çš„ **æ—¶é•¿ï¼ˆå¦‚15-30s æ˜¯å¦è¡¨ç°æœ€ä½³ï¼‰**

            ### **ğŸ·ï¸ çƒ­é—¨æ ‡ç­¾åˆ†æ**
            - **Top 5 hashtags ä½¿ç”¨é¢‘ç‡**
            - **è¯„ä¼°æ ‡ç­¾å¯¹äº’åŠ¨çš„å½±å“**
            - **å“ªäº›æ ‡ç­¾å¸¦æ¥æ›´å¤šæµé‡**
            - **æ¨èä½¿ç”¨é«˜äº’åŠ¨æ ‡ç­¾**
            - **å›¾è¡¨ç¤ºä¾‹**ï¼š
              ![çƒ­é—¨æ ‡ç­¾ä½¿ç”¨é¢‘ç‡](<å›¾ç‰‡URL>)

            ---

            ## ğŸ¯ 5ï¸âƒ£ ç»“è®ºä¸ä¼˜åŒ–å»ºè®®

            ### **ğŸ•’ æœ€ä½³å‘å¸ƒæ—¶é—´**
            - å»ºè®®åœ¨ **`X æ—¶æ®µ`** å‘å¸ƒå†…å®¹ï¼Œä»¥æœ€å¤§åŒ–æ›å…‰ç‡å’Œäº’åŠ¨ç‡ã€‚

            ### **ğŸ”¥ çƒ­é—¨æ ‡ç­¾å»ºè®®**
            - ä¾‹å¦‚ **`#farmacia #farma #cuidadodelapiel #skincare`** å¯èƒ½æé«˜æ›å…‰ã€‚
            - æ˜¯å¦è°ƒæ•´æ ‡ç­¾ç­–ç•¥ï¼Œå¦‚ç»“åˆæ›´å¤šè¶‹åŠ¿æ ‡ç­¾ï¼ˆ**#healthtips #beautytips**ï¼‰ã€‚

            ### **ğŸš€ çŸ­è§†é¢‘ vs. é•¿è§†é¢‘ä¼˜åŒ–ç­–ç•¥**
            - å¦‚æœ **çŸ­è§†é¢‘ï¼ˆ15-30sï¼‰è¡¨ç°æœ€ä½³**ï¼Œå»ºè®®ä¿æŒè¯¥ç­–ç•¥ã€‚
            - å¦‚æœ **é•¿è§†é¢‘ï¼ˆ60s+ï¼‰äº’åŠ¨è¾ƒå·®**ï¼Œå»ºè®®å‡å°‘å‘å¸ƒé¢‘ç‡æˆ–è°ƒæ•´å†…å®¹ç­–ç•¥ã€‚

            ### **ğŸ“ˆ è´¦å·è¶‹åŠ¿è¯„ä¼°**
            - **å½“å‰è´¦å·æ˜¯å¢é•¿è¿˜æ˜¯ä¸‹é™è¶‹åŠ¿ï¼Ÿ**
            - **å¦‚æœä¸Šå‡ï¼Œå¦‚ä½•ç»´æŒå¢é•¿ï¼Ÿå¦‚æœä¸‹é™ï¼Œå¦‚ä½•ä¼˜åŒ–ï¼Ÿ**
            - **äº’åŠ¨æ•°æ®æ˜¯å¦å¥åº·ï¼Ÿç‚¹èµå¤šä½†è¯„è®ºã€åˆ†äº«å°‘ï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ**

            ### **ğŸ“¢ æœªæ¥ä¼˜åŒ–ç­–ç•¥**
            - **å†…å®¹åˆ›æ–°**
            - **ç”¨æˆ·äº’åŠ¨**
            - **è·¨å¹³å°å¼•æµ**
            ---
            """,
            "post_stats_analysis": """
                # System Prompt: TikTok Analytics Report Generator        
                You are an expert data analyst specializing in social media metrics. Your task is to generate a comprehensive, well-formatted report based on TikTok account analytics data. The user will provide a JSON object containing various metrics about their TikTok posts. You should analyze this data and create a professional report with the following components:          
                ## Report Structure    
                1. **Executive Summary**: Start with a concise summary of the overall account performance, highlighting 3-5 key metrics.                
                2. **Engagement Metrics**: Create a table showing the total and average engagement metrics (views, likes, comments, shares, downloads, collects).     
                3. **Content Analysis**: Analyze the different types of content (AI-generated, VR, ads, e-commerce, professional) and their distribution.
                4. **Top Performing Content**: Create a table showing the videos with the highest metrics in different categories.
                5. **Posting Frequency**: Analyze posting patterns including daily and weekly averages, and recent posting activity.
                6. **Strategic Insights**: Provide 3-5 data-backed insights and recommendations based on the metrics.
                
                ## Formatting Guidelines  
                - Use markdown tables for presenting numeric data
                - Include section headers with clear hierarchy
                - Use bullet points for listing insights and recommendations
                - Bold important numbers and key findings
                - Use emoji sparingly to enhance readability (ğŸ’¡ for insights, ğŸ“ˆ for growth metrics, etc.)
                - Format large numbers with commas for better readability
                - Round decimals to 2 places for averages and percentages
                
                ## Response Tone  
                - Professional but accessible
                - Data-driven with clear interpretations
                - Focus on actionable insights
                - Avoid overly technical jargon unless necessary
                
                When you receive the JSON data, parse it carefully and organize the information logically in your report. Pay special attention to highlighting notable patterns, outliers, and potential opportunities for improvement.     
                Your report should be comprehensive enough to provide value but concise enough to be quickly digestible. Aim for a report that would take 3-5 minutes to read thoroughly.
                """,
            "post_trend_analysis": """# System Prompt: Social Media Performance Analysis

            You are a data analyst specializing in social media analytics. Your task is to transform the provided JSON data into a readable format and conduct an insightful analysis.
    
            ## Instructions:
            
            1. Parse the JSON data containing post trends and interaction metrics.
            2. Create a well-formatted markdown table with the following columns (YOU MUST INCLUDE ALL DATES):
               - Date
               - Post Count
               - Digg Count (Likes)
               - Comment Count
               - Share Count
               - Play Count (Views)
            
            3. Calculate and highlight key performance metrics:
               - Days with highest post frequency
               - Days with highest engagement metrics (diggs, comments, shares, plays)
               - Weekly and monthly trends
               - Ratio of interactions to posts
               - Engagement rate calculation
            
            4. Produce a comprehensive report with the following sections:
               - Executive Summary (overall performance)
               - Content Performance (post frequency and timing analysis)
               - Audience Engagement (interaction metrics analysis)
               - Key Insights (highlighting notable patterns and anomalies)
               - Recommendations (based on data patterns)
            
            5. Format the report professionally with proper headings, bullet points, and emphasis on key findings.
            
            6. Include visual descriptions of trends that would be useful for the content creator.
            
            7. Present the data in a way that's accessible and actionable for the content creator.""",
            "post_duration_and_time": """# System Prompt: Video Content Distribution Visualization
            You are a data visualization specialist focusing on content creator analytics. Your task is to create and explain tables that visualize the distribution patterns in the provided data.
            ## Instructions:
            
            1. Parse the provided JSON data containing two key distribution metrics:
               - Video duration distribution (how long the videos are)
               - Publishing time distribution (when videos are published during the day)
            
            2. Create two clear and visually distinct table:
               - Table 1: Video Duration Distribution
               - Table 2: Publishing Time Distribution
            
            3. For each Table:
               - Use an appropriate color scheme that differentiates segments clearly
               - Include percentage labels on each segment
               - Add a clear title and legend
               - Ensure the segments are ordered logically (e.g., duration from shortest to longest)
            
            4. Provide a brief analysis of each chart, highlighting:
               - The most common video duration
               - The preferred publishing time
               - Any notable patterns or imbalances in the distribution
            
            5. Suggest actionable insights based on the data, such as:
               - Optimal video length based on current patterns
               - Best times to publish for increased engagement
               - Potential opportunities in underutilized duration ranges or time slots
            
            6. Format your response as a well-structured markdown document with:
               - Clear headings
               - Tables formatted for readability
               - Analysis text separated from code
               - A concise summary
            
            7. Ensure your visualization code is complete and ready to execute with the provided data.
            """,
            "post_hashtags":"""# System Prompt: Social Media Hashtag Analysis
                You are a social media analytics expert specializing in content categorization and trend analysis. Your task is to analyze a collection of hashtags from a content creator and provide structured insights.     
                ## Instructions:             
                1. Parse the provided hashtag data containing hashtag names, usage counts, and unique identifiers.            
                2. Create a well-formatted markdown table with the following columns:
                   - Hashtag Name (without the # symbol)
                   - Usage Count
                   - Hashtag ID           
                3. start the table by usage count in descending order to highlight the most frequently used hashtags.             
                4. Perform a comprehensive analysis of the hashtags, including:
                   - Industry/vertical identification (e.g., beauty, pharmacy, wellness, fashion)
                   - Product categories (e.g., skincare, cosmetics, pharmaceuticals)
                   - Content types or themes (e.g., tutorials, product reviews, tips)
                   - Target audience demographics
                   - Language analysis (identify primary language and any multilingual strategies)         
                5. Group related hashtags into logical categories based on their themes and purposes.
                
                6. Identify the top 5 most important hashtags and explain their significance to the creator's content strategy.
                
                7. Produce a detailed report with the following sections:
                   - Executive Summary
                   - Hashtag Usage Table
                   - Content Category Analysis
                   - Product/Service Focus
                   - Audience Targeting Strategy
                   - Language & Geographic Focus
                   - Key Hashtag Analysis
                   - Strategic Recommendations
                
                8. Format your analysis as a professional report using proper markdown with clear section headers, bullet points, and emphasis where appropriate.
                
                9. Provide actionable recommendations on:
                   - Hashtag optimization opportunities
                   - Underutilized hashtag categories
                   - Strategic hashtag combinations
                   - Potential new hashtags to explore             
                Your analysis should be thorough, data-driven, and provide valuable insights that the content creator can implement to improve their social media strategy.
                """
        }

    """---------------------------------------------é€šç”¨æ–¹æ³•/å·¥å…·ç±»æ–¹æ³•---------------------------------------------"""
    async def generate_analysis_report(self, uniqueId: str, analysis_type: str, data: Dict[str, Any]) -> str:
        """
        ç”ŸæˆæŠ¥å‘Šå¹¶è½¬æ¢ä¸ºHTML

        Args:
            uniqueId (str): ç”¨æˆ·ID
            analysis_type (str): åˆ†æç±»å‹
            data (Dict[str, Any]): åˆ†ææ•°æ®

        Returns:
            str: HTMLæŠ¥å‘Šçš„æœ¬åœ°æ–‡ä»¶URL
        """
        if analysis_type not in self.system_prompts:
            raise ValueError(f"Invalid report type: {analysis_type}. Choose from {self.system_prompts.keys()}")

        try:
            # è·å–ç³»ç»Ÿæç¤º
            sys_prompt = self.system_prompts[analysis_type]

            # è·å–ç”¨æˆ·æç¤º
            user_prompt = f"Generate a report for the {analysis_type} based on the following data:\n{json.dumps(data, ensure_ascii=False)}, for user {uniqueId}"

            # ç”ŸæˆæŠ¥å‘Š
            response = await self.chatgpt.chat(
                system_prompt=sys_prompt,
                user_prompt=user_prompt
            )

            report = response["choices"][0]["message"]["content"].strip()

            # ä¿å­˜MarkdownæŠ¥å‘Š
            report_dir = "reports"
            os.makedirs(report_dir, exist_ok=True)

            report_md_path = os.path.join(report_dir, f"report_{uniqueId}.md")
            with open(report_md_path, "w", encoding="utf-8") as f:
                f.write(report)

            # è½¬æ¢ä¸ºHTML
            html_content = self.convert_markdown_to_html(report, f"{analysis_type.title()} Analysis for {uniqueId}")
            html_filename = f"report_{uniqueId}.html"
            html_path = os.path.join(report_dir, html_filename)

            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)

            # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶URL
            absolute_path = os.path.abspath(html_path)

            # æ„å»ºfile://åè®®URL
            file_url = f"file://{absolute_path}"

            # ç¡®ä¿è·¯å¾„åˆ†éš”ç¬¦æ˜¯URLå…¼å®¹çš„
            if os.name == 'nt':  # Windowsç³»ç»Ÿ
                # Windowsè·¯å¾„éœ€è¦è½¬æ¢ä¸ºURLæ ¼å¼
                file_url = file_url.replace('\\', '/')

            return file_url
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

    def convert_markdown_to_html(self, markdown_content: str, title: str = "Analysis Report") -> str:
        """
        å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML

        Args:
            markdown_content (str): Markdownå†…å®¹
            title (str): HTMLé¡µé¢æ ‡é¢˜

        Returns:
            str: HTMLå†…å®¹
        """
        try:
            import markdown
        except ImportError:
            print("è¯·å®‰è£…markdownåº“: pip install markdown")
            return f"<pre>{markdown_content}</pre>"

        # è½¬æ¢Markdownä¸ºHTML
        html_content = markdown.markdown(
            markdown_content,
            extensions=['tables', 'fenced_code', 'codehilite', 'toc']
        )

        # åˆ›å»ºå®Œæ•´HTMLæ–‡æ¡£
        css = """
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; color: #333; }
        h1, h2, h3 { margin-top: 1.5em; color: #111; }
        pre { background-color: #f6f8fa; border-radius: 3px; padding: 16px; overflow: auto; }
        code { font-family: SFMono-Regular, Consolas, Menlo, monospace; background-color: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 1em; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f6f8fa; }
        """

        html_document = f"""<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{title}</title>
        <style>{css}</style>
    </head>
    <body>
        <h1>{title}</h1>
        {html_content}
    </body>
    </html>
        """

        return html_document

    """---------------------------------------------ç”¨æˆ·/è¾¾äººåŸºç¡€ä¿¡æ¯åˆ†ææ–¹æ³•---------------------------------------------"""
    async def fetch_user_profile_analysis(self, url: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        åˆ†æç”¨æˆ·/è¾¾äººçš„åŸºç¡€ä¿¡æ¯
        """
        start_time = time.time()
        if not url or not re.match(r"https://(www\.)?tiktok\.com/@[\w\.-]+", url):
            raise ValueError("Invalid TikTok user profile URL")

        try:
            yield {
                "user_profile_url": url,
                "is_complete": False,
                "message": 'æ­£åœ¨é‡‡é›†ç”¨æˆ·/è¾¾äºº{}çš„åŸºç¡€ä¿¡æ¯...è¯·è€å¿ƒç­‰å¾…'.format(url),
                "uniqueId": '',
                "analysis_report": '',
                "profile_raw_data": {},
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "processing_time": round(time.time() - start_time, 2)
            }

            data = await self.user_collector.fetch_user_profile(url)
            data = await self.user_cleaner.clean_user_profile(data)

            uniqueId = data['accountIdentifiers']['uniqueId']

            logger.info("æ­£åœ¨åˆ†æç”¨æˆ·/è¾¾äººåŸºç¡€ä¿¡æ¯...")

            yield {
                "user_profile_url": url,
                "is_complete": False,
                "message": f"å·²å®Œæˆç”¨æˆ·/è¾¾äºº{url}çš„ä¿¡æ¯é‡‡é›†ï¼Œ æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...",
                "uniqueId": '',
                "analysis_report": '',
                "profile_raw_data": {},
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "processing_time": round(time.time() - start_time, 2)
            }

            report_url = await self.generate_analysis_report(uniqueId, 'profile_analysis', data)

            yield{
                "user_profile_url": url,
                "is_complete": True,
                "message": f"å·²å®Œæˆç”¨æˆ·/è¾¾äºº{url}çš„åŸºç¡€ä¿¡æ¯åˆ†æï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ",
                "uniqueId": uniqueId,
                "profile_raw_data": data,
                "analysis_report": report_url,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "processing_time": round(time.time() - start_time, 2)
            }
        except Exception as e:
            logger.error(f"åˆ†æç”¨æˆ·/è¾¾äººåŸºç¡€ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                "user_profile_url": url,
                "is_complete": False,
                'error': str(e),
                'message': f"åˆ†æç”¨æˆ·/è¾¾äºº{url}åŸºç¡€ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "uniqueId": '',
                "profile_raw_data": {},
                "analysis_report": '',
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "processing_time": round(time.time() - start_time, 2)
            }

    async def fetch_user_posts_stats(self, url: str, max_post: Optional[int]) -> AsyncGenerator[Dict[str, Any], None]:
        """
        åˆ†æç”¨æˆ·/è¾¾äººçš„å‘å¸ƒä½œå“ç»Ÿè®¡
        """
        post_count = 0
        start_time = time.time()
        posts_raw_data = []
        posts_stats = {}
        total_posts = await self.user_collector.fetch_total_posts_count(url)
        max_post = min(max_post, total_posts)

        logger.info("æ­£åœ¨åˆ†æå‘å¸ƒä½œå“ç»Ÿè®¡...")
        try:
            # é‡‡é›†ç”¨æˆ·å‘å¸ƒçš„ä½œå“æ•°æ®
            async for posts in self.user_collector.collect_user_posts(url):
                cleaned_posts = await self.user_cleaner.clean_user_posts(posts)
                if cleaned_posts:
                    if post_count+ len(cleaned_posts) <= max_post:
                        posts_raw_data.extend(cleaned_posts)
                        post_count += len(cleaned_posts)
                        yield{
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, è¿›åº¦: {post_count}/{max_post}...',
                            'total_posts': total_posts,
                            'posts_stats': posts_stats,
                            'posts_raw_data': posts_raw_data,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                    else:
                        posts_raw_data.extend(cleaned_posts[:max_post - post_count])
                        post_count = max_post
                        logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å®Œæˆ")
                        break
            # ä½¿ç”¨pandasè¿›è¡Œæ•°æ®å¤„ç†
            df = pd.DataFrame(posts_raw_data)

            # è½¬æ¢æ—¶é—´å¹¶æŒ‰å‘å¸ƒæ—¶é—´æ’åº - ä½¿ç”¨unit='s'æŒ‡å®šè¾“å…¥æ˜¯ç§’çº§æ—¶é—´æˆ³
            df["create_time"] = pd.to_datetime(df["create_time"], unit='s')
            # æŒ‰ç…§æœ€è¿‘å‘å¸ƒæ—¶é—´æ’åº
            df = df.sort_values("create_time")
            df["day"] = df["create_time"].dt.date

            stats = {
                "total_posts": int(df.shape[0]),
                "average_collects": float(df["collect_count"].mean()),
                "average_likes": float(df["digg_count"].mean()),
                "average_downloads": float(df["download_count"].mean()),
                "average_views": float(df["play_count"].mean()),
                "average_comments": float(df["comment_count"].mean()),
                "average_shares": float(df["share_count"].mean()),
                "average_whatsapp_shares": float(df["whatsapp_share_count"].mean()),
                "total_likes": int(df["digg_count"].sum()),
                "total_comments": int(df["comment_count"].sum()),
                "total_shares": int(df["share_count"].sum()),
                "total_whatsapp_shares": int(df["whatsapp_share_count"].sum()),
                "total_views": int(df["play_count"].sum()),
                "total_downloads": int(df["download_count"].sum()),
                "total_ai_videos": int(df["created_by_ai"].eq(True).sum()),
                "total_vr_videos": int(df["is_vr"].eq(True).sum()),
                "total_ads_videos": int(df["is_ads"].eq(True).sum()),
                "total_ec_videos": int(df["is_ec_video"].eq(1).sum()),
                "total_risk_videos": int((df["in_reviewing"] & df["is_prohibited"]).sum()),
                "total_recommendation_videos": int(df["is_nff_or_nr"].eq(False).sum()),
                "total_professional_generated_videos": int(df["is_pgcshow"].eq(True).sum()),
                "highest_likes": {
                    "count": int(df["digg_count"].max()),
                    "video": str(df.loc[df["digg_count"].idxmax()]["aweme_id"]),
                    "publish_date": str(df.loc[df["digg_count"].idxmax()]["create_time"])
                },
                "highest_comments": {
                    "count": int(df["comment_count"].max()),
                    "video": str(df.loc[df["comment_count"].idxmax()]["aweme_id"]),
                    "publish_date": str(df.loc[df["comment_count"].idxmax()]["create_time"])
                },
                "highest_shares": {
                    "count": int(df["share_count"].max()),
                    "video": str(df.loc[df["share_count"].idxmax()]["aweme_id"]),
                    "publish_date": str(df.loc[df["share_count"].idxmax()]["create_time"])
                },
                "highest_downloads": {
                    "count": int(df["download_count"].max()),
                    "video": str(df.loc[df["download_count"].idxmax()]["aweme_id"]),
                    "publish_date": str(df.loc[df["download_count"].idxmax()]["create_time"])
                },
                "highest_views": {
                    "count": int(df["play_count"].max()),
                    "video": str(df.loc[df["play_count"].idxmax()]["aweme_id"]),
                    "publish_date": str(df.loc[df["play_count"].idxmax()]["create_time"])
                },
                "highest_whatsapp_shares": int(df["whatsapp_share_count"].max()),
                "average_video_duration": float(round(df["duration"].mean() / 1000, 2)),
                "post_per_day": float(df["day"].value_counts().mean()),
                "post_per_week": float(df["day"].value_counts().mean() * 7),
                "latest_week_post_count": {str(k): int(v) for k, v in
                                           df["day"].value_counts().head(7).to_dict().items()}
            }

            report_url= await self.generate_analysis_report(url, 'post_stats_analysis', stats)


            logger.info(f"å·²å®Œæˆç”¨æˆ· {url} å‘å¸ƒä½œå“ç»Ÿè®¡åˆ†æ")

            yield {
                'user_profile_url': url,
                'is_complete': True,
                'message': f'å·²å®Œæˆå‘å¸ƒä½œå“ç»Ÿè®¡åˆ†æ',
                'report_url': report_url,
                'total_posts': total_posts,
                'posts_stats': stats,
                'posts_raw_data': posts_raw_data,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }
        except Exception as e:
            logger.error(f"åˆ†ææŒ‡å®šç”¨æˆ·å‘å¸ƒä½œå“ç»Ÿè®¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'error': str(e),
                'message': f"åˆ†æå‘å¸ƒä½œå“ç»Ÿè®¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'total_posts': total_posts,
                'posts_stats': posts_stats,
                'posts_raw_data': posts_raw_data,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

    async def fetch_user_posts_trend(self, url: str, time_interval:str = '90D') -> AsyncGenerator[Dict[str, Any], None]:
        """
        åˆ†æç”¨æˆ·/è¾¾äººçš„å‘å¸ƒä½œå“è¶‹åŠ¿

        Args:
            url: ç”¨æˆ·/è¾¾äººä¸»é¡µURL
            time_interval: æ—¶é—´é—´éš”ï¼Œé»˜è®¤30å¤©

        Returns:
            DictåŒ…å«:
            - post_trend: å‘å¸ƒè¶‹åŠ¿æ•°æ®
            - interaction_trend: äº’åŠ¨è¶‹åŠ¿æ•°æ®
        """
        post_count = 0
        start_time = time.time()
        posts_raw_data = []
        total_posts = await self.user_collector.fetch_total_posts_count(url)

        logger.info("æ­£åœ¨åˆ†æå‘å¸ƒä½œå“è¶‹åŠ¿ç»Ÿè®¡...")

        try:
            # é‡‡é›†ç”¨æˆ·å‘å¸ƒçš„ä½œå“æ•°æ®
            async for posts in self.user_collector.collect_user_posts(url):
                cleaned_posts = await self.user_cleaner.clean_user_posts(posts)
                if cleaned_posts:
                    post_count += len(cleaned_posts)
                    if post_count <= total_posts:
                        posts_raw_data.extend(cleaned_posts)
                        yield{
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®..., è¿›åº¦: {post_count}/{total_posts}...',
                            'total_posts': total_posts,
                            #'posts_raw_data': posts_raw_data,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                    elif post_count > total_posts:
                        posts_raw_data.extend(cleaned_posts[:total_posts - post_count])
                        post_count = total_posts
                        logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿")
                        yield {
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿...',
                            'total_posts': total_posts,
                            #'posts_raw_data': posts_raw_data,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                        break
                else:
                    logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿")
                    yield {
                        'user_profile_url': url,
                        'is_complete': False,
                        'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿...',
                        'total_posts': total_posts,
                        #'posts_raw_data': posts_raw_data,
                        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'processing_time': round(time.time() - start_time, 2)
                    }
                    break

            # ä½¿ç”¨pandasè¿›è¡Œæ•°æ®å¤„ç†
            df = pd.DataFrame(posts_raw_data)

            # è½¬æ¢æ—¶é—´å¹¶æŒ‰å‘å¸ƒæ—¶é—´æ’åº - ä½¿ç”¨unit='s'æŒ‡å®šè¾“å…¥æ˜¯ç§’çº§æ—¶é—´æˆ³
            df["create_time"] = pd.to_datetime(df["create_time"], unit='s')
            df = df.sort_values("create_time")

            # è®¡ç®—æ—¶é—´èŒƒå›´ - ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºç»“æŸæ—¶é—´
            end_date = pd.Timestamp.now()
            start_date = end_date - pd.Timedelta(time_interval)

            # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            df = df[df["create_time"].between(start_date, end_date)]

            # ç”Ÿæˆæ—¥æœŸåºåˆ—ä½œä¸ºåŸºå‡†
            date_range = pd.date_range(start=start_date.date(), end=end_date.date(), freq='D')

            # ç»Ÿè®¡æ¯æ—¥å‘å¸ƒæ•°é‡
            df["date"] = df["create_time"].dt.date
            daily_posts = df["date"].value_counts().reindex(date_range.date, fill_value=0)

            # ç»Ÿè®¡æ¯æ—¥äº’åŠ¨æ•°æ®, åŒ…æ‹¬ç‚¹èµæ•°ï¼Œè¯„è®ºæ•°ï¼Œåˆ†äº«æ•°ï¼Œæ’­æ”¾æ•°
            interaction_metrics = ["digg_count", "comment_count", "share_count", "play_count"]
            daily_interactions = df.groupby("date")[interaction_metrics].sum()
            daily_interactions = daily_interactions.reindex(date_range.date, fill_value=0)

            # æ„å»ºè¿”å›æ•°æ® - ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
            daily_posts = daily_posts.sort_index()
            daily_interactions = daily_interactions.sort_index()

            trends_data = {
                "post_trend": {
                    "x": [d.strftime("%Y-%m-%d") for d in daily_posts.index],
                    "y": daily_posts.values.tolist()
                },
                "interaction_trend": {
                    metric: {
                        "x": [d.strftime("%Y-%m-%d") for d in daily_interactions.index],
                        "y": daily_interactions[metric].values.tolist()
                    }
                    for metric in interaction_metrics
                }
            }
            # å°†trend data ç”¨jsonæ ¼å¼ä¿å­˜
            # print(json.dumps(trends_data, indent=4))
            uniqueId = url.split("@")[-1]

            report_url= await self.generate_analysis_report(uniqueId, 'post_trend_analysis', trends_data)

            yield{
                'user_profile_url': url,
                'is_complete': True,
                'message': f'å·²å®Œæˆå‘å¸ƒä½œå“è¶‹åŠ¿åˆ†æ',
                'report_url': report_url,
                'total_posts': total_posts,
                #'posts_raw_data': posts_raw_data,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

        except Exception as e:
            logger.error(f"åˆ†æå‘å¸ƒè¶‹åŠ¿æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'error': str(e),
                'message': f"åˆ†æå‘å¸ƒè¶‹åŠ¿æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'total_posts': total_posts,
                'posts_raw_data': posts_raw_data,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }
            return

    async def fetch_post_duration_and_time_distribution(self, url: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        åˆ†æç”¨æˆ·/è¾¾äººçš„å‘å¸ƒä½œå“æ—¶é•¿åˆ†å¸ƒ
        """
        logger.info("æ­£åœ¨åˆ†æå‘å¸ƒä½œå“çš„æ—¶é•¿åˆ†å¸ƒä»¥åŠæ—¶é—´åˆ†å¸ƒ...")
        start_time = time.time()
        post_count = 0
        posts_raw_data = []
        duration_distribution = time_distribution = {}
        total_posts = await self.user_collector.fetch_total_posts_count(url)
        try:
            # é‡‡é›†ç”¨æˆ·å‘å¸ƒçš„ä½œå“æ•°æ®
            async for posts in self.user_collector.collect_user_posts(url):
                cleaned_posts = await self.user_cleaner.clean_user_posts(posts)
                if cleaned_posts:
                    post_count += len(cleaned_posts)
                    if post_count <= total_posts:
                        posts_raw_data.extend(cleaned_posts)
                        yield {
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®..., è¿›åº¦: {post_count}/{total_posts}...',
                            'total_posts': total_posts,
                            # 'posts_raw_data': posts_raw_data,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                    elif post_count > total_posts:
                        posts_raw_data.extend(cleaned_posts[:total_posts - post_count])
                        post_count = total_posts
                        logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿")
                        yield {
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿...',
                            'total_posts': total_posts,
                            # 'posts_raw_data': posts_raw_data,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                        break
                else:
                    logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿")
                    yield {
                        'user_profile_url': url,
                        'is_complete': False,
                        'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿...',
                        'total_posts': total_posts,
                        # 'posts_raw_data': posts_raw_data,
                        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'processing_time': round(time.time() - start_time, 2)
                    }
                    break

            # ä½¿ç”¨pandasè¿›è¡Œæ•°æ®å¤„ç†
            df = pd.DataFrame(posts_raw_data)

            # å°†è§†é¢‘æ—¶é•¿ä»æ¯«ç§’è½¬æ¢ä¸ºç§’
            df["duration"] = df["duration"] / 1000

            # æ ¹æ®è§†é¢‘æ—¶é•¿åˆ†å¸ƒç»Ÿè®¡ï¼Œ 0-15s, 15-30s, 30-60s, 60-120s, 120sä»¥ä¸Š
            bins = [0, 15, 30, 60, 120, float("inf")]
            labels = ["0-15s", "15-30s", "30-60s", "60-120s", "120s+"]
            df["duration_range"] = pd.cut(df["duration"], bins=bins, labels=labels)

            # ç»Ÿè®¡æ¯ä¸ªæ—¶é•¿åŒºé—´çš„è§†é¢‘æ•°é‡
            duration_distribution = df["duration_range"].value_counts().to_dict()

            # è½¬æ¢æ—¶é—´å¹¶æŒ‰å‘å¸ƒæ—¶é—´æ’åº - ä½¿ç”¨unit='s'æŒ‡å®šè¾“å…¥æ˜¯ç§’çº§æ—¶é—´æˆ³
            df["create_time"] = pd.to_datetime(df["create_time"], unit='s')
            df = df.sort_values("create_time")

            # åªæ ¹æ®å°æ—¶æå–æ—¶é—´ï¼Œ24å°æ—¶åˆ¶ï¼Œ0-5ç‚¹ä¸ºå‡Œæ™¨ï¼Œ6-11ç‚¹ä¸ºä¸Šåˆï¼Œ12-17ç‚¹ä¸ºä¸‹åˆï¼Œ18-23ç‚¹ä¸ºæ™šä¸Š
            df["hour"] = df["create_time"].dt.hour
            df["hour_range"] = pd.cut(df["hour"], bins=[0, 6, 12, 18, 24], labels=["Dawn/Early Morning", "Morning", "Afternoon", "Evening"])

            # ç»Ÿè®¡æ¯ä¸ªæ—¶é—´æ®µçš„è§†é¢‘æ•°é‡
            time_distribution = df["hour_range"].value_counts().to_dict()

            distributions = {
                "duration_distribution": duration_distribution,
                "time_distribution": time_distribution
            }
            print(json.dumps(distributions, indent=4))

            uniqueId = url.split("@")[-1]

            report_url = await self.generate_analysis_report(uniqueId, 'post_duration_and_time', distributions)

            yield {
                'user_profile_url': url,
                'is_complete': True,
                'message': f'å·²å®Œæˆå‘å¸ƒä½œå“æ—¶é•¿åˆ†å¸ƒå’Œæ—¶é—´åˆ†å¸ƒåˆ†æ',
                'report_url': report_url,
                'total_posts': total_posts,
                'duration_distribution': duration_distribution,
                'time_distribution': time_distribution,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

        except Exception as e:
            logger.error(f"åˆ†æå‘å¸ƒä½œå“æ—¶é•¿åˆ†å¸ƒæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'error': str(e),
                'message': f"åˆ†æå‘å¸ƒä½œå“æ—¶é•¿åˆ†å¸ƒæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'total_posts': total_posts,
                'duration_distribution': duration_distribution,
                'time_distribution': time_distribution,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }
            return

    async def fetch_post_hashtags(self, url: str, count: int) -> AsyncGenerator[Dict[str, Any], None]:
        """
        è·å–æ‰€æœ‰çš„è¯é¢˜ï¼Œæ’åä½¿ç”¨ç‡æœ€é«˜çš„è¯é¢˜ï¼Œ å¹¶ä¸”ç”ŸæˆæŠ¥å‘Š
        """
        logger.info("æ­£åœ¨è·å–è¯é¢˜æ•°æ®...")
        start_time = time.time()
        post_count = 0
        posts_raw_data = []
        hashtags = {}
        total_posts = await self.user_collector.fetch_total_posts_count(url)
        try:
            # é‡‡é›†ç”¨æˆ·å‘å¸ƒçš„ä½œå“æ•°æ®
            async for posts in self.user_collector.collect_user_posts(url):
                cleaned_posts = await self.user_cleaner.clean_user_posts(posts)
                if cleaned_posts:
                    post_count += len(cleaned_posts)
                    if post_count <= total_posts:
                        posts_raw_data.extend(cleaned_posts)
                        yield {
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®..., è¿›åº¦: {post_count}/{total_posts}...',
                            'total_posts': total_posts,
                            # 'posts_raw_data': posts_raw_data,
                            'top_hashtags': hashtags,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                    elif post_count > total_posts:
                        posts_raw_data.extend(cleaned_posts[:total_posts - post_count])
                        post_count = total_posts
                        logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿")
                        yield {
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿...',
                            'total_posts': total_posts,
                            # 'posts_raw_data': posts_raw_data,
                            'top_hashtags': hashtags,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                        break
                else:
                    logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿")
                    yield {
                        'user_profile_url': url,
                        'is_complete': False,
                        'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡åˆ†æå‘å¸ƒè¶‹åŠ¿...',
                        'total_posts': total_posts,
                        'top_hashtags': hashtags,
                        # 'posts_raw_data': posts_raw_data,
                        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'processing_time': round(time.time() - start_time, 2)
                    }
                    break

            # ä½¿ç”¨pandasè¿›è¡Œæ•°æ®å¤„ç†
            df = pd.DataFrame(posts_raw_data)

            # è·å–æ‰€æœ‰çš„è¯é¢˜, ä»¥åŠæ¯ä¸ªè¯é¢˜çš„ä½¿ç”¨æ¬¡æ•°
            all_hashtags = df["hashtags"]
            hashtags_regroup = {}
            for hashtags in all_hashtags:
                if hashtags is None:
                    continue
                hashtags = json.loads(hashtags)
                for name, id in hashtags.items():
                    if name in hashtags_regroup:
                        hashtags_regroup[name]["count"] += 1
                    else:
                        hashtags_regroup[name] = {"count": 1, "id": id}

            # è·å–ä½¿ç”¨ç‡æœ€é«˜çš„è¯é¢˜
            count = min(count, len(hashtags_regroup))
            hashtags = sorted(hashtags_regroup.items(), key=lambda x: x[1]["count"], reverse=True)[:count]
            hashtags_dict = {hashtag: data for hashtag, data in hashtags}

            uniqueID = url.split("@")[-1]

            report_url = await self.generate_analysis_report(uniqueID, 'post_hashtags', hashtags_dict)

            yield {
                'user_profile_url': url,
                'is_complete': True,
                'message': f'å·²å®Œæˆè·å–è¯é¢˜æ•°æ®',
                'report_url': report_url,
                'total_posts': total_posts,
                'top_hashtags': hashtags,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

        except Exception as e:
            logger.error(f"âŒ è·å–è¯é¢˜æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'error': str(e),
                'message': f"è·å–è¯é¢˜æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'total_posts': total_posts,
                'top_hashtags': hashtags,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

    async def fetch_post_creator_analysis(self, url: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ç»¼åˆåˆ†æåˆ›ä½œè€…è§†é¢‘ï¼ŒåŒ…æ‹¬çƒ­é—¨è§†é¢‘ã€å¹¿å‘Š/å¸¦è´§è§†é¢‘ã€AI/VRè§†é¢‘ã€é£é™©è§†é¢‘

        Args:
            url: ç”¨æˆ·ä¸ªäººä¸»é¡µURL

        Yields:
            Dict: å„ä¸ªé˜¶æ®µçš„åˆ†æç»“æœï¼Œè§†é¢‘ä¿¡æ¯ä»…åŒ…å«aweme_id, desc, download_addr, create_time
        """
        logger.info("ğŸ” å¼€å§‹å…¨é¢åˆ†æåˆ›ä½œè€…è§†é¢‘æ•°æ®...")
        start_time = time.time()
        post_count = 0
        posts_raw_data = []
        analysis_results = {}

        try:
            # è·å–ç”¨æˆ·æ€»å‘å¸ƒä½œå“æ•°
            total_posts = await self.user_collector.fetch_total_posts_count(url)

            # é‡‡é›†ç”¨æˆ·å‘å¸ƒçš„ä½œå“æ•°æ®
            async for posts in self.user_collector.collect_user_posts(url):
                cleaned_posts = await self.user_cleaner.clean_user_posts(posts)
                if cleaned_posts:
                    post_count += len(cleaned_posts)
                    if post_count <= total_posts:
                        posts_raw_data.extend(cleaned_posts)
                        yield {
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®..., è¿›åº¦: {post_count}/{total_posts}...',
                            'total_posts': total_posts,
                            'analysis_results': analysis_results,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                    elif post_count > total_posts:
                        posts_raw_data.extend(cleaned_posts[:total_posts - post_count])
                        post_count = total_posts
                        logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡å¼€å§‹åˆ†æ...")
                        yield {
                            'user_profile_url': url,
                            'is_complete': False,
                            'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡å¼€å§‹åˆ†æ...',
                            'total_posts': total_posts,
                            'analysis_results': analysis_results,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'processing_time': round(time.time() - start_time, 2)
                        }
                        break
                else:
                    logger.info(f"å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡å¼€å§‹åˆ†æ...")
                    yield {
                        'user_profile_url': url,
                        'is_complete': False,
                        'message': f'å·²é‡‡é›†{post_count}æ¡ä½œå“æ•°æ®, å‡†å¤‡å¼€å§‹åˆ†æ...',
                        'total_posts': total_posts,
                        'analysis_results': analysis_results,
                        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'processing_time': round(time.time() - start_time, 2)
                    }
                    break

            # ä½¿ç”¨pandasè¿›è¡Œæ•°æ®å¤„ç†
            df = pd.DataFrame(posts_raw_data)

            # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºç®€åŒ–è§†é¢‘æ•°æ®ï¼Œåªä¿ç•™æŒ‡å®šå­—æ®µ
            def simplify_video_data(videos_list):
                simplified_videos = []
                for video in videos_list:
                    simplified_videos.append({
                        'aweme_id': video.get('aweme_id'),
                        'desc': video.get('desc'),
                        'download_addr': video.get('download_addr'),
                        'create_time': video.get('create_time')
                    })
                return simplified_videos

            # 1. åˆ†æçƒ­é—¨è§†é¢‘
            logger.info("ğŸ“Š æ­£åœ¨åˆ†æçƒ­é—¨è§†é¢‘...")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'message': 'æ­£åœ¨åˆ†æçƒ­é—¨è§†é¢‘...',
                'total_posts': total_posts,
                'analysis_results': analysis_results,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

            # è·å–çƒ­é—¨è§†é¢‘ï¼ŒæŒ‰ç…§ç‚¹èµæ•°æ’åºï¼Œå–å‰5
            hot_videos_digg = df.sort_values("digg_count", ascending=False).head(5).to_dict(orient="records")

            # è·å–çƒ­é—¨è§†é¢‘ï¼ŒæŒ‰ç…§æ’­æ”¾é‡æ•°æ’åºï¼Œå–å‰5
            hot_videos_views = df.sort_values("play_count", ascending=False).head(5).to_dict(orient="records")

            # è·å–çƒ­é—¨è§†é¢‘ï¼ŒæŒ‰ç…§è¯„è®ºæ•°æ’åºï¼Œå–å‰5
            hot_videos_comments = df.sort_values("comment_count", ascending=False).head(5).to_dict(orient="records")

            # è·å–çƒ­é—¨è§†é¢‘ï¼ŒæŒ‰ç…§åˆ†äº«æ•°æ’åºï¼Œå–å‰5
            hot_videos_shares = df.sort_values("share_count", ascending=False).head(5).to_dict(orient="records")

            # å°†å®ƒä»¬åˆå¹¶ï¼Œæ ¹æ®aweme_idå»é‡
            hot_videos = []
            seen_ids = set()

            for video in hot_videos_digg + hot_videos_views + hot_videos_comments + hot_videos_shares:
                video_id = video['aweme_id']
                if video_id not in seen_ids:
                    seen_ids.add(video_id)
                    hot_videos.append(video)

            # ç®€åŒ–çƒ­é—¨è§†é¢‘æ•°æ®
            simplified_hot_videos = simplify_video_data(hot_videos)

            # è·å–ç½®é¡¶è§†é¢‘
            top_videos = df[df["is_top"].eq(True)].to_dict(orient="records")

            # ç®€åŒ–ç½®é¡¶è§†é¢‘æ•°æ®
            simplified_top_videos = simplify_video_data(top_videos)

            analysis_results["hot_videos"] = {
                "hot_videos": simplified_hot_videos,
                "top_videos": simplified_top_videos,
                "top_videos_count": len(simplified_top_videos)
            }

            # 2. åˆ†æå¹¿å‘Š/å¸¦è´§è§†é¢‘
            logger.info("ğŸ“Š æ­£åœ¨åˆ†æå¹¿å‘Š/å¸¦è´§è§†é¢‘...")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'message': 'æ­£åœ¨åˆ†æå¹¿å‘Š/å¸¦è´§è§†é¢‘...',
                'total_posts': total_posts,
                'analysis_results': analysis_results,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

            # è·å–å¹¿å‘Šè§†é¢‘
            ads_videos = df[df["is_ads"].eq(True)].to_dict(orient="records")

            # ç®€åŒ–å¹¿å‘Šè§†é¢‘æ•°æ®
            simplified_ads_videos = simplify_video_data(ads_videos)

            # è·å–ç”µå•†è§†é¢‘
            ec_videos = df[df["is_ec_video"].eq(True)].to_dict(orient="records")

            # ç®€åŒ–ç”µå•†è§†é¢‘æ•°æ®
            simplified_ec_videos = simplify_video_data(ec_videos)

            analysis_results["commerce_videos"] = {
                "ads_videos_count": len(simplified_ads_videos),
                'ec_videos_count': len(simplified_ec_videos),
                "ads_videos": simplified_ads_videos,
                'ec_videos': simplified_ec_videos
            }

            # 3. åˆ†æAI/VRç”Ÿæˆè§†é¢‘
            logger.info("ğŸ“Š æ­£åœ¨åˆ†æAI/VRç”Ÿæˆè§†é¢‘...")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'message': 'æ­£åœ¨åˆ†æAI/VRç”Ÿæˆè§†é¢‘...',
                'total_posts': total_posts,
                'analysis_results': analysis_results,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

            # è·å–AIç”Ÿæˆè§†é¢‘
            ai_videos = df[df["created_by_ai"].eq(True)].to_dict(orient="records")

            # ç®€åŒ–AIç”Ÿæˆè§†é¢‘æ•°æ®
            simplified_ai_videos = simplify_video_data(ai_videos)

            # è·å–VRè§†é¢‘
            vr_videos = df[df["is_vr"].eq(True)].to_dict(orient="records")

            # ç®€åŒ–VRè§†é¢‘æ•°æ®
            simplified_vr_videos = simplify_video_data(vr_videos)

            analysis_results["synthetic_videos"] = {
                "ai_videos_count": len(simplified_ai_videos),
                'vr_videos_count': len(simplified_vr_videos),
                "ai_videos": simplified_ai_videos,
                'vr_videos': simplified_vr_videos
            }

            # 4. åˆ†æé£é™©è§†é¢‘
            logger.info("ğŸ“Š æ­£åœ¨åˆ†æé£é™©è§†é¢‘...")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'message': 'æ­£åœ¨åˆ†æé£é™©è§†é¢‘...',
                'total_posts': total_posts,
                'analysis_results': analysis_results,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

            # è·å–é£é™©è§†é¢‘
            risk_videos = df[df["in_reviewing"] | df["is_prohibited"]].to_dict(orient="records")

            # ç®€åŒ–é£é™©è§†é¢‘æ•°æ®
            simplified_risk_videos = simplify_video_data(risk_videos)

            analysis_results["risk_videos"] = {
                "risk_videos_count": len(simplified_risk_videos),
                "risk_videos": simplified_risk_videos
            }

            uniqueID = url.split("@")[-1]
            report_url = await self.generate_analysis_report(uniqueID, 'post_creator_analysis', analysis_results)

            # å®Œæˆæ‰€æœ‰åˆ†æï¼Œè¿”å›æœ€ç»ˆç»“æœ
            yield {
                'user_profile_url': url,
                'is_complete': True,
                'message': 'åˆ†æå®Œæˆ',
                'report_url': report_url,
                'total_posts': total_posts,
                'analysis_results': analysis_results,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

        except Exception as e:
            logger.error(f"åˆ†æåˆ›ä½œè€…è§†é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                'user_profile_url': url,
                'is_complete': False,
                'error': str(e),
                'message': f"åˆ†æåˆ›ä½œè€…è§†é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                'total_posts': total_posts,
                'analysis_results': analysis_results,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'processing_time': round(time.time() - start_time, 2)
            }

        """
        è·å–ç”¨æˆ·/è¾¾äººçš„ç²‰ä¸ç”»åƒ
        """

        data = kwargs.get('data')
        logger.info("ğŸ“Š æ­£åœ¨åˆ†æç”¨æˆ·ç²‰ä¸ç”»åƒ...")
        with open(f"{config.DATA_DIR}/fans_analysis.json", "w") as f:
            json.dump(data, f)
        return data



async def main():
    crawler = UserCollector()
    cleaner = UserCleaner()
    analyzer = UserAgent()

    user_url = "https://www.tiktok.com/@galileofarma"

    # æµ‹è¯•fetch_user_posts_trend
    async for data in analyzer.fetch_post_hashtags(user_url, 30):
        print(data)


if __name__ == "__main__":
    asyncio.run(main())

